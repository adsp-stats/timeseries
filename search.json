[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "",
    "text": "Introduction\nHello! I’m Jonathan, and I want to teach you time series analysis I have designed this website to accompany the University of Chicago course ADSP 31006 “Time Series Analysis and Forecasting”. Although the course will mostly be taught in a traditional lecture format, I would like to use this website to provide backing material for the lectures.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-referencetexts",
    "href": "index.html#sec-referencetexts",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "Recommended textbooks",
    "text": "Recommended textbooks\nUnlike my statistics course website, I do not intend that this course website be considered the principle text for the class. We will have two primary textbooks (described below), and the notes in these pages supplement the readings without replacing the readings.\n\nForecasting: Principles and Practice 3rd edition, by Rob Hyndman and George Athanasopoulos\nThis book is freely available and one of our two major reference sources. The book relies heavily on R code using the tidyverse constellation of packages and the new fable package written by the authors, which interleaves specifically with the tsibble package. Readers who prefer base R may instead read the book’s second edition (also freely available), and readers who prefer Python will benefit from the new Python edition (also freely available).\nPractical Time Series Analysis, by Aileen Nielsen\nThis is the second of our two major reference sources. In contrast to the Hyndman and Athanasopoulos textbook, Nielsen includes more on-the-job tips about working with real datasets, structuring and storing the data, and putting forecasting models into production. She uses a combination of R (using the less-common data.table environment) and Python examples, and describes time series analysis from both a statistical and a machine learning perspective.\nTime Series Analysis: Forecasting and Control 5th edition, by George Box, et al.\nOne of the classic texts in the field. Box and his co-authors have introduced and refined (over several decades) the best all-purpose textbook from a purely statistical perspective. They spend less time discussing programming considerations or machine learning models.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-these-notes-were-made",
    "href": "index.html#how-these-notes-were-made",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "How these notes were made",
    "text": "How these notes were made\nI assembled these notes using Quarto, a publishing system built around the Pandoc markdown language. I wrote all the code backing these notes in R, and alongside every figure or table you can find the corresponding R code.\nNeither the text nor the R code in these notes were generated by AI tools: for better and worse the opinions expressed here are my own, and the I’ve described these concepts in my own voice.1 Complaints can be submitted here.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "",
    "text": "AI assistance was used to brainstorm case studies and examples, and to help with the layout and coding of the website itself.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "whatisatimeseries.html",
    "href": "whatisatimeseries.html",
    "title": "What is a time series?",
    "section": "",
    "text": "Notation\nData frequently comes to us in a pile, an unordered heap of observations:\nAll of these observations can be matched to a specific date and time:\nTime-based information might be helpful in understanding these data:\nAnd yet, these orders and timestamps are not required for an understanding of the data. We are not observing one process over time, we are observing many different processes which happen to be “sampled” in a particular order. In a different world, they could easily have been placed in a different order.\nContrast this to a different set of data series:\nThe order of observations within each of these datasets matters. They would tell very different stories if presented out of order. Knowing one observation (one month’s unemployment, one split second’s temperature, one week’s album sales) severely constrains the likely or possible values for the next observation. In linear regression we would view the flow of information across observations as an undesired bug; in these datasets, serial correlation is a feature.\nData scientists do not have a standard definition for time series data. I will attempt a working definition, useful for our purposes but not meant to invalidate other definitions:\nIn this definition I have avoided any reference to statistical inference such as expectation, probability, distributions of random variables, etc. Of course we can use statistical models to study time series data, but we can also use machine learning models or naive models which make no assumptions about the data generating process.\nThis definition is very broad, but we only have ten weeks together, and we will need to define what is in-scope and out-of-scope for this course:\nBecause we will be focusing on a narrow subset of time series processes, we can afford to be a little loose with our notation. In other textbooks you may see more complex representations, meant to flexibly extend to irregularly-observed or continuous time series. Instead, we will adopt the following conventions:",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a time series?</span>"
    ]
  },
  {
    "objectID": "whatisatimeseries.html#notation",
    "href": "whatisatimeseries.html#notation",
    "title": "What is a time series?",
    "section": "",
    "text": "\\(T\\) is the time index. All observations of all time series will happen at times \\(T \\in \\mathbb{N} = \\{0, 1, 2, \\ldots\\}\\). Most samples will start with \\(T=1\\), but we sometimes have a use for the zero-period \\(T=0\\) (e.g. to initialize a series). Arbitrary time indices will generally be represented by \\(T=t\\), and a pair of time indices will generally be represented by \\(T=s\\) and \\(T=t\\).\n\\(\\boldsymbol{Y}\\) is a time series process, a theoretical number-generating sequence observed at regular intervals. It is an ordered collection of random variables.\n\\(Y_1, Y_2, \\ldots, Y_n\\) are the random variables formed by the observation of \\(\\boldsymbol{Y}\\) at time index \\(T = 1, 2, \\ldots, n\\). Each \\(Y_t\\) is itself a random variable.\n\\(\\boldsymbol{y}\\) is a finite sample taken from the process \\(\\boldsymbol{Y}\\). Frequently, \\(\\boldsymbol{y}\\) is the dataset in front of us.\n\\(y_1, y_2, \\ldots, y_n\\) are the specific observations from the sample \\(\\boldsymbol{y}\\) at time index \\(T = 1, 2, \\ldots, n\\).\nIf we need another time series, we can use X: \\(\\boldsymbol{X} = X_1, X_2, \\ldots, X_n\\) is the generating process and its random variables, while \\(\\boldsymbol{x} = x_1, x_2, \\ldots, x_n\\) is the sample.\nWhen referencing a single random variable with no time-varying component, we will use unbolded uppercase letters without a subscript: \\(Z \\sim \\textrm{Normal}(\\mu, \\sigma^2)\\), \\(U \\sim \\textrm{Uniform}(0,1)\\), etc.\nLowercase omega will always be reserved for a white noise process: \\(\\boldsymbol{\\omega} = \\omega_1, \\omega_2, \\ldots \\omega_n\\). These processes are usually unobserved but if we do need to describe a sample (e.g. for a simulation), we may use \\(\\boldsymbol{w} = w_1, w_2, \\ldots, w_n\\).2",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a time series?</span>"
    ]
  },
  {
    "objectID": "whatisatimeseries.html#footnotes",
    "href": "whatisatimeseries.html#footnotes",
    "title": "What is a time series?",
    "section": "",
    "text": "At least conceptually — e.g. while some years are 365 days and others are 366 days, yearly data is still considered regular.↩︎\nSimilar to how the OLS residuals \\(\\boldsymbol{e} = e_1, e_2, \\ldots, e_n\\) are estimated realizations of the theoretical error process \\(\\boldsymbol{\\varepsilon} = \\varepsilon_1, \\varepsilon_2, \\ldots, \\varepsilon_n\\).↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a time series?</span>"
    ]
  },
  {
    "objectID": "autocovariance.html",
    "href": "autocovariance.html",
    "title": "Autocovariance and autocorrelation",
    "section": "",
    "text": "Review: covariance and correlation\nIn previous courses, you would probably have learned about covariance and correlation. The covariance of two random variables measures the raw amount by which they move together or separately.\n\\[\\textrm{Cov}(Y,X) = \\mathbb{E}[(Y - \\mu_Y)(X - \\mu_X)]\\]\nCovariance is an important metric for a lot of theoretical calculations, but it’s difficult for humans to work with because it’s very sensitive to the units in which \\(X\\) and \\(Y\\) are measured. For this reason, we often prefer to summarize the relation between two random variables with the Pearson correlation coefficient,1 which normalizes the covariance by the respective standard deviations of \\(X\\) and \\(Y\\).\n\\[\\mathrm{Cor}(Y,X) = \\rho_{YX} = \\frac{\\textrm{Cov}(Y,X)}{\\sigma_Y \\cdot \\sigma_X}\\]\nCorrelation is very convenient to work with, because it’s a unitless measure which is always bounded between -1 and +1. We can easily describe the strength of the linear relationship between two variables in ways which are directly comparable to other pairs of variables.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#review-estimating-covariance-and-correlation-from-a-sample",
    "href": "autocovariance.html#review-estimating-covariance-and-correlation-from-a-sample",
    "title": "Autocovariance and autocorrelation",
    "section": "Review: Estimating covariance and correlation from a sample",
    "text": "Review: Estimating covariance and correlation from a sample\nCovariance and correlation are two properties that describe the theoretical relationship between two random variables. When we work from samples of data, we don’t always know the true covariance or correlation, and we need to estimate these metrics from our samples. Unbiased estimators for both metrics can be found below:\n\\[\\textrm{Sample covariance:} \\quad S_{\\boldsymbol{yx}} = \\frac{1}{n-1} \\sum_i (y_i - \\bar{y})(x_i - \\bar{x})\\]\n\\[\\textrm{Sample correlation:} \\quad r_{\\boldsymbol{yx}} = \\frac{\\sum_i (y_i - \\bar{y})(x_i - \\bar{x})}{\\sqrt{\\sum_i (y_i - \\bar{y})^2} \\sqrt{\\sum_i (x_i - \\bar{x})^2}}\\]",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#autocovariance-and-autocorrelation",
    "href": "autocovariance.html#autocovariance-and-autocorrelation",
    "title": "Autocovariance and autocorrelation",
    "section": "Autocovariance and autocorrelation",
    "text": "Autocovariance and autocorrelation\nThese two measures have very close counterparts in time series analysis.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series observed at regular time periods \\(T = \\{1,2,\\ldots,n\\}\\) and denote the mean and variance of the random variable at each time index \\(t\\) as \\(\\mathbb{E}[Y_t] = \\mu_t\\) and \\(\\mathbb{V}(Y_t) = \\sigma^2_t\\). Then, for any two time indices \\(s,t \\in T\\), the autocovariance between \\(Y_s\\) and \\(Y_t\\) is defined as:\n\\[\\textrm{Cov}(Y_s,Y_t) = \\gamma_{s,t}  = \\mathbb{E}[(Y_s - \\mu_s)(Y_t - \\mu_t)]\\]\nAnd the autocorrelation between \\(Y_s\\) and \\(Y_t\\) is defined as:\n\\[\\textrm{Cor}(Y_s,Y_t) = \\rho_{s,t}  = \\frac{\\textrm{Cov}(Y_s,Y_t)}{\\sigma_s \\cdot \\sigma_t}\\]\n\n\nIf a time series is weakly stationary, then its mean and standard deviation are the same at every time period, and the autocovariance (and autocorrelation) will only depend on the lag between the two time periods:\n\n\n\n\n\n\nWarningOnly when Y is weakly stationary\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a weakly stationary time series observed at regular time periods \\(T = \\{1,2,\\ldots,n\\}\\) with mean \\(\\mathbb{E}[Y_t] = \\mu\\) and variance \\(\\mathbb{V}[Y_t] = \\sigma^2\\) for all \\(t \\in T\\). Then, the autocovariance between any two observations of the series \\(Y_t\\) and \\(Y_{t+k}\\) (a second random variable observed \\(k\\) periods later) is defined as:\n\\[\\begin{aligned} \\textrm{Cov}(Y_t,Y_{t+k}) = \\gamma_{k}  &= \\mathbb{E}[(Y_t - \\mu)(Y_{t+k} - \\mu)] \\\\ &= \\mathbb{E}[Y_t \\cdot Y_{t+k}] - \\mu^2 \\end{aligned}\\]\nNote that each covariance between \\(Y_t\\) and \\(Y_{t+k}\\) will be equal to \\(\\gamma_k\\) regardless of the time index t. And the autocorrelation between \\(Y_t\\) and \\(Y_{t+k}\\) is defined as:\n\\[\\textrm{Cor}(Y_t,Y_{t+k}) = \\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\frac{\\textrm{Cov}(Y_t,Y_{t+k})}{\\sigma^2}\\]",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#estimating-autocovariance-and-autocorrelation-from-a-sample",
    "href": "autocovariance.html#estimating-autocovariance-and-autocorrelation-from-a-sample",
    "title": "Autocovariance and autocorrelation",
    "section": "Estimating autocovariance and autocorrelation from a sample",
    "text": "Estimating autocovariance and autocorrelation from a sample\nIf our time series is not stationary, then we cannot really estimate the autocovariance \\(\\gamma_{s,t}\\) or the autocorrelation \\(\\rho_{s,t}\\) from a sample, because there will only be one pair of values to observe at those time periods. However, if our time series is stationary, then we can estimate \\(\\gamma_k\\) and \\(\\rho_k\\) from all the pairs of observations which are \\(k\\) time periods apart.\nThe exact calculation method for estimating autocovariance varies from source to source. All of the popular methods involve bias-variance compromises. The R software environment generally uses the following definition for autocorrelation:2\n\\[r_k = \\hat{\\rho}_k = \\frac{c_k}{c_0}\\]\n\\[c_k = \\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=1}^{n-k}(y_t - \\bar{y})(y_{t+k} - \\bar{y})\\]\nIn some sources the terms autocovariance and autocorrelation are used synonymously, while in other sources those words are used for the two quantities \\(\\gamma\\) and \\(\\rho\\) respectively, where \\(\\rho\\) is normalized to bounds between -1 and +1. In this course, we will always observe the difference between the two terms.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#footnotes",
    "href": "autocovariance.html#footnotes",
    "title": "Autocovariance and autocorrelation",
    "section": "",
    "text": "There are other named correlation coefficients, but Pearson’s is so widely used that if someone says ‘correlation’, you can assume they mean Pearson’s correlation.↩︎\nNotice that the covariance estimator uses a denominator of \\(n\\) instead ofg \\(n-k\\), which will introduce bias but generally lower MSE.↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "whitenoise.html",
    "href": "whitenoise.html",
    "title": "White noise",
    "section": "",
    "text": "Motivation\nImagine observing a signal with a strong periodicity, like a sine wave. We could depict that signal as a waveform in the time domain (the world we experience). We could also depict that signal as a peak in the frequency domain (i.e. the spectral density). Let’s suppose that the signal repeats every 100 time-units, for a frequency of 0.01:\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny1 &lt;- sin((1:1200)*2*pi/100)\nplot(y1,type='b',pch=20,xlim=c(1,200))\nspectrum(y1,log='no',xlim=c(0,0.2))\n\n\n\n\n\nSingle waveform viewed in the time domain and the frequency domain\nNow imagine a more complicated signal, a composite of our first signal and a new signal which repeats every 20 units, creating a new spectral density peak at 0.05:\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny2 &lt;- sin((1:1200)*2*pi/20)\nplot(y1+y2,type='b',pch=20,xlim=c(1,200))\nspectrum(y1+y2,log='no',xlim=c(0,0.2))\n\n\n\n\n\nDouble waveform viewed in the time domain and the frequency domain\nWe can continue to add more peaks to the spectral density, for example by adding a new waveform with a period every 8 observations (for a frequency of 0.125):\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny3 &lt;- sin((1:1200)*2*pi/8)\nplot(y1+y2+y3,type='b',pch=20,xlim=c(1,200))\nspectrum(y1+y2+y3,log='no',xlim=c(0,0.2))\n\n\n\n\n\nTreble waveform viewed in the time domain and the frequency domain\nLet’s take this to the limit case: what if the spectral density were equally powerful at every frequency? We call this white noise:\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny4 &lt;- rnorm(100000)\nplot(y4,type='b',pch=20,xlim=c(1,200))\nspectrum(y4,log='no',xlim=c(0,0.2))\n\n\n\n\n\nStandard normal data viewed in the time domain and the frequency domain\nWhite noise refers to any time-domain generating process which has uniform power across the frequency domain. It has no connection to anyone named White1, and instead is a metaphor for “white light” — light which is equally strong across all wavelength frequencies in the visible part of the electomagnetic spectrum.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "whitenoise.html#definition",
    "href": "whitenoise.html#definition",
    "title": "White noise",
    "section": "Definition",
    "text": "Definition\nMany different series can be white noise. Choosing either -1 or +1 with equal and independent probability would be a series of white noise. White noise must have the following properties in the time domain:\n\nThe observations must show no serial correlation, and in fact most definitions require that they be statistically independent of each other.2\nThe observations must have mean 0.\nThe observations must have a finite variance.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "whitenoise.html#additive-gaussian-white-noise",
    "href": "whitenoise.html#additive-gaussian-white-noise",
    "title": "White noise",
    "section": "Additive Gaussian white noise",
    "text": "Additive Gaussian white noise\nAlthough many series can be white noise, some of the models we will study in this course assume that the white noise takes a specific form, where each observation is drawn from an IID Normal distribution with mean zero and constant variance. In other words, the error process we normally3 associate with a linear regression is also white noise, and is often used to describe the error process of time series models. (The Normal distribution is also called the Gaussian distribution.)\nAn individual value from a white noise process is sometimes called an “innovation” or a “random shock” instead of an “error”. We will generally use the term “innovation” in this course, rather than “error”, to emphasize that the shocks are not wrong or misleading in any sense, they are simply unknown until the moment they happen; they are new discoveries that change the status quo.\nTo reinforce this perspective, white noise innovations in a time series model are generally denoted as \\(\\omega_1, \\omega_2, \\ldots, \\omega_n\\) and not as \\(\\varepsilon_1, \\varepsilon_2, \\ldots, \\varepsilon_n\\). As a mnemonic, you could say they are “w for white noise, not e for error”.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{\\omega}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). If \\(\\omega_t \\sim \\textrm{Normal}(0,\\sigma^2)\\) for all \\(t \\in T\\) and \\(\\omega_s \\perp \\omega_t\\) for all \\(s,t \\in T\\), then the generating process \\(\\boldsymbol{\\omega}\\) is said to be additive Gaussian white noise",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "whitenoise.html#footnotes",
    "href": "whitenoise.html#footnotes",
    "title": "White noise",
    "section": "",
    "text": "E.g., the econometrician Halbert White for whom we name White standard errors and the White test for heteroskedasticity.↩︎\nIndependent does imply uncorrelated, but not all uncorrelated variables are independent.↩︎\nNo pun intended!↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "randomwalk.html",
    "href": "randomwalk.html",
    "title": "Random walks",
    "section": "",
    "text": "Motivation\nBefore learning about time series which we can usefully study or forecast, it might help to see an example of a times series we cannot usefully study or forecast: the random walk.\nCode\nset.seed(1999)\nw &lt;- rnorm(100)\nrw &lt;- cumsum(w)\nplot(rw,type='b',pch=20,xlab='Index',ylab='Measurement')\n\n\n\n\n\n100 observations of a random walk\nThis times series seems to carry information. We see a positive trend. Perhaps we see some evidence of periodicity/seasonality. Sadly, we are mistaken: what we are looking at instead is a random walk.\nRandom walks refer to memoryless time series processes which “drift” randomly across 1, 2, or more dimensions. They may return to their starting value, but do not do so predictably. Let’s examine five possible futures that the random walk above might take — each of these are completely consistent evolutions of the series, each as likely as the other.\nCode\nset.seed(1999)\nw2 &lt;- rbind(cbind(w,w,w,w,w),matrix(rnorm(500),ncol=5))\nrw2 &lt;- apply(w2,2,cumsum)\nplot(rw,type='l',lwd=2,xlab='Index',xlim=c(1,200),ylab='Measurement',ylim=c(-5,25))\nmatplot(x=101:200,y=rw2[101:200,],type='l',lwd=2,pch=20,add=TRUE,lty=1,\n        col=colorRampPalette(c('#0000ff','#bbbbbb','#ff0000'))(5))\n\n\n\n\n\nFive possible extensions of the same random walk\nRandom walks can be one-dimensional, like the example above, or multidimensional. The time between steps can be regular, irregular, or even continuous. The step sizes themselves can be equal, discrete, or continuous. (Random walks are, for example, closely tied to the idea of ‘Brownian motion’, which is the drift through space and time of tiny particles suspended in air or fluid, such as smoke, or dust in a sunbeam.)\nBelow is an example of a two-dimensional random walk where each step size is one unit.\nCode\nnw &lt;- 2000\nrwcols &lt;- colorRampPalette(c('#0000ff','#bbbbbb','#ff0000'))(nw)\nset.seed(1235)\nrwx &lt;- cumsum(sample(c(-1,1),nw,TRUE))\nrwy &lt;- cumsum(sample(c(-1,1),nw,TRUE))\nplot(rwx,rwy,type='n',asp=1,,axes=FALSE,xlab=NA,ylab=NA,main=NA,sub=NA)\naxis(1,at=c(-40,-20,0)); axis(2,at=c(-20,0,20))\nsegments(x0=c(0,rwx[-nw]),x1=rwx,y0=c(0,rwy[-nw]),y1=rwy,col=rwcols)\npoints(c(0,rwx[nw]),c(0,rwy[nw]),col=rwcols[c(1,nw)],pch=19)\nlegend(x='bottomleft',legend=c('Start (t=0)','End (t=2000)'),\n       pch=19,col=rwcols[c(1,nw)],bty='n',cex=0.8)      \n\n\n\n\n\n2000 steps of a 2D binary random walk.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random walks</span>"
    ]
  },
  {
    "objectID": "randomwalk.html#definition",
    "href": "randomwalk.html#definition",
    "title": "Random walks",
    "section": "Definition",
    "text": "Definition\nAlthough random walks appear in many forms, we will mostly concern ourselves in this course with a specific, one-dimensional walk observed in regular time intervals.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{\\omega}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\), where the distribution of each random observation \\(\\omega_t\\) is independently and identically distributed with mean 0 and finite variance \\(\\sigma^2\\) for all \\(t \\in T\\). Define a new time series random variable \\(\\boldsymbol{Y} = Y_1, Y_2, \\ldots, Y_n\\) as follows:\n\\[\\begin{aligned} Y_1 &= \\omega_1 \\\\ Y_t &= Y_{t-1} + \\omega_t \\quad \\forall t&gt;1\\end{aligned}\\]\nThen \\(\\boldsymbol{Y}\\) is a random walk, specifically a homogenous discrete-time random walk in one dimension. If \\(\\omega_t \\stackrel{iid}{\\sim} \\textrm{Normal}(0,\\sigma^2) \\; \\forall t \\in T\\), then we say that \\(\\boldsymbol{Y}\\) is a Gaussian random walk.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random walks</span>"
    ]
  },
  {
    "objectID": "randomwalk.html#properties",
    "href": "randomwalk.html#properties",
    "title": "Random walks",
    "section": "Properties",
    "text": "Properties\nNote that a Gaussian random walk is equivalent to the cumulative sum of a Gaussian white noise process, and that more generally any the cumulative sum of any IID white noise process is some type of random walk.\nConsider the implications of these two calculations:\n\\[\\mathbb{E}[Y_t] = \\mathbb{E}[\\sum_{i=1}^t \\omega_i] = \\sum_{i=1}^t \\mathbb{E}[\\omega_i] = \\sum_{i=1}^t 0 = 0\\]\n\\[\\mathbb{V}[Y_t] = \\mathbb{V}[\\sum_{i=1}^t \\omega_i] = \\sum_{i=1}^t \\mathbb{V}[\\omega_i] = \\sum_{i=1}^t \\sigma^2 = t\\sigma^2\\]\nTaken together, we see that while the expectation or the prediction for the future location of a random walk remains its starting place, the variance increases proportionally with the time period, meaning that the range of likely values grows wider and wider, and that the series becomes quite unlikely to be found at the starting point itself, even though this is technically the “average” outcome.\nNote also that the expectation of a random walk given its past history has nothing to do with its starting point or how long it has run, but simply wherever it was last observed. Consider trying to forecast the value of a random walk at time \\(t\\) given a series of observations ending at time \\(s \\lt t\\):\n\\[\\begin{aligned} \\mathbb{E}[Y_t|y_1,\\ldots,y_s] &= \\mathbb{E}[(Y_s + \\sum_{i=s+1}^t \\omega_i)|y_1,\\ldots,y_s] \\\\ &= y_s + \\mathbb{E}[\\sum_{i=s+1}^t \\omega_i] = y_s + \\sum_{i=s+1}^t \\mathbb{E}[\\omega_i] \\\\ &= y_s + \\sum_{i=s+1}^t 0 = y_s \\end{aligned}\\]",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random walks</span>"
    ]
  },
  {
    "objectID": "stationarity.html",
    "href": "stationarity.html",
    "title": "Stationarity",
    "section": "",
    "text": "Motivation\nWe can’t perfectly forecast everything (or anything!), but perhaps we can draw a meaningful distinction between two different scenarios:\nIt would be nice to define these two scenarios more precisely, and to develop a test which can help us to identify which scenario we face, and perhaps even to develop techniques which transform the first scenario into the second.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#motivation",
    "href": "stationarity.html#motivation",
    "title": "Stationarity",
    "section": "",
    "text": "Scenario 1: The time series is inherently unpredictable. We may be able to guess that the next observation will be somewhere near the current observation, but further into the future we have no idea “where to look” for the time series.\nScenario 2: The exact values of the time series may be random, but the likely future values of the time series are estimable and the further into the future we look, the more consistent our estimates will be.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#definitions",
    "href": "stationarity.html#definitions",
    "title": "Stationarity",
    "section": "Definitions",
    "text": "Definitions\nWe will call the concept discussed in Scenario 2 above stationarity, which is meant to suggest that the time series “stays in place” over time. It may fluctuate, perhaps severely, but will always stay in the neighborhood of “home base”.\nThere are two common definitions of stationarity: we say that a time series is either strongly stationary or weakly stationary (or, of course, nonstationary).\n\nStrong stationarity\nWhen a time series is strongly stationary, its joint unconditional distribution does not depend on the time index. This is a bold assertion and difficult to test, but when true it unlocks many theoretical results and properties.1\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(F_\\boldsymbol{Y}\\) be the joint cumulative distribution function of a set of random observations \\(Y_1, Y_2, ...\\) from the time series \\(\\boldsymbol{Y}\\). Then if\n\\[F_\\boldsymbol{Y}(Y_1 \\le y_1, \\ldots, Y_n \\le y_n) = F_\\boldsymbol{Y}(Y_{1+k} \\le y_1, \\ldots, Y_{n+k} \\le y_n)\\]\nFor all \\(k \\in \\mathbb{N}\\) and all \\(n \\in \\mathbb{N}\\) and all real values of \\(y_1, \\ldots, y_n\\), then we say that the time series \\(\\boldsymbol{Y}\\) exhibits strong stationarity.\n\n\nEssentially, until we begin to actually observe values from a strongly stationary process, the probability distribution of all future points is the same, and any serial dependence between a set of observations is shared between all sets of observations with the same relative time-orderings. (For example, \\(F_\\boldsymbol{Y}(Y_3|Y_1=c)\\) = \\(F_\\boldsymbol{Y}(Y_8|Y_6=c)\\).)\n\n\nWeak stationarity\nKnowing or estimating the full joint distribution of a set of time series observations seems like a lot of work. For many purposes, we may accept a slightly looser definition of a stationary process without losing the mathematical results and guarantees we need to perform our analysis.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(F_\\boldsymbol{Y}\\) be the joint cumulative distribution function of a set of random observations \\(Y_1, Y_2, ...\\) from the time series \\(\\boldsymbol{Y}\\). Then if\n\\[\\begin{aligned} (1) & \\quad \\mathbb{E}[Y_t] = \\mu \\qquad \\forall \\, t \\in T \\\\ \\\\ (2) & \\quad \\mathbb{V}[Y_t] = \\sigma^2 \\lt \\infty \\qquad \\forall \\, t \\in T \\\\ \\\\ (3) & \\quad \\textrm{Cov}(Y_s,Y_t) = \\gamma_{|t-s|} \\qquad \\forall \\, s,t \\in T\\end{aligned}\\]\nThen we say that the time series \\(\\boldsymbol{Y}\\) exhibits weak stationarity.\n\n\nWeak stationarity makes fairly straightforward claims about the first and second moments of \\(F_\\boldsymbol{Y}\\) rather than trying to define its entire joint distribution. Constant mean, constant and finite variance, and an autocovariance function which depends only on the time interval between two random observations.\n\n\nNon-nested definitions\nConfusingly, neither strong stationarity nor weak stationarity imply the other. While it is often true that strongly stationary processes are also weakly stationary, exceptions can be found:\n\nIf \\(Y_t \\stackrel{iid}{\\sim} \\textrm{Cauchy}\\) for all \\(t \\in T\\) then the variance of each observation \\(Y_t\\) and the autocovariance between observations \\(Y_s\\) and \\(Y_t\\) will be undefined or infinite, meaning that \\(\\boldsymbol{Y}\\) cannot be weakly stationary — but since the distribution is known and identical across all time indices, \\(\\boldsymbol{Y}\\) is strongly stationary.\nIf \\(Y_t \\stackrel{iid}{\\sim} \\textrm{Exponential}(1)\\) for odd values of \\(t\\) and \\(Y_t \\stackrel{iid}{\\sim} \\textrm{Poisson}(1)\\) for even values of \\(t\\), then the mean and variance of each observation \\(Y_t\\) would still be 1, and the covariance between the (independent) observations would be 0, meaning that the covariance does not depend on the time indices: \\(\\boldsymbol{Y}\\) would be weakly stationary. However, since (for example) \\(F_\\boldsymbol{Y}(Y_1,Y_2) \\ne F_\\boldsymbol{Y}(Y_2,Y_3)\\), the process is not strongly stationary.2\n\nHowever, these are edge cases. Importantly for our future work, a Gaussian white noise process is both strongly and weakly stationary.\nEqually important, a random walk is never stationary.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#recovering-stationarity-from-non-stationary-processes",
    "href": "stationarity.html#recovering-stationarity-from-non-stationary-processes",
    "title": "Stationarity",
    "section": "Recovering stationarity from non-stationary processes",
    "text": "Recovering stationarity from non-stationary processes\nAlthough most time series are not stationary, we can often apply simple transformations to regain stationary behavior. I will quickly describe three examples below:\n\nTrend stationarity\nSometimes a process is observed with a deterministic trend, that is, a steady drift of the mean away from its initial condition. If the drift in each period \\(t\\) is nonrandom, and known or estimable, we can remove the trend and the “de-trended” series which remains may be stationary.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(\\mathbb{E}[Y_t] = \\mu + \\delta t\\), let \\(\\mathbb{V}[Y_t] = \\sigma^2 \\lt \\infty\\), and let \\(\\textrm{Cov}(Y_s,Y_t) = \\gamma_{|t-s|}\\) for all \\(s,t \\in T\\).\nThen \\(\\boldsymbol{Y}\\) is a trend-stationary time series, and if \\(X_t = Y_t - \\delta t\\) then \\(\\boldsymbol{X}\\) is a weakly stationary time series.\n\n\nThe intuition here is quite simple: if we know or can estimate a non-random component of our time series which moves the mean (destroying stationary), then we can simply remove it when we want to study the random time series behavior and add it back in when we need to perform prediction.\n\n\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\nset.seed(1229)\ntsts &lt;- 70 + 3*arima.sim(list(ar=c(0.5,0.25)),n=100) - 0.4*(1:100)\nplot(tsts,ylab='Y_t',main='Original data series')\nabline(70,-0.4,col='#bbbbbb')\nplot(tsts +  0.4*(1:100),ylab='Y_t - 0.4t',main='Detrended data series')\n\n\n\n\n\nTrend-stationary process with and without detrending\n\n\n\n\nAlthough the deterministic trend described here is additive, this concept generalizes to multiplicative trends or other situations, such as a variance which shrinks or grows at a steady rate over time.\n\n\nDifference stationarity\nWhen a time series drifts away from its mean through a stochastic (i.e. random) process, then simply “tilting” the series by removing a linear trendline will not be enough to restore stationarity.\nIn some cases, the differences between the values of a nonstationary series can themselves be a stationary series. The most well-known example of this is a random walk. Since the random walk is created by cumulating a white noise series, the difference of the random walk is simply the same white noise series, and a white noise series is stationary.\nWe now have cause to introduce the backward difference operator \\(\\nabla\\).\n\\[\\nabla Y_t = Y_t - Y_{t-1}\\]\n\\[\\nabla_{\\!k} Y_t = Y_t - Y_{t-k}\\]\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(\\mathbb{E}[\\nabla Y_t] = \\mu\\), let \\(\\mathbb{V}[\\nabla Y_t] = \\sigma^2 \\lt \\infty\\), and let \\(\\textrm{Cov}(\\nabla Y_s,\\nabla Y_t) = \\gamma_{|t-s|}\\) for all \\(s,t \\in T\\).\nThen \\(\\boldsymbol{Y}\\) is a difference-stationary time series, and if \\(X_t = \\nabla Y_t\\) then \\(\\boldsymbol{X}\\) is a weakly stationary time series.\n\n\nMany real-life processes are essentially random walks and their levels are difficult to study, but their differences (additive or proportional) are useful targets for our analysis. One example woudl be stock prices: the growth of a company’s value over time, combined with the changing scale caused by inflation, and the complications caused by stock splits, dividends, new share issuances, and stock buybacks all combine to make the price of a stock relatively uninteresting to market analysts. However, the percentage price return, or daily proportional change, is a fundamental unit of analysis which we may model as being trend-stationary (the deterministic trend being the risk-free rate).\nKnowing when to de-trend and when to difference is important. A random walk may appear to have a roughly linear trend, but the de-trended series will still be nonstationary:\n\n\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\nn &lt;- 200\nset.seed(1230)\nrw &lt;- cumsum(rnorm(n))\nplot(rw,ylab='Y_t',main='Original data series',type='l')\nlines(x=c(1,n),y=c(rw[1],rw[n]),col='#bbbbbb')\nplot(rw-(1:n)*(rw[n]-rw[1])/(n-1),ylab='Y_t - Avg Drift',main='Detrended data series',type='l')\n\n\n\n\n\nRandom walks de-trended (top) and differenced (bottom)\n\n\n\n\nCode\nplot(rw,ylab='Y_t',main='Original data series',type='l')\nsegments(x0=2:n,x1=2:n,y0=0,y1=diff(rw),col='#bbbbbb')\nplot(diff(rw),ylab='First difference of Y_t',main='Differenced data series',type='l')\n\n\n\n\n\nRandom walks de-trended (top) and differenced (bottom)\n\n\n\n\n\n\nThe Box-Cox transformation\nIn still other cases, the problem with achieving stationarity is that some growth or shrinkage over time effectively places different parts of the time series on different scales. The S&P500 index did not move by more than 5 units per day for its first 20 years… nowadays, it moves by more than 5 almost every day. Its starting value was near 50… nowadays, its value is near 5000. Both the mean and the variance have shifted over time. A special class of transformation can bring series like this closer to stationarity:\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Define \\(\\boldsymbol{Y}^{(\\lambda)}\\), the Box-Cox transformation of \\(\\boldsymbol{Y}\\) as follows, for any choice of parameter \\(\\lambda \\in \\mathbb{R}\\):\n\\[Y_t^{(\\lambda)} = \\left\\{\\begin{array}{ll} \\log Y_t & \\textrm{if}\\; \\lambda = 0 \\\\ \\frac{Y_t^\\lambda - 1}{\\lambda} & \\textrm{if}\\; \\lambda \\ne 0 \\end{array}\\right\\}\\]\n\n\nThe exact parameter \\(\\lambda\\) which brings a nonstationary series closest to stationarity must be estimated through algorithmic means, including (but not limited to) maximum likelihood estimation. As an example below, a Box-Cox transformation with a parameter of 0.1 takes this highly nonstationary airline data and transforms it into something much closer to trend-stationarity.3\n\n\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\nplot(AirPassengers,ylab='Monthly Air Passengers (1000s)',main='Original nonstationary series')\nplot(box_cox(AirPassengers,0.1),ylab='Transformed series',main='Box-Cox transformation with lambda=0.1')\nabline(reg=lm(box_cox(AirPassengers,0.1)~time(AirPassengers)),col='#bbbbbb')\n\n\n\n\n\nBox-Cox transformation of monthly airline passengers",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#tests-for-stationarity",
    "href": "stationarity.html#tests-for-stationarity",
    "title": "Stationarity",
    "section": "Tests for stationarity",
    "text": "Tests for stationarity\nHow do we know if a series is stationary? Well, the short answer is, we don’t:\n\nA statistical test would not prove that a series is stationary or nonstationary, only provide some degree of evidence against the null hypothesis.\nEven if a time series seems stationary for the period in which we observe it, we have no guarantee it will remain stationary in the future.\n\nEven so, optimistic statisticians have developed several tests for stationarity. Although the mathematics behind these tests is within the reach of most readers, I will focus on just two tests and discuss how they are used rather than how they are calculated.\n\nThe augmented Dickey-Fuller (ADF) test\nThe Dickey-Fuller test (1979) examines whether a time series might actually be a random walk or stationary (including trend-stationary). The specific model being tested is whether\n\\[\\nabla Y_t = c + \\alpha Y_{t-1} + \\omega_t\\]\nIf \\(\\alpha=0\\) then we have a random walk with a deterministic trend: the step size between every pair of observations of \\(\\boldsymbol{Y}\\) are a constant (creating drift) plus white noise (creating the random walk).\nBecause of this setup, the null hypothesis of \\(\\alpha \\ge 0\\) codes for nonstationarity, while the one-sided alternative hypothesis is stationarity (since negative values of \\(\\alpha\\) create a “rubber band” mean-reverting process).\nThe augmented Dickey-Fuller (ADF) test extends this concept to control for complex autocorrelations in the original data series which might obscure the presence of a random walk. While the ADF test is slightly less powerful than the original Dickey-Fuller test, it’s more widely used today and considered an improvement upon the original.\nWe see that the original air passenger data series shown above fails to display stationarity, while the Box-Cox transformation does show stationarity using an ADF test:\n\nadf.test(AirPassengers)\n\nWarning in adf.test(AirPassengers): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  AirPassengers\nDickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\nadf.test(box_cox(AirPassengers,lambda=0.1))\n\nWarning in adf.test(box_cox(AirPassengers, lambda = 0.1)): p-value smaller than\nprinted p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  box_cox(AirPassengers, lambda = 0.1)\nDickey-Fuller = -6.9092, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\nThe ADF test is useful and widely accepted, but carries one drawback: even when applied to truly stationary data, it often fails to reject the null hypothesis of a random walk. This created an opening for a newer test which makes stationarity the null hypothesis.\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test proceeds by assuming that every time series can be decomposed into three parts: a deterministic time trend, a stationary error process, and a non-stationary random walk with some unknown variance:\n\\[Y_t = \\delta t + r_t + \\varepsilon_t\\]\nWhere \\(r_t\\) is a random walk with variance \\(\\sigma^2\\), and \\(\\varepsilon_t\\) is a stationary error process.4\nThe KPSS test examines whether the random walk variance could be 0 (in which case it is not really present at all). By testing \\(\\sigma^2 = 0\\), it creates a null hypothesis of stationarity and a one-sided alternative hypothesis that \\(\\boldsymbol{Y}\\) is a random walk.\nWe see that the original air passenger data series shown above fails to display stationarity, while the Box-Cox transformation does show stationarity using a KPSS test:\n\nkpss.test(AirPassengers,null='Trend')\n\nWarning in kpss.test(AirPassengers, null = \"Trend\"): p-value greater than\nprinted p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  AirPassengers\nKPSS Trend = 0.09615, Truncation lag parameter = 4, p-value = 0.1\n\nkpss.test(box_cox(AirPassengers,lambda=0.1),null='Trend')\n\nWarning in kpss.test(box_cox(AirPassengers, lambda = 0.1), null = \"Trend\"):\np-value greater than printed p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  box_cox(AirPassengers, lambda = 0.1)\nKPSS Trend = 0.077267, Truncation lag parameter = 4, p-value = 0.1\n\n\nSo what have we gained, since these results are the same as the ADF test results above? Essentially, by using both tests, we can now differentiate between three distinct situations:\n\nCompelling evidence for stationarity: The ADF and KPSS tests agree on stationarity.\nCompelling evidence for a random walk: The ADF and KPSS tests agree on random walk.\nNot enough evidence and power to be sure: The ADF test cannot reject a random walk but the KPSS test cannot reject stationarity.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#footnotes",
    "href": "stationarity.html#footnotes",
    "title": "Stationarity",
    "section": "",
    "text": "Recall that the definition below is for regularly-observed, one-dimensional discrete time series processes. There are broader definitions of stationarity which generalize to other situations.↩︎\nConsider that \\(Y_1\\) cannot be 0 while \\(Y_2\\) can with probability \\(1/e\\).↩︎\nThe seasonal peaks are nonstationary behavior, but we will learn to model these too in future lesson.↩︎\nWe do not use the notation \\(\\omega_t\\) since the error process need not be white noise.↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "codingbasicsinr.html",
    "href": "codingbasicsinr.html",
    "title": "Coding it up in R",
    "section": "",
    "text": "Time series functions used in this document\nIn the table below, packages with italicized names will need to be installed, while the package names in a standard font face can be found in most base R distributions (though they may need to be loaded into your workspace).",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "codingbasicsinr.html#time-series-functions-used-in-this-document",
    "href": "codingbasicsinr.html#time-series-functions-used-in-this-document",
    "title": "Coding it up in R",
    "section": "",
    "text": "Time series functions used in the code below\n\n\nPackage\nFunction name\nPurpose\n\n\n\n\nstats\nts\nDefine a time series\n\n\nstats\ntime\nExtract the time indices of a ts\n\n\nstats\nstart\nExtract the first time index\n\n\nstats\nend\nExtract the last time index\n\n\nstats\nwindow\nExtract the last time index\n\n\nstats\narima.sim\nSimulate ARIMA data (including random walks)\n\n\ntseries\nadf.test\nADF stationarity test\n\n\ntseries\nkpss.test\nKPSS stationarity test\n\n\nMASS\nboxcox\nSuggest Box-Cox parameters\n\n\nstats\nacf\nCompute and plot autocorrelation\n\n\nstats\nmonthplot\nPlot annual change across seasons\n\n\nforecast\nseasonplot\nPlot annual change across seasons",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "codingbasicsinr.html#creating-and-storing-time-series-data",
    "href": "codingbasicsinr.html#creating-and-storing-time-series-data",
    "title": "Coding it up in R",
    "section": "Creating and storing time series data",
    "text": "Creating and storing time series data\nDifferent packages have different standards for defining a time series. In many cases, you can pass a simple numeric vector to a time series function: the function will typecast the vector into a time series object and proceed as intended. In other cases, you need to make sure that you have typed the vector as a time series.\nTwo common standards for representing time series are stats::ts from base R and tsibble::tsibble from the tidyverse constellation of packages. This document will use base R conventions and functions.\n\n#define a time series\nsample_ts &lt;- ts(1:18, frequency=4, start=c(1999,2))\n\n#print some basic information and the series values\nprint(sample_ts, calendar=FALSE)\n\nTime Series:\nStart = c(1999, 2) \nEnd = c(2003, 3) \nFrequency = 4 \n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18\n\n#alternate print view (still treated as a vector, not a matrix)\nprint(sample_ts, calendar=TRUE)\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1999         1    2    3\n2000    4    5    6    7\n2001    8    9   10   11\n2002   12   13   14   15\n2003   16   17   18     \n\n#recover the time index values for each obseration\ntime(sample_ts)\n\n        Qtr1    Qtr2    Qtr3    Qtr4\n1999         1999.25 1999.50 1999.75\n2000 2000.00 2000.25 2000.50 2000.75\n2001 2001.00 2001.25 2001.50 2001.75\n2002 2002.00 2002.25 2002.50 2002.75\n2003 2003.00 2003.25 2003.50        \n\n#recover the first and last time indices\nstart(sample_ts)\n\n[1] 1999    2\n\nend(sample_ts)\n\n[1] 2003    3\n\n#subset the series and optionally downsample to yearly\nwindow(sample_ts,start=2000,end=2002)\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n2000    4    5    6    7\n2001    8    9   10   11\n2002   12               \n\nwindow(sample_ts,start=2000,end=2002,frequency=1)\n\nTime Series:\nStart = 2000 \nEnd = 2002 \nFrequency = 1 \n[1]  4  8 12",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "codingbasicsinr.html#creating-a-random-walk-in-r",
    "href": "codingbasicsinr.html#creating-a-random-walk-in-r",
    "title": "Coding it up in R",
    "section": "Creating a random walk in R",
    "text": "Creating a random walk in R\nAlthough random walks are defined recursively, “for loops” are almost always the wrong way to accomplish any task in R. Instead, we can rely on R’s native vectorization and parallelization to quickly create a one-dimensional gaussian random walk:\n\n#simulate a gaussian random walk 'manually'\nset.seed(1044)\nrw_a &lt;- cumsum(rnorm(20))\nround(rw_a,2)\n\n [1] -0.05  1.26  2.67  3.47  2.86  2.23  0.94  0.36 -0.26  0.02 -0.65 -0.38\n[13] -0.38 -0.15  0.85  1.60  1.07 -0.73 -0.15 -0.88\n\nplot(rw_a,type='b',ylab='Values',main='Gaussian random walk')\n\n\n\n\n\n\n\n#simulate through a dedicated time series function\n#note result is offset by one observation from above \nset.seed(1044)\nrw_b &lt;- arima.sim(model=list(order=c(0,1,0)),n=20)\nround(rw_b,2)\n\nTime Series:\nStart = 0 \nEnd = 20 \nFrequency = 1 \n [1]  0.00 -0.05  1.26  2.67  3.47  2.86  2.23  0.94  0.36 -0.26  0.02 -0.65\n[13] -0.38 -0.38 -0.15  0.85  1.60  1.07 -0.73 -0.15 -0.88\n\n#recover the white noise innovations (multiple options)\nround(diff(rw_b),2)\n\nTime Series:\nStart = 1 \nEnd = 20 \nFrequency = 1 \n [1] -0.05  1.31  1.41  0.80 -0.61 -0.63 -1.28 -0.59 -0.61  0.27 -0.66  0.27\n[13]  0.00  0.23  1.00  0.75 -0.53 -1.80  0.58 -0.74\n\nset.seed(1044)\nround(rnorm(20),2)\n\n [1] -0.05  1.31  1.41  0.80 -0.61 -0.63 -1.28 -0.59 -0.61  0.27 -0.66  0.27\n[13]  0.00  0.23  1.00  0.75 -0.53 -1.80  0.58 -0.74\n\n\nOf course there are more random walks… we could create Gaussian random walks in two dimensions, or random walks with non-Gaussian innovations.\n\nset.seed(1415)\nrw_c &lt;- apply(mvrnorm(n=20,mu=c(0,0),Sigma=diag(2)),2,cumsum)\nplot(rw_c,type='b',pch=NA,xlab='Y (d1)',ylab='Y (d2)',main='2D random walk')\ntext(x=rw_c,labels=1:20)",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "codingbasicsinr.html#assessing-stationarity",
    "href": "codingbasicsinr.html#assessing-stationarity",
    "title": "Coding it up in R",
    "section": "Assessing stationarity",
    "text": "Assessing stationarity\nLet’s examine some real data. Johnson & Johnson (“J&J”) is a Fortune 500 company which focuses on medical technology and biotech products. Market analysts often describe the performance of publicly traded companies like J&J using the metric of earnings per share (EPS).\n\nplot(JohnsonJohnson,ylab='EPS ($)',main='Quarterly EPS for Johnson & Johnson')\n\n\n\n\n\n\n\n\nWe see strong visual evidence that the series is not stationary: the mean changes over time and so does the variance. Because the variance changes, we can rule out trend-stationarity (and besides, we see that any trend would be non-linear.) Still, we can confirm this visual impression through two quick tests:1\n\nadf.test(JohnsonJohnson)\n\nWarning in adf.test(JohnsonJohnson): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  JohnsonJohnson\nDickey-Fuller = 1.9321, Lag order = 4, p-value = 0.99\nalternative hypothesis: stationary\n\nkpss.test(JohnsonJohnson,null='Trend')\n\nWarning in kpss.test(JohnsonJohnson, null = \"Trend\"): p-value smaller than\nprinted p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  JohnsonJohnson\nKPSS Trend = 0.50099, Truncation lag parameter = 3, p-value = 0.01\n\n\nAlthough the J&J EPS data is nonstationary, neither is it a true Gaussian random walk, since it it very steadily increases and the variance seems to increase over time. This is probably a good candidate for a Box-Cox transformation. Note that the MASS::boxcox function requires a model or formula as its input, and not the raw time series vector. We can create a simple “intercept only” model for the J&J EPS data.\n\nboxcox(JohnsonJohnson~1)\n\n\n\n\n\n\n\n\nThe plot above shows the likelihood function associated with a stationarity test at different choices for the Box-Cox parameter \\(\\lambda\\). It looks like the a range of values from about -0.2 to +0.2 would be acceptable, but since \\(\\lambda=0\\) is so nearly the MLE choice, and since logarithmic transformations are common in econometric analyses, we may adopt \\(\\lambda=0\\) for now, meaning that \\(Y_t^{(\\lambda)} = \\log Y_t\\).\n\nplot(log(JohnsonJohnson),ylab='Log EPS ($)',main='Box-Cox transform of J&J EPS')\n\n\n\n\n\n\n\nadf.test(log(JohnsonJohnson))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(JohnsonJohnson)\nDickey-Fuller = -1.1543, Lag order = 4, p-value = 0.9087\nalternative hypothesis: stationary\n\nkpss.test(log(JohnsonJohnson),null='Trend')\n\nWarning in kpss.test(log(JohnsonJohnson), null = \"Trend\"): p-value smaller than\nprinted p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  log(JohnsonJohnson)\nKPSS Trend = 0.25322, Truncation lag parameter = 3, p-value = 0.01\n\n\nWhat gives?! The Box-Cox transformed EPS data seem to be trend-stationary, but both the ADF test and the KPSS test confirm that the data are not yet stationary. To solve the riddle, we will need to examine the autocorrelation function.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "codingbasicsinr.html#measuring-and-plotting-autocorrelation",
    "href": "codingbasicsinr.html#measuring-and-plotting-autocorrelation",
    "title": "Coding it up in R",
    "section": "Measuring and plotting autocorrelation",
    "text": "Measuring and plotting autocorrelation\n\npar(mfcol=c(1,2))\nplot(JohnsonJohnson,main='Original data',ylab='EPS ($)')\nacf(JohnsonJohnson,main='ACF of original data',xlab='Lag (in years)')\n\n\n\n\n\n\n\n\nThe plots above show the original J&J EPS data alongside a plot of the autocorrelation function, which pairs each lag \\(k = 1, 2, \\ldots,\\) with the estimated autocorrelation \\(\\hat{\\rho}_k = r_k\\). Note that the autocorrelation of any series with itself is of course 1, and so the peak at \\(k=0\\) extends all the way to 1.\nThe remaining peaks show the autocorrelation of this EPS data with its own past values. Because the data have been structured with frequency 4 (appropriate for quarterly data), it takes four lags to reach “1” (year) on the x-axis, but I shall call each past quarter the first lag, second lag, etc. and ignore the x-axis values.\nThe autocorrelation function at the first lag is quite high too – visually about 0.92:\n\nacf(JohnsonJohnson,plot=FALSE)$acf[1:5]\n\n[1] 1.0000000 0.9251021 0.8882631 0.8328480 0.8240770\n\n\n(Okay, about 0.925.) The location of the current quarter’s EPS is highly correlated with the previous quarter’s EPS. Even though EPS is not a cumulated measure, this makes sense: the fortunes of huge companies do not often change overnight, and if a company is profitable in one quarter, it will likely be similarly profitable in the next quarter.\nThe autocorrelations with further lags are also quite high. We should expect this behavior: if this quarter’s EPS is highly correlated with last quarter’s EPS, then last quarter’s EPS is highly correlated with the EPS from two quarters ago, and so this quarter’s EPS is also at least moderately correlated with the EPS from two quarters ago!\nLet’s try examining the ACF plot for the de-trended, Box-Cox transformed EPS data:\n\neps_boxcox_detrend &lt;- ts(lm(log(JohnsonJohnson)~time(JohnsonJohnson))$residuals,\n                         start=1960,frequency=4)\npar(mfcol=c(1,2))\nplot(eps_boxcox_detrend,main='De-trended, transformed data',ylab='Transformed EPS ($)')\nacf(eps_boxcox_detrend,main='ACF of de-trended, transformed data',xlab='Lag (in years)')\n\n\n\n\n\n\n\n\nThe Box-Cox transformation and de-trending successfully removed a lot of the nonstationary behavior, but the remaining autocorrelation is highly seasonal — each transformed EPS observation is very similar to the EPS from four quarters ago (and thus eight, and twelve…). If we plot the quarterly earnings by year (or each year’s earnings by quarter), we can see this pattern more plainly, where Q3 usually brings very strong earnings for J&J, while Q4 brings weaker earnings:\n\nmonthplot(eps_boxcox_detrend,ylab='Transformed EPS')\n\n\n\n\n\n\n\nseasonplot(eps_boxcox_detrend,year.labels.left=TRUE,type='l',ylab='Transformed EPS',\n           col=colorRampPalette(c('#ff0000','#bbbbbb','#0000ff'))(21))\n\n\n\n\n\n\n\n\nWe see now why the data failed our stationarity tests: the strong seasonal effects lead the mean of the serties to shift predictably in different quarters. If not every observation has the same predicted mean, then the data cannot be weakly stationary.\nWe will learn more elegant ways of modeling seasonality, but for now we could fall back on a tool we already know: linear regression. Rather than simply de-trending over time, we can add a linear time trend as well as quarterly effect dummies. The residuals from this OLS regression should be more stationary:\n\neps_boxcox_detrend_deseason &lt;- ts(\n  lm(log(JohnsonJohnson) ~ time(JohnsonJohnson) + factor(cycle(JohnsonJohnson)))$residuals,\n  start=1960,frequency=4)\n\nadf.test(eps_boxcox_detrend_deseason)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  eps_boxcox_detrend_deseason\nDickey-Fuller = -1.1176, Lag order = 4, p-value = 0.9144\nalternative hypothesis: stationary\n\nkpss.test(eps_boxcox_detrend_deseason,null='Trend')\n\nWarning in kpss.test(eps_boxcox_detrend_deseason, null = \"Trend\"): p-value\nsmaller than printed p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  eps_boxcox_detrend_deseason\nKPSS Trend = 0.25394, Truncation lag parameter = 3, p-value = 0.01\n\npar(mfrow=c(1,2))\nplot(eps_boxcox_detrend_deseason,\n     main='De-trended and de-seasoned data',ylab='Transformed EPS ($)')\nacf(eps_boxcox_detrend_deseason,\n    main='ACF of de-trended and de-seasoned data',xlab='Lag (in years)')\n\n\n\n\n\n\n\n\nAlthough the de-seasoned data are more stationary than ever, ADF and KPSS tests agree that the remaining stochastic signal is not yet fully stationary. We see from a simple plot of the series that there were good years (1960, the early 1970s) and bad years (the mid 1960s, 1980), patterns which were previously hidden from us.\nUntil we learn better tools, we will not be able to fully decompose this EPS series into a deterministic model and random error. However, even this stumbling block represents progress, because we are starting to see signal through the noise.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  },
  {
    "objectID": "codingbasicsinr.html#footnotes",
    "href": "codingbasicsinr.html#footnotes",
    "title": "Coding it up in R",
    "section": "",
    "text": "Note that both test outputs include an error message — because the p-values are interpolated from tables found in textbooks, and the test statistics for our data are outside of the table ranges, the ‘true’ p-value for the AFD test is more than 0.99 and the ‘true’ p-value for the KPSS test is less than 0.01. Nothing here has actually gone wrong.↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding it up in R</span>"
    ]
  }
]