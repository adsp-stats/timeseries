[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "",
    "text": "Introduction\nHello! I’m Jonathan, and I want to teach you time series analysis I have designed this website to accompany the University of Chicago course ADSP 31006 “Time Series Analysis and Forecasting”. Although the course will mostly be taught in a traditional lecture format, I would like to use this website to provide backing material for the lectures.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-referencetexts",
    "href": "index.html#sec-referencetexts",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "Recommended textbooks",
    "text": "Recommended textbooks\nUnlike my statistics course website, I do not intend that this course website be considered the principle text for the class. We will have two primary textbooks (described below), and the notes in these pages supplement the readings without replacing the readings.\n\nForecasting: Principles and Practice 3rd edition, by Rob Hyndman and George Athanasopoulos\nThis book is freely available and one of our two major reference sources. The book relies heavily on R code using the tidyverse constellation of packages and the new fable package written by the authors, which interleaves specifically with the tsibble package. Readers who prefer base R may instead read the book’s second edition (also freely available), and readers who prefer Python will benefit from the new Python edition (also freely available).\nPractical Time Series Analysis, by Aileen Nielsen\nThis is the second of our two major reference sources. In contrast to the Hyndman and Athanasopoulos textbook, Nielsen includes more on-the-job tips about working with real datasets, structuring and storing the data, and putting forecasting models into production. She uses a combination of R (using the less-common data.table environment) and Python examples, and describes time series analysis from both a statistical and a machine learning perspective.\nTime Series Analysis: Forecasting and Control 5th edition, by George Box, et al.\nOne of the classic texts in the field. Box and his co-authors have introduced and refined (over several decades) the best all-purpose textbook from a purely statistical perspective. They spend less time discussing programming considerations or machine learning models.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-these-notes-were-made",
    "href": "index.html#how-these-notes-were-made",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "How these notes were made",
    "text": "How these notes were made\nI assembled these notes using Quarto, a publishing system built around the Pandoc markdown language. I wrote all the code backing these notes in R, and alongside every figure or table you can find the corresponding R code.\nNeither the text nor the R code in these notes were generated by AI tools: for better and worse the opinions expressed here are my own, and the I’ve described these concepts in my own voice.1 Complaints can be submitted here.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Forecasting Essentials and Notes: Online Resource",
    "section": "",
    "text": "AI assistance was used to brainstorm case studies and examples, and to help with the layout and coding of the website itself.↩︎",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "whatisatimeseries.html",
    "href": "whatisatimeseries.html",
    "title": "What is a time series?",
    "section": "",
    "text": "Notation\nData frequently comes to us in a pile, an unordered heap of observations:\nAll of these observations can be matched to a specific date and time:\nTime-based information might be helpful in understanding these data:\nAnd yet, these orders and timestamps are not required for an understanding of the data. We are not observing one process over time, we are observing many different processes which happen to be “sampled” in a particular order. In a different world, they could easily have been placed in a different order.\nContrast this to a different set of data series:\nThe order of observations within each of these datasets matters. They would tell very different stories if presented out of order. Knowing one observation (one month’s unemployment, one split second’s temperature, one week’s album sales) severely constrains the likely or possible values for the next observation. In linear regression we would view the flow of information across observations as an undesired bug; in these datasets, serial correlation is a feature.\nData scientists do not have a standard definition for time series data. I will attempt a working definition, useful for our purposes but not meant to invalidate other definitions:\nIn this definition I have avoided any reference to statistical inference such as expectation, probability, distributions of random variables, etc. Of course we can use statistical models to study time series data, but we can also use machine learning models or naive models which make no assumptions about the data generating process.\nThis definition is very broad, but we only have ten weeks together, and we will need to define what is in-scope and out-of-scope for this course:\nBecause we will be focusing on a narrow subset of time series processes, we can afford to be a little loose with our notation. In other textbooks you may see more complex representations, meant to flexibly extend to irregularly-observed or continuous time series. Instead, we will adopt the following conventions:",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a time series?</span>"
    ]
  },
  {
    "objectID": "whatisatimeseries.html#notation",
    "href": "whatisatimeseries.html#notation",
    "title": "What is a time series?",
    "section": "",
    "text": "\\(T\\) is the time index. All observations of all time series will happen at times \\(T \\in \\mathbb{N} = \\{0, 1, 2, \\ldots\\}\\). Most samples will start with \\(T=1\\), but we sometimes have a use for the zero-period \\(T=0\\) (e.g. to initialize a series). Arbitrary time indices will generally be represented by \\(T=t\\), and a pair of time indices will generally be represented by \\(T=s\\) and \\(T=t\\).\n\\(\\boldsymbol{Y}\\) is a time series process, a theoretical number-generating sequence observed at regular intervals. It is an ordered collection of random variables.\n\\(Y_1, Y_2, \\ldots, Y_n\\) are the random variables formed by the observation of \\(\\boldsymbol{Y}\\) at time index \\(T = 1, 2, \\ldots, n\\). Each \\(Y_t\\) is itself a random variable.\n\\(\\boldsymbol{y}\\) is a finite sample taken from the process \\(\\boldsymbol{Y}\\). Frequently, \\(\\boldsymbol{y}\\) is the dataset in front of us.\n\\(y_1, y_2, \\ldots, y_n\\) are the specific observations from the sample \\(\\boldsymbol{y}\\) at time index \\(T = 1, 2, \\ldots, n\\).\nIf we need another time series, we can use X: \\(\\boldsymbol{X} = X_1, X_2, \\ldots, X_n\\) is the generating process and its random variables, while \\(\\boldsymbol{x} = x_1, x_2, \\ldots, x_n\\) is the sample.\nWhen referencing a single random variable with no time-varying component, we will use unbolded uppercase letters without a subscript: \\(Z \\sim \\textrm{Normal}(\\mu, \\sigma^2)\\), \\(U \\sim \\textrm{Uniform}(0,1)\\), etc.\nLowercase omega will always be reserved for a white noise process: \\(\\boldsymbol{\\omega} = \\omega_1, \\omega_2, \\ldots \\omega_n\\). These processes are usually unobserved but if we do need to describe a sample (e.g. for a simulation), we may use \\(\\boldsymbol{w} = w_1, w_2, \\ldots, w_n\\).2",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a time series?</span>"
    ]
  },
  {
    "objectID": "whatisatimeseries.html#footnotes",
    "href": "whatisatimeseries.html#footnotes",
    "title": "What is a time series?",
    "section": "",
    "text": "At least conceptually — e.g. while some years are 365 days and others are 366 days, yearly data is still considered regular.↩︎\nSimilar to how the OLS residuals \\(\\boldsymbol{e} = e_1, e_2, \\ldots, e_n\\) are estimated realizations of the theoretical error process \\(\\boldsymbol{\\varepsilon} = \\varepsilon_1, \\varepsilon_2, \\ldots, \\varepsilon_n\\).↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a time series?</span>"
    ]
  },
  {
    "objectID": "autocovariance.html",
    "href": "autocovariance.html",
    "title": "Autocovariance and autocorrelation",
    "section": "",
    "text": "Review: covariance and correlation\nIn previous courses, you would probably have learned about covariance and correlation. The covariance of two random variables measures the raw amount by which they move together or separately.\n\\[\\textrm{Cov}(Y,X) = \\mathbb{E}[(Y - \\mu_Y)(X - \\mu_X)]\\]\nCovariance is an important metric for a lot of theoretical calculations, but it’s difficult for humans to work with because it’s very sensitive to the units in which \\(X\\) and \\(Y\\) are measured. For this reason, we often prefer to summarize the relation between two random variables with the Pearson correlation coefficient,1 which normalizes the covariance by the respective standard deviations of \\(X\\) and \\(Y\\).\n\\[\\mathrm{Cor}(Y,X) = \\rho_{YX} = \\frac{\\textrm{Cov}(Y,X)}{\\sigma_Y \\cdot \\sigma_X}\\]\nCorrelation is very convenient to work with, because it’s a unitless measure which is always bounded between -1 and +1. We can easily describe the strength of the linear relationship between two variables in ways which are directly comparable to other pairs of variables.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#review-estimating-covariance-and-correlation-from-a-sample",
    "href": "autocovariance.html#review-estimating-covariance-and-correlation-from-a-sample",
    "title": "Autocovariance and autocorrelation",
    "section": "Review: Estimating covariance and correlation from a sample",
    "text": "Review: Estimating covariance and correlation from a sample\nCovariance and correlation are two properties that describe the theoretical relationship between two random variables. When we work from samples of data, we don’t always know the true covariance or correlation, and we need to estimate these metrics from our samples. Unbiased estimators for both metrics can be found below:\n\\[\\textrm{Sample covariance:} \\quad S_{\\boldsymbol{yx}} = \\frac{1}{n-1} \\sum_i (y_i - \\bar{y})(x_i - \\bar{x})\\]\n\\[\\textrm{Sample correlation:} \\quad r_{\\boldsymbol{yx}} = \\frac{\\sum_i (y_i - \\bar{y})(x_i - \\bar{x})}{\\sqrt{\\sum_i (y_i - \\bar{y})^2} \\sqrt{\\sum_i (x_i - \\bar{x})^2}}\\]",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#autocovariance-and-autocorrelation",
    "href": "autocovariance.html#autocovariance-and-autocorrelation",
    "title": "Autocovariance and autocorrelation",
    "section": "Autocovariance and autocorrelation",
    "text": "Autocovariance and autocorrelation\nThese two measures have very close counterparts in time series analysis.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series observed at regular time periods \\(T = \\{1,2,\\ldots,n\\}\\) and denote the mean and variance of the random variable at each time index \\(t\\) as \\(\\mathbb{E}[Y_t] = \\mu_t\\) and \\(\\mathbb{V}(Y_t) = \\sigma^2_t\\). Then, for any two time indices \\(s,t \\in T\\), the autocovariance between \\(Y_s\\) and \\(Y_t\\) is defined as:\n\\[\\textrm{Cov}(Y_s,Y_t) = \\gamma_{s,t}  = \\mathbb{E}[(Y_s - \\mu_s)(Y_t - \\mu_t)]\\]\nAnd the autocorrelation between \\(Y_s\\) and \\(Y_t\\) is defined as:\n\\[\\textrm{Cor}(Y_s,Y_t) = \\rho_{s,t}  = \\frac{\\textrm{Cov}(Y_s,Y_t)}{\\sigma_s \\cdot \\sigma_t}\\]\n\n\nIf a time series is weakly stationary, then its mean and standard deviation are the same at every time period, and the autocovariance (and autocorrelation) will only depend on the lag between the two time periods:\n\n\n\n\n\n\nWarningOnly when Y is weakly stationary\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a weakly stationary time series observed at regular time periods \\(T = \\{1,2,\\ldots,n\\}\\) with mean \\(\\mathbb{E}[Y_t] = \\mu\\) and variance \\(\\mathbb{V}[Y_t] = \\sigma^2\\) for all \\(t \\in T\\). Then, the autocovariance between any two observations of the series \\(Y_t\\) and \\(Y_{t+k}\\) (a second random variable observed \\(k\\) periods later) is defined as:\n\\[\\begin{aligned} \\textrm{Cov}(Y_t,Y_{t+k}) = \\gamma_{k}  &= \\mathbb{E}[(Y_t - \\mu)(Y_{t+k} - \\mu)] \\\\ &= \\mathbb{E}[Y_t \\cdot Y_{t+k}] - \\mu^2 \\end{aligned}\\]\nNote that each covariance between \\(Y_t\\) and \\(Y_{t+k}\\) will be equal to \\(\\gamma_k\\) regardless of the time index t. And the autocorrelation between \\(Y_t\\) and \\(Y_{t+k}\\) is defined as:\n\\[\\textrm{Cor}(Y_t,Y_{t+k}) = \\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\frac{\\textrm{Cov}(Y_t,Y_{t+k})}{\\sigma^2}\\]",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#estimating-autocovariance-and-autocorrelation-from-a-sample",
    "href": "autocovariance.html#estimating-autocovariance-and-autocorrelation-from-a-sample",
    "title": "Autocovariance and autocorrelation",
    "section": "Estimating autocovariance and autocorrelation from a sample",
    "text": "Estimating autocovariance and autocorrelation from a sample\nIf our time series is not stationary, then we cannot really estimate the autocovariance \\(\\gamma_{s,t}\\) or the autocorrelation \\(\\rho_{s,t}\\) from a sample, because there will only be one pair of values to observe at those time periods. However, if our time series is stationary, then we can estimate \\(\\gamma_k\\) and \\(\\rho_k\\) from all the pairs of observations which are \\(k\\) time periods apart.\nThe exact calculation method for estimating autocovariance varies from source to source. All of the popular methods involve bias-variance compromises. The R software environment generally uses the following definition for autocorrelation:2\n\\[r_k = \\hat{\\rho}_k = \\frac{c_k}{c_0}\\]\n\\[c_k = \\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=1}^{n-k}(y_t - \\bar{y})(y_{t+k} - \\bar{y})\\]\nIn some sources the terms autocovariance and autocorrelation are used synonymously, while in other sources those words are used for the two quantities \\(\\gamma\\) and \\(\\rho\\) respectively, where \\(\\rho\\) is normalized to bounds between -1 and +1. In this course, we will always observe the difference between the two terms.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocovariance.html#footnotes",
    "href": "autocovariance.html#footnotes",
    "title": "Autocovariance and autocorrelation",
    "section": "",
    "text": "There are other named correlation coefficients, but Pearson’s is so widely used that if someone says ‘correlation’, you can assume they mean Pearson’s correlation.↩︎\nNotice that the covariance estimator uses a denominator of \\(n\\) instead ofg \\(n-k\\), which will introduce bias but generally lower MSE.↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autocovariance and autocorrelation</span>"
    ]
  },
  {
    "objectID": "whitenoise.html",
    "href": "whitenoise.html",
    "title": "White noise",
    "section": "",
    "text": "Motivation\nImagine observing a signal with a strong periodicity, like a sine wave. We could depict that signal as a waveform in the time domain (the world we experience). We could also depict that signal as a peak in the frequency domain (i.e. the spectral density). Let’s suppose that the signal repeats every 100 time-units, for a frequency of 0.01:\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny1 &lt;- sin((1:1200)*2*pi/100)\nplot(y1,type='b',pch=20,xlim=c(1,200))\nspectrum(y1,log='no',xlim=c(0,0.2))\n\n\n\n\n\nSingle waveform viewed in the time domain and the frequency domain\nNow imagine a more complicated signal, a composite of our first signal and a new signal which repeats every 20 units, creating a new spectral density peak at 0.05:\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny2 &lt;- sin((1:1200)*2*pi/20)\nplot(y1+y2,type='b',pch=20,xlim=c(1,200))\nspectrum(y1+y2,log='no',xlim=c(0,0.2))\n\n\n\n\n\nDouble waveform viewed in the time domain and the frequency domain\nWe can continue to add more peaks to the spectral density, for example by adding a new waveform with a period every 8 observations (for a frequency of 0.125):\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny3 &lt;- sin((1:1200)*2*pi/8)\nplot(y1+y2+y3,type='b',pch=20,xlim=c(1,200))\nspectrum(y1+y2+y3,log='no',xlim=c(0,0.2))\n\n\n\n\n\nTreble waveform viewed in the time domain and the frequency domain\nLet’s take this to the limit case: what if the spectral density were equally powerful at every frequency? We call this white noise:\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\ny4 &lt;- rnorm(100000)\nplot(y4,type='b',pch=20,xlim=c(1,200))\nspectrum(y4,log='no',xlim=c(0,0.2))\n\n\n\n\n\nStandard normal data viewed in the time domain and the frequency domain\nWhite noise refers to any time-domain generating process which has uniform power across the frequency domain. It has no connection to anyone named White1, and instead is a metaphor for “white light” — light which is equally strong across all wavelength frequencies in the visible part of the electomagnetic spectrum.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "whitenoise.html#definition",
    "href": "whitenoise.html#definition",
    "title": "White noise",
    "section": "Definition",
    "text": "Definition\nMany different series can be white noise. Choosing either -1 or +1 with equal and independent probability would be a series of white noise. White noise must have the following properties in the time domain:\n\nThe observations must show no serial correlation, and in fact most definitions require that they be statistically independent of each other.2\nThe observations must have mean 0.\nThe observations must have a finite variance.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "whitenoise.html#additive-gaussian-white-noise",
    "href": "whitenoise.html#additive-gaussian-white-noise",
    "title": "White noise",
    "section": "Additive Gaussian white noise",
    "text": "Additive Gaussian white noise\nAlthough many series can be white noise, some of the models we will study in this course assume that the white noise takes a specific form, where each observation is drawn from an IID Normal distribution with mean zero and constant variance. In other words, the error process we normally3 associate with a linear regression is also white noise, and is often used to describe the error process of time series models. (The Normal distribution is also called the Gaussian distribution.)\nAn individual value from a white noise process is sometimes called an “innovation” or a “random shock” instead of an “error”. We will generally use the term “innovation” in this course, rather than “error”, to emphasize that the shocks are not wrong or misleading in any sense, they are simply unknown until the moment they happen; they are new discoveries that change the status quo.\nTo reinforce this perspective, white noise innovations in a time series model are generally denoted as \\(\\omega_1, \\omega_2, \\ldots, \\omega_n\\) and not as \\(\\varepsilon_1, \\varepsilon_2, \\ldots, \\varepsilon_n\\). As a mnemonic, you could say they are “w for white noise, not e for error”.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{\\omega}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). If \\(\\omega_t \\sim \\textrm{Normal}(0,\\sigma^2)\\) for all \\(t \\in T\\) and \\(\\omega_s \\perp \\omega_t\\) for all \\(s,t \\in T\\), then the generating process \\(\\boldsymbol{\\omega}\\) is said to be additive Gaussian white noise",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "whitenoise.html#footnotes",
    "href": "whitenoise.html#footnotes",
    "title": "White noise",
    "section": "",
    "text": "E.g., the econometrician Halbert White for whom we name White standard errors and the White test for heteroskedasticity.↩︎\nIndependent does imply uncorrelated, but not all uncorrelated variables are independent.↩︎\nNo pun intended!↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>White noise</span>"
    ]
  },
  {
    "objectID": "randomwalk.html",
    "href": "randomwalk.html",
    "title": "Random walks",
    "section": "",
    "text": "Motivation\nBefore learning about time series which we can usefully study or forecast, it might help to see an example of a times series we cannot usefully study or forecast: the random walk.\nCode\nset.seed(1999)\nw &lt;- rnorm(100)\nrw &lt;- cumsum(w)\nplot(rw,type='b',pch=20,xlab='Index',ylab='Measurement')\n\n\n\n\n\n100 observations of a random walk\nThis times series seems to carry information. We see a positive trend. Perhaps we see some evidence of periodicity/seasonality. Sadly, we are mistaken: what we are looking at instead is a random walk.\nRandom walks refer to memoryless time series processes which “drift” randomly across 1, 2, or more dimensions. They may return to their starting value, but do not do so predictably. Let’s examine five possible futures that the random walk above might take — each of these are completely consistent evolutions of the series, each as likely as the other.\nCode\nset.seed(1999)\nw2 &lt;- rbind(cbind(w,w,w,w,w),matrix(rnorm(500),ncol=5))\nrw2 &lt;- apply(w2,2,cumsum)\nplot(rw,type='l',lwd=2,xlab='Index',xlim=c(1,200),ylab='Measurement',ylim=c(-5,25))\nmatplot(x=101:200,y=rw2[101:200,],type='l',lwd=2,pch=20,add=TRUE,lty=1,\n        col=colorRampPalette(c('#0000ff','#bbbbbb','#ff0000'))(5))\n\n\n\n\n\nFive possible extensions of the same random walk\nRandom walks can be one-dimensional, like the example above, or multidimensional. The time between steps can be regular, irregular, or even continuous. The step sizes themselves can be equal, discrete, or continuous. (Random walks are, for example, closely tied to the idea of ‘Brownian motion’, which is the drift through space and time of tiny particles suspended in air or fluid, such as smoke, or dust in a sunbeam.)\nBelow is an example of a two-dimensional random walk where each step size is one unit.\nCode\nnw &lt;- 2000\nrwcols &lt;- colorRampPalette(c('#0000ff','#bbbbbb','#ff0000'))(nw)\nset.seed(1235)\nrwx &lt;- cumsum(sample(c(-1,1),nw,TRUE))\nrwy &lt;- cumsum(sample(c(-1,1),nw,TRUE))\nplot(rwx,rwy,type='n',asp=1,,axes=FALSE,xlab=NA,ylab=NA,main=NA,sub=NA)\naxis(1,at=c(-40,-20,0)); axis(2,at=c(-20,0,20))\nsegments(x0=c(0,rwx[-nw]),x1=rwx,y0=c(0,rwy[-nw]),y1=rwy,col=rwcols)\npoints(c(0,rwx[nw]),c(0,rwy[nw]),col=rwcols[c(1,nw)],pch=19)\nlegend(x='bottomleft',legend=c('Start (t=0)','End (t=2000)'),\n       pch=19,col=rwcols[c(1,nw)],bty='n',cex=0.8)      \n\n\n\n\n\n2000 steps of a 2D binary random walk.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random walks</span>"
    ]
  },
  {
    "objectID": "randomwalk.html#definition",
    "href": "randomwalk.html#definition",
    "title": "Random walks",
    "section": "Definition",
    "text": "Definition\nAlthough random walks appear in many forms, we will mostly concern ourselves in this course with a specific, one-dimensional walk observed in regular time intervals.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{\\omega}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\), where the distribution of each random observation \\(\\omega_t\\) is independently and identically distributed with mean 0 and finite variance \\(\\sigma^2\\) for all \\(t \\in T\\). Define a new time series random variable \\(\\boldsymbol{Y} = Y_1, Y_2, \\ldots, Y_n\\) as follows:\n\\[\\begin{aligned} Y_1 &= \\omega_1 \\\\ Y_t &= Y_{t-1} + \\omega_t \\quad \\forall t&gt;1\\end{aligned}\\]\nThen \\(\\boldsymbol{Y}\\) is a random walk, specifically a homogenous discrete-time random walk in one dimension. If \\(\\omega_t \\stackrel{iid}{\\sim} \\textrm{Normal}(0,\\sigma^2) \\; \\forall t \\in T\\), then we say that \\(\\boldsymbol{Y}\\) is a Gaussian random walk.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random walks</span>"
    ]
  },
  {
    "objectID": "randomwalk.html#properties",
    "href": "randomwalk.html#properties",
    "title": "Random walks",
    "section": "Properties",
    "text": "Properties\nNote that a Gaussian random walk is equivalent to the cumulative sum of a Gaussian white noise process, and that more generally any the cumulative sum of any IID white noise process is some type of random walk.\nConsider the implications of these two calculations:\n\\[\\mathbb{E}[Y_t] = \\mathbb{E}[\\sum_{i=1}^t \\omega_i] = \\sum_{i=1}^t \\mathbb{E}[\\omega_i] = \\sum_{i=1}^t 0 = 0\\]\n\\[\\mathbb{V}[Y_t] = \\mathbb{V}[\\sum_{i=1}^t \\omega_i] = \\sum_{i=1}^t \\mathbb{V}[\\omega_i] = \\sum_{i=1}^t \\sigma^2 = t\\sigma^2\\]\nTaken together, we see that while the expectation or the prediction for the future location of a random walk remains its starting place, the variance increases proportionally with the time period, meaning that the range of likely values grows wider and wider, and that the series becomes quite unlikely to be found at the starting point itself, even though this is technically the “average” outcome.\nNote also that the expectation of a random walk given its past history has nothing to do with its starting point or how long it has run, but simply wherever it was last observed. Consider trying to forecast the value of a random walk at time \\(t\\) given a series of observations ending at time \\(s \\lt t\\):\n\\[\\begin{aligned} \\mathbb{E}[Y_t|y_1,\\ldots,y_s] &= \\mathbb{E}[(Y_s + \\sum_{i=s+1}^t \\omega_i)|y_1,\\ldots,y_s] \\\\ &= y_s + \\mathbb{E}[\\sum_{i=s+1}^t \\omega_i] = y_s + \\sum_{i=s+1}^t \\mathbb{E}[\\omega_i] \\\\ &= y_s + \\sum_{i=s+1}^t 0 = y_s \\end{aligned}\\]",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random walks</span>"
    ]
  },
  {
    "objectID": "stationarity.html",
    "href": "stationarity.html",
    "title": "Stationarity",
    "section": "",
    "text": "Motivation\nWe can’t perfectly forecast everything (or anything!), but perhaps we can draw a meaningful distinction between two different scenarios:\nIt would be nice to define these two scenarios more precisely, and to develop a test which can help us to identify which scenario we face, and perhaps even to develop techniques which transform the first scenario into the second.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#motivation",
    "href": "stationarity.html#motivation",
    "title": "Stationarity",
    "section": "",
    "text": "Scenario 1: The time series is inherently unpredictable. We may be able to guess that the next observation will be somewhere near the current observation, but further into the future we have no idea “where to look” for the time series.\nScenario 2: The exact values of the time series may be random, but the likely future values of the time series are estimable and the further into the future we look, the more consistent our estimates will be.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#definitions",
    "href": "stationarity.html#definitions",
    "title": "Stationarity",
    "section": "Definitions",
    "text": "Definitions\nWe will call the concept discussed in Scenario 2 above stationarity, which is meant to suggest that the time series “stays in place” over time. It may fluctuate, perhaps severely, but will always stay in the neighborhood of “home base”.\nThere are two common definitions of stationarity: we say that a time series is either strongly stationary or weakly stationary (or, of course, nonstationary).\n\nStrong stationarity\nWhen a time series is strongly stationary, its joint unconditional distribution does not depend on the time index. This is a bold assertion and difficult to test, but when true it unlocks many theoretical results and properties.1\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(F_\\boldsymbol{Y}\\) be the joint cumulative distribution function of a set of random observations \\(Y_1, Y_2, ...\\) from the time series \\(\\boldsymbol{Y}\\). Then if\n\\[F_\\boldsymbol{Y}(Y_1 \\le y_1, \\ldots, Y_n \\le y_n) = F_\\boldsymbol{Y}(Y_{1+k} \\le y_1, \\ldots, Y_{n+k} \\le y_n)\\]\nFor all \\(k \\in \\mathbb{N}\\) and all \\(n \\in \\mathbb{N}\\) and all real values of \\(y_1, \\ldots, y_n\\), then we say that the time series \\(\\boldsymbol{Y}\\) exhibits strong stationarity.\n\n\nEssentially, until we begin to actually observe values from a strongly stationary process, the probability distribution of all future points is the same, and any serial dependence between a set of observations is shared between all sets of observations with the same relative time-orderings. (For example, \\(F_\\boldsymbol{Y}(Y_3|Y_1=c)\\) = \\(F_\\boldsymbol{Y}(Y_8|Y_6=c)\\).)\n\n\nWeak stationarity\nKnowing or estimating the full joint distribution of a set of time series observations seems like a lot of work. For many purposes, we may accept a slightly looser definition of a stationary process without losing the mathematical results and guarantees we need to perform our analysis.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(F_\\boldsymbol{Y}\\) be the joint cumulative distribution function of a set of random observations \\(Y_1, Y_2, ...\\) from the time series \\(\\boldsymbol{Y}\\). Then if\n\\[\\begin{aligned} (1) & \\quad \\mathbb{E}[Y_t] = \\mu \\qquad \\forall \\, t \\in T \\\\ \\\\ (2) & \\quad \\mathbb{V}[Y_t] = \\sigma^2 \\lt \\infty \\qquad \\forall \\, t \\in T \\\\ \\\\ (3) & \\quad \\textrm{Cov}(Y_s,Y_t) = \\gamma_{|t-s|} \\qquad \\forall \\, s,t \\in T\\end{aligned}\\]\nThen we say that the time series \\(\\boldsymbol{Y}\\) exhibits weak stationarity.\n\n\nWeak stationarity makes fairly straightforward claims about the first and second moments of \\(F_\\boldsymbol{Y}\\) rather than trying to define its entire joint distribution. Constant mean, constant and finite variance, and an autocovariance function which depends only on the time interval between two random observations.\n\n\nNon-nested definitions\nConfusingly, neither strong stationarity nor weak stationarity imply the other. While it is often true that strongly stationary processes are also weakly stationary, exceptions can be found:\n\nIf \\(Y_t \\stackrel{iid}{\\sim} \\textrm{Cauchy}\\) for all \\(t \\in T\\) then the variance of each observation \\(Y_t\\) and the autocovariance between observations \\(Y_s\\) and \\(Y_t\\) will be undefined or infinite, meaning that \\(\\boldsymbol{Y}\\) cannot be weakly stationary — but since the distribution is known and identical across all time indices, \\(\\boldsymbol{Y}\\) is strongly stationary.\nIf \\(Y_t \\stackrel{iid}{\\sim} \\textrm{Exponential}(1)\\) for odd values of \\(t\\) and \\(Y_t \\stackrel{iid}{\\sim} \\textrm{Poisson}(1)\\) for even values of \\(t\\), then the mean and variance of each observation \\(Y_t\\) would still be 1, and the covariance between the (independent) observations would be 0, meaning that the covariance does not depend on the time indices: \\(\\boldsymbol{Y}\\) would be weakly stationary. However, since (for example) \\(F_\\boldsymbol{Y}(Y_1,Y_2) \\ne F_\\boldsymbol{Y}(Y_2,Y_3)\\), the process is not strongly stationary.2\n\nHowever, these are edge cases. Importantly for our future work, a Gaussian white noise process is both strongly and weakly stationary.\nEqually important, a random walk is never stationary.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#recovering-stationarity-from-non-stationary-processes",
    "href": "stationarity.html#recovering-stationarity-from-non-stationary-processes",
    "title": "Stationarity",
    "section": "Recovering stationarity from non-stationary processes",
    "text": "Recovering stationarity from non-stationary processes\nAlthough most time series are not stationary, we can often apply simple transformations to regain stationary behavior. I will quickly describe three examples below:\n\nTrend stationarity\nSometimes a process is observed with a deterministic trend, that is, a steady drift of the mean away from its initial condition. If the drift in each period \\(t\\) is nonrandom, and known or estimable, we can remove the trend and the “de-trended” series which remains may be stationary.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(\\mathbb{E}[Y_t] = \\mu + \\delta t\\), let \\(\\mathbb{V}[Y_t] = \\sigma^2 \\lt \\infty\\), and let \\(\\textrm{Cov}(Y_s,Y_t) = \\gamma_{|t-s|}\\) for all \\(s,t \\in T\\).\nThen \\(\\boldsymbol{Y}\\) is a trend-stationary time series, and if \\(X_t = Y_t - \\delta t\\) then \\(\\boldsymbol{X}\\) is a weakly stationary time series.\n\n\nThe intuition here is quite simple: if we know or can estimate a non-random component of our time series which moves the mean (destroying stationary), then we can simply remove it when we want to study the random time series behavior and add it back in when we need to perform prediction.\n\n\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\nset.seed(1229)\ntsts &lt;- 70 + 3*arima.sim(list(ar=c(0.5,0.25)),n=100) - 0.4*(1:100)\nplot(tsts,ylab='Y_t',main='Original data series')\nabline(70,-0.4,col='#bbbbbb')\nplot(tsts +  0.4*(1:100),ylab='Y_t - 0.4t',main='Detrended data series')\n\n\n\n\n\nTrend-stationary process with and without detrending\n\n\n\n\nAlthough the deterministic trend described here is additive, this concept generalizes to multiplicative trends or other situations, such as a variance which shrinks or grows at a steady rate over time.\n\n\nDifference stationarity\nWhen a time series drifts away from its mean through a stochastic (i.e. random) process, then simply “tilting” the series by removing a linear trendline will not be enough to restore stationarity.\nIn some cases, the differences between the values of a nonstationary series can themselves be a stationary series. The most well-known example of this is a random walk. Since the random walk is created by cumulating a white noise series, the difference of the random walk is simply the same white noise series, and a white noise series is stationary.\nWe now have cause to introduce the backward difference operator \\(\\nabla\\).\n\\[\\nabla Y_t = Y_t - Y_{t-1}\\]\n\\[\\nabla_{\\!k} Y_t = Y_t - Y_{t-k}\\]\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Let \\(\\mathbb{E}[\\nabla Y_t] = \\mu\\), let \\(\\mathbb{V}[\\nabla Y_t] = \\sigma^2 \\lt \\infty\\), and let \\(\\textrm{Cov}(\\nabla Y_s,\\nabla Y_t) = \\gamma_{|t-s|}\\) for all \\(s,t \\in T\\).\nThen \\(\\boldsymbol{Y}\\) is a difference-stationary time series, and if \\(X_t = \\nabla Y_t\\) then \\(\\boldsymbol{X}\\) is a weakly stationary time series.\n\n\nMany real-life processes are essentially random walks and their levels are difficult to study, but their differences (additive or proportional) are useful targets for our analysis. One example woudl be stock prices: the growth of a company’s value over time, combined with the changing scale caused by inflation, and the complications caused by stock splits, dividends, new share issuances, and stock buybacks all combine to make the price of a stock relatively uninteresting to market analysts. However, the percentage price return, or daily proportional change, is a fundamental unit of analysis which we may model as being trend-stationary (the deterministic trend being the risk-free rate).\nKnowing when to de-trend and when to difference is important. A random walk may appear to have a roughly linear trend, but the de-trended series will still be nonstationary:\n\n\nCode\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\nn &lt;- 200\nset.seed(1230)\nrw &lt;- cumsum(rnorm(n))\nplot(rw,ylab='Y_t',main='Original data series',type='l')\nlines(x=c(1,n),y=c(rw[1],rw[n]),col='#bbbbbb')\nplot(rw-(1:n)*(rw[n]-rw[1])/(n-1),ylab='Y_t - Avg Drift',main='Detrended data series',type='l')\n\n\n\n\n\nRandom walks de-trended (top) and differenced (bottom)\n\n\n\n\nCode\nplot(rw,ylab='Y_t',main='Original data series',type='l')\nsegments(x0=2:n,x1=2:n,y0=0,y1=diff(rw),col='#bbbbbb')\nplot(diff(rw),ylab='First difference of Y_t',main='Differenced data series',type='l')\n\n\n\n\n\nRandom walks de-trended (top) and differenced (bottom)\n\n\n\n\n\n\nThe Box-Cox transformation\nIn still other cases, the problem with achieving stationarity is that some growth or shrinkage over time effectively places different parts of the time series on different scales. The S&P500 index did not move by more than 5 units per day for its first 20 years… nowadays, it moves by more than 5 almost every day. Its starting value was near 50… nowadays, its value is near 5000. Both the mean and the variance have shifted over time. A special class of transformation can bring series like this closer to stationarity:\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots\\}\\). Define \\(\\boldsymbol{Y}^{(\\lambda)}\\), the Box-Cox transformation of \\(\\boldsymbol{Y}\\) as follows, for any choice of parameter \\(\\lambda \\in \\mathbb{R}\\):\n\\[Y_t^{(\\lambda)} = \\left\\{\\begin{array}{ll} \\log Y_t & \\textrm{if}\\; \\lambda = 0 \\\\ \\frac{Y_t^\\lambda - 1}{\\lambda} & \\textrm{if}\\; \\lambda \\ne 0 \\end{array}\\right\\}\\]\n\n\nThe exact parameter \\(\\lambda\\) which brings a nonstationary series closest to stationarity must be estimated through algorithmic means, including (but not limited to) maximum likelihood estimation. As an example below, a Box-Cox transformation with a parameter of 0.1 takes this highly nonstationary airline data and transforms it into something much closer to trend-stationarity.3\n\n\nCode\ndata(tcm)\nyields &lt;- window(tcm1y,end=1982.24)\npar(mfrow=c(1,2),mar=c(4, 4, 3, 1))\nplot(yields,ylab='1Y yield',main='Original nonstationary series')\nplot(box_cox(yields,0.15),ylab='Transformed series',main='Box-Cox with lambda=0.15')\nabline(reg=lm(box_cox(yields,0.15)~time(yields)),col='#bbbbbb')\n\n\n\n\n\nBox-Cox transformation of monthly 1Y Treasury yields",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#tests-for-stationarity",
    "href": "stationarity.html#tests-for-stationarity",
    "title": "Stationarity",
    "section": "Tests for stationarity",
    "text": "Tests for stationarity\nHow do we know if a series is stationary? Well, the short answer is, we don’t:\n\nA statistical test would not prove that a series is stationary or nonstationary, only provide some degree of evidence against the null hypothesis.\nEven if a time series seems stationary for the period in which we observe it, we have no guarantee it will remain stationary in the future.\n\nEven so, optimistic statisticians have developed several tests for stationarity. Although the mathematics behind these tests is within the reach of most readers, I will focus on just two tests and discuss how they are used rather than how they are calculated.\n\nThe augmented Dickey-Fuller (ADF) test\nThe Dickey-Fuller test (1979) examines whether a time series might actually be a random walk or stationary (including trend-stationary). The specific model being tested is whether\n\\[\\nabla Y_t = c + \\alpha Y_{t-1} + \\omega_t\\]\nIf \\(\\alpha=0\\) then we have a random walk with a deterministic trend: the step size between every pair of observations of \\(\\boldsymbol{Y}\\) are a constant (creating drift) plus white noise (creating the random walk).\nBecause of this setup, the null hypothesis of \\(\\alpha \\ge 0\\) codes for nonstationarity, while the one-sided alternative hypothesis is stationarity (since negative values of \\(\\alpha\\) create a “rubber band” mean-reverting process).\nThe augmented Dickey-Fuller (ADF) test extends this concept to control for complex autocorrelations in the original data series which might obscure the presence of a random walk. While the ADF test is slightly less powerful than the original Dickey-Fuller test, it’s more widely used today and considered an improvement upon the original.\nWe see that the original air passenger data series shown above fails to display stationarity, while the Box-Cox transformation does show stationarity using an ADF test:\n\nadf.test(yields)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  yields\nDickey-Fuller = -1.7333, Lag order = 7, p-value = 0.6893\nalternative hypothesis: stationary\n\nadf.test(box_cox(yields,lambda=0.15))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  box_cox(yields, lambda = 0.15)\nDickey-Fuller = -3.4646, Lag order = 7, p-value = 0.0463\nalternative hypothesis: stationary\n\n\n\n\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\nThe ADF test is useful and widely accepted, but carries one drawback: even when applied to truly stationary data, it often fails to reject the null hypothesis of a random walk. This created an opening for a newer test which makes stationarity the null hypothesis.\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test proceeds by assuming that every time series can be decomposed into three parts: a deterministic time trend, a stationary error process, and a non-stationary random walk with some unknown variance:\n\\[Y_t = \\delta t + r_t + \\varepsilon_t\\]\nWhere \\(r_t\\) is a random walk with variance \\(\\sigma^2\\), and \\(\\varepsilon_t\\) is a stationary error process.4\nThe KPSS test examines whether the random walk variance could be 0 (in which case it is not really present at all). By testing \\(\\sigma^2 = 0\\), it creates a null hypothesis of stationarity and a one-sided alternative hypothesis that \\(\\boldsymbol{Y}\\) is a random walk.\nWe see that the original air passenger data series shown above fails to display stationarity, while the Box-Cox transformation does show stationarity using a KPSS test:\n\nkpss.test(yields,null='Trend')\n\nWarning in kpss.test(yields, null = \"Trend\"): p-value smaller than printed\np-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  yields\nKPSS Trend = 0.42815, Truncation lag parameter = 5, p-value = 0.01\n\nkpss.test(box_cox(yields,lambda=0.15),null='Trend')\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  box_cox(yields, lambda = 0.15)\nKPSS Trend = 0.14306, Truncation lag parameter = 5, p-value = 0.05544\n\n\nSo what have we gained, since these results are the same as the ADF test results above? Essentially, by using both tests, we can now differentiate between three distinct situations:\n\nCompelling evidence for stationarity: The ADF and KPSS tests agree on stationarity.\nCompelling evidence for a random walk: The ADF and KPSS tests agree on random walk.\nNot enough evidence and power to be sure: The ADF test cannot reject a random walk but the KPSS test cannot reject stationarity.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "stationarity.html#footnotes",
    "href": "stationarity.html#footnotes",
    "title": "Stationarity",
    "section": "",
    "text": "Recall that the definition below is for regularly-observed, one-dimensional discrete time series processes. There are broader definitions of stationarity which generalize to other situations.↩︎\nConsider that \\(Y_1\\) cannot be 0 while \\(Y_2\\) can with probability \\(1/e\\).↩︎\nThe seasonal peaks are nonstationary behavior, but we will learn to model these too in future lesson.↩︎\nWe do not use the notation \\(\\omega_t\\) since the error process need not be white noise.↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stationarity</span>"
    ]
  },
  {
    "objectID": "basicsinr.html",
    "href": "basicsinr.html",
    "title": "Time series basics in R",
    "section": "",
    "text": "Time series functions used in this document\nIn the table below, packages with italicized names will need to be installed, while the package names in a standard font face can be found in most base R distributions (though they may need to be loaded into your workspace).",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time series basics in R</span>"
    ]
  },
  {
    "objectID": "basicsinr.html#time-series-functions-used-in-this-document",
    "href": "basicsinr.html#time-series-functions-used-in-this-document",
    "title": "Time series basics in R",
    "section": "",
    "text": "Time series functions used in the code below\n\n\nPackage\nFunction name\nPurpose\n\n\n\n\nstats\nts\nDefine a time series\n\n\nstats\ntime\nExtract the time indices of a ts\n\n\nstats\nstart\nExtract the first time index\n\n\nstats\nend\nExtract the last time index\n\n\nstats\nwindow\nSubset/downsample a time series\n\n\nstats\narima.sim\nSimulate ARIMA data (including random walks)\n\n\ntseries\nadf.test\nADF stationarity test\n\n\ntseries\nkpss.test\nKPSS stationarity test\n\n\nMASS\nboxcox\nSuggest Box-Cox parameter lambda\n\n\nforecast\nBoxCox\nTransform data with known lambda\n\n\nstats\nacf\nCompute and plot autocorrelation\n\n\nstats\nmonthplot\nPlot annual change across seasons\n\n\nforecast\nseasonplot\nPlot seasonal change across years",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time series basics in R</span>"
    ]
  },
  {
    "objectID": "basicsinr.html#creating-and-storing-time-series-data",
    "href": "basicsinr.html#creating-and-storing-time-series-data",
    "title": "Time series basics in R",
    "section": "Creating and storing time series data",
    "text": "Creating and storing time series data\nDifferent packages have different standards for defining a time series. In many cases, you can pass a simple numeric vector to a time series function: the function will typecast the vector into a time series object and proceed as intended. In other cases, you need to make sure that you have typed the vector as a time series.\nTwo common standards for representing time series are stats::ts from base R and tsibble::tsibble from the tidyverse constellation of packages. This document will use base R conventions and functions.\n\n#define a time series\nsample_ts &lt;- ts(1:18, frequency=4, start=c(1999,2))\n\n#print some basic information and the series values\nprint(sample_ts, calendar=FALSE)\n\nTime Series:\nStart = c(1999, 2) \nEnd = c(2003, 3) \nFrequency = 4 \n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18\n\n#alternate print view (still treated as a vector, not a matrix)\nprint(sample_ts, calendar=TRUE)\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n1999         1    2    3\n2000    4    5    6    7\n2001    8    9   10   11\n2002   12   13   14   15\n2003   16   17   18     \n\n#recover the time index values for each obseration\ntime(sample_ts)\n\n        Qtr1    Qtr2    Qtr3    Qtr4\n1999         1999.25 1999.50 1999.75\n2000 2000.00 2000.25 2000.50 2000.75\n2001 2001.00 2001.25 2001.50 2001.75\n2002 2002.00 2002.25 2002.50 2002.75\n2003 2003.00 2003.25 2003.50        \n\n#recover the first and last time indices\nstart(sample_ts)\n\n[1] 1999    2\n\nend(sample_ts)\n\n[1] 2003    3\n\n#subset the series and optionally downsample to yearly\nwindow(sample_ts,start=2000,end=2002)\n\n     Qtr1 Qtr2 Qtr3 Qtr4\n2000    4    5    6    7\n2001    8    9   10   11\n2002   12               \n\nwindow(sample_ts,start=2000,end=2002,frequency=1)\n\nTime Series:\nStart = 2000 \nEnd = 2002 \nFrequency = 1 \n[1]  4  8 12",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time series basics in R</span>"
    ]
  },
  {
    "objectID": "basicsinr.html#creating-a-random-walk-in-r",
    "href": "basicsinr.html#creating-a-random-walk-in-r",
    "title": "Time series basics in R",
    "section": "Creating a random walk in R",
    "text": "Creating a random walk in R\nAlthough random walks are defined recursively, “for loops” are almost always the wrong way to accomplish any task in R. Instead, we can rely on R’s native vectorization and parallelization to quickly create a one-dimensional gaussian random walk:\n\n#simulate a gaussian random walk 'manually'\nset.seed(1044)\nrw_a &lt;- cumsum(rnorm(20))\nround(rw_a,2)\n\n [1] -0.05  1.26  2.67  3.47  2.86  2.23  0.94  0.36 -0.26  0.02 -0.65 -0.38\n[13] -0.38 -0.15  0.85  1.60  1.07 -0.73 -0.15 -0.88\n\nplot(rw_a,type='b',ylab='Values',main='Gaussian random walk')\n\n\n\n\n\n\n\n#simulate through a dedicated time series function\n#note result is offset by one observation from above \nset.seed(1044)\nrw_b &lt;- arima.sim(model=list(order=c(0,1,0)),n=20)\nround(rw_b,2)\n\nTime Series:\nStart = 0 \nEnd = 20 \nFrequency = 1 \n [1]  0.00 -0.05  1.26  2.67  3.47  2.86  2.23  0.94  0.36 -0.26  0.02 -0.65\n[13] -0.38 -0.38 -0.15  0.85  1.60  1.07 -0.73 -0.15 -0.88\n\n#recover the white noise innovations (multiple options)\nround(diff(rw_b),2)\n\nTime Series:\nStart = 1 \nEnd = 20 \nFrequency = 1 \n [1] -0.05  1.31  1.41  0.80 -0.61 -0.63 -1.28 -0.59 -0.61  0.27 -0.66  0.27\n[13]  0.00  0.23  1.00  0.75 -0.53 -1.80  0.58 -0.74\n\nset.seed(1044)\nround(rnorm(20),2)\n\n [1] -0.05  1.31  1.41  0.80 -0.61 -0.63 -1.28 -0.59 -0.61  0.27 -0.66  0.27\n[13]  0.00  0.23  1.00  0.75 -0.53 -1.80  0.58 -0.74\n\n\nOf course there are more random walks… we could create Gaussian random walks in two dimensions, or random walks with non-Gaussian innovations.\n\nset.seed(1415)\nrw_c &lt;- apply(mvrnorm(n=20,mu=c(0,0),Sigma=diag(2)),2,cumsum)\nplot(rw_c,type='b',pch=NA,xlab='Y (d1)',ylab='Y (d2)',main='2D random walk')\ntext(x=rw_c,labels=1:20)",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time series basics in R</span>"
    ]
  },
  {
    "objectID": "basicsinr.html#assessing-stationarity",
    "href": "basicsinr.html#assessing-stationarity",
    "title": "Time series basics in R",
    "section": "Assessing stationarity",
    "text": "Assessing stationarity\nLet’s examine some real data. Johnson & Johnson (“J&J”) is a Fortune 500 company which focuses on medical technology and biotech products. Market analysts often describe the performance of publicly traded companies like J&J using the metric of earnings per share (EPS).\n\nplot(JohnsonJohnson,ylab='EPS ($)',main='Quarterly EPS for Johnson & Johnson')\n\n\n\n\n\n\n\n\nWe see strong visual evidence that the series is not stationary: the mean changes over time and so does the variance. Because the variance changes, we can rule out trend-stationarity (and besides, we see that any trend would be non-linear.) Still, we can confirm this visual impression through two quick tests:1\n\nadf.test(JohnsonJohnson)\n\nWarning in adf.test(JohnsonJohnson): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  JohnsonJohnson\nDickey-Fuller = 1.9321, Lag order = 4, p-value = 0.99\nalternative hypothesis: stationary\n\nkpss.test(JohnsonJohnson,null='Trend')\n\nWarning in kpss.test(JohnsonJohnson, null = \"Trend\"): p-value smaller than\nprinted p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  JohnsonJohnson\nKPSS Trend = 0.50099, Truncation lag parameter = 3, p-value = 0.01\n\n\nAlthough the J&J EPS data is nonstationary, neither is it a true Gaussian random walk, since it it very steadily increases and the variance seems to increase over time. This is probably a good candidate for a Box-Cox transformation. Note that the MASS::boxcox function requires a model or formula as its input, and not the raw time series vector. We can create a simple “intercept only” model for the J&J EPS data.\n\nboxcox(JohnsonJohnson~1)\n\n\n\n\n\n\n\n\nThe plot above shows the likelihood function associated with a stationarity test at different choices for the Box-Cox parameter \\(\\lambda\\). It looks like the a range of values from about -0.2 to +0.2 would be acceptable, but since \\(\\lambda=0\\) is so nearly the MLE choice, and since logarithmic transformations are common in econometric analyses, we may adopt \\(\\lambda=0\\) for now, meaning that \\(Y_t^{(\\lambda)} = \\log Y_t\\).\n\nplot(log(JohnsonJohnson),ylab='Log EPS ($)',main='Box-Cox transform of J&J EPS')\n\n\n\n\n\n\n\nadf.test(log(JohnsonJohnson))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(JohnsonJohnson)\nDickey-Fuller = -1.1543, Lag order = 4, p-value = 0.9087\nalternative hypothesis: stationary\n\nkpss.test(log(JohnsonJohnson),null='Trend')\n\nWarning in kpss.test(log(JohnsonJohnson), null = \"Trend\"): p-value smaller than\nprinted p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  log(JohnsonJohnson)\nKPSS Trend = 0.25322, Truncation lag parameter = 3, p-value = 0.01\n\n\nWhat gives?! The Box-Cox transformed EPS data seem to be trend-stationary, but both the ADF test and the KPSS test confirm that the data are not yet stationary. To solve the riddle, we will need to examine the autocorrelation function.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time series basics in R</span>"
    ]
  },
  {
    "objectID": "basicsinr.html#measuring-and-plotting-autocorrelation",
    "href": "basicsinr.html#measuring-and-plotting-autocorrelation",
    "title": "Time series basics in R",
    "section": "Measuring and plotting autocorrelation",
    "text": "Measuring and plotting autocorrelation\n\npar(mfcol=c(1,2))\nplot(JohnsonJohnson,main='Original data',ylab='EPS ($)')\nacf(JohnsonJohnson,main='ACF of original data',xlab='Lag (in years)')\n\n\n\n\n\n\n\n\nThe plots above show the original J&J EPS data alongside a plot of the autocorrelation function, which pairs each lag \\(k = 1, 2, \\ldots,\\) with the estimated autocorrelation \\(\\hat{\\rho}_k = r_k\\). Note that the autocorrelation of any series with itself is of course 1, and so the peak at \\(k=0\\) extends all the way to 1.\nThe remaining peaks show the autocorrelation of this EPS data with its own past values. Because the data have been structured with frequency 4 (appropriate for quarterly data), it takes four lags to reach “1” (year) on the x-axis, but I shall call each past quarter the first lag, second lag, etc. and ignore the x-axis values.\nThe autocorrelation function at the first lag is quite high too – visually about 0.92:\n\nacf(JohnsonJohnson,plot=FALSE)$acf[1:5]\n\n[1] 1.0000000 0.9251021 0.8882631 0.8328480 0.8240770\n\n\n(Okay, about 0.925.) The location of the current quarter’s EPS is highly correlated with the previous quarter’s EPS. Even though EPS is not a cumulated measure, this makes sense: the fortunes of huge companies do not often change overnight, and if a company is profitable in one quarter, it will likely be similarly profitable in the next quarter.\nThe autocorrelations with further lags are also quite high. We should expect this behavior: if this quarter’s EPS is highly correlated with last quarter’s EPS, then last quarter’s EPS is highly correlated with the EPS from two quarters ago, and so this quarter’s EPS is also at least moderately correlated with the EPS from two quarters ago!\nLet’s try examining the ACF plot for the de-trended, Box-Cox transformed EPS data:\n\neps_boxcox_detrend &lt;- ts(lm(log(JohnsonJohnson)~time(JohnsonJohnson))$residuals,\n                         start=1960,frequency=4)\npar(mfcol=c(1,2))\nplot(eps_boxcox_detrend,main='De-trended, transformed data',ylab='Transformed EPS ($)')\nacf(eps_boxcox_detrend,main='De-trended, transformed ACF',xlab='Lag (in years)')\n\n\n\n\n\n\n\n\nThe Box-Cox transformation and de-trending successfully removed a lot of the nonstationary behavior, but the remaining autocorrelation is highly seasonal — each transformed EPS observation is very similar to the EPS from four quarters ago (and thus eight, and twelve…). If we plot the quarterly earnings by year (or each year’s earnings by quarter), we can see this pattern more plainly, where Q3 usually brings very strong earnings for J&J, while Q4 brings weaker earnings:\n\nmonthplot(eps_boxcox_detrend,ylab='Transformed EPS',\n          main='Month plot: eps_boxcox_detrend')\n\n\n\n\n\n\n\nseasonplot(eps_boxcox_detrend,year.labels.left=TRUE,type='l',ylab='Transformed EPS',\n           col=colorRampPalette(c('#ff0000','#bbbbbb','#0000ff'))(21))\n\n\n\n\n\n\n\n\nWe see now why the data failed our stationarity tests: the strong seasonal effects lead the mean of the serties to shift predictably in different quarters. If not every observation has the same predicted mean, then the data cannot be weakly stationary.\nWe will learn more elegant ways of modeling seasonality, but for now we could fall back on a tool we already know: linear regression. Rather than simply de-trending over time, we can add a linear time trend as well as quarterly effect dummies. The residuals from this OLS regression should be more stationary:\n\neps_boxcox_detrend_deseason &lt;- ts(\n  lm(log(JohnsonJohnson) ~ time(JohnsonJohnson) + factor(cycle(JohnsonJohnson)))$residuals,\n  start=1960,frequency=4)\n\nadf.test(eps_boxcox_detrend_deseason)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  eps_boxcox_detrend_deseason\nDickey-Fuller = -1.1176, Lag order = 4, p-value = 0.9144\nalternative hypothesis: stationary\n\nkpss.test(eps_boxcox_detrend_deseason,null='Trend')\n\nWarning in kpss.test(eps_boxcox_detrend_deseason, null = \"Trend\"): p-value\nsmaller than printed p-value\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  eps_boxcox_detrend_deseason\nKPSS Trend = 0.25394, Truncation lag parameter = 3, p-value = 0.01\n\npar(mfrow=c(1,2))\nplot(eps_boxcox_detrend_deseason,\n     main='De-trended, de-seasoned data',ylab='Transformed EPS ($)')\nacf(eps_boxcox_detrend_deseason,\n    main='De-trended de-seasoned ACF',xlab='Lag (in years)')\n\n\n\n\n\n\n\n\nAlthough the de-seasoned data are more stationary than ever, ADF and KPSS tests agree that the remaining stochastic signal is not yet fully stationary. We see from a simple plot of the series that there were good years (1960, the early 1970s) and bad years (the mid 1960s, 1980), patterns which were previously hidden from us.\nUntil we learn better tools, we will not be able to fully decompose this EPS series into a deterministic model and random error. However, even this stumbling block represents progress, because we are starting to see signal through the noise.",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time series basics in R</span>"
    ]
  },
  {
    "objectID": "basicsinr.html#footnotes",
    "href": "basicsinr.html#footnotes",
    "title": "Time series basics in R",
    "section": "",
    "text": "Note that both test outputs include an error message — because the p-values are interpolated from tables found in textbooks, and the test statistics for our data are outside of the table ranges, the ‘true’ p-value for the AFD test is more than 0.99 and the ‘true’ p-value for the KPSS test is less than 0.01. Nothing here has actually gone wrong.↩︎",
    "crumbs": [
      "1. Time series basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time series basics in R</span>"
    ]
  },
  {
    "objectID": "ar.html",
    "href": "ar.html",
    "title": "AR models",
    "section": "",
    "text": "AR(1) process\nOne building block of statistical time series analysis is the autoregressive (AR) model. As the name implies, an AR model presumes that past values of the series can be used to directly predict current values using a regression-like structure:\n\\[Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\ldots + \\varepsilon_t\\]\nI wrote the equation above using the notation of linear regression. We will be expressing the same concept using slightly different notation below.\nLet’s take the simplest case, where each value of the series is predicted only by the previous value. We would name this an autoregressive model with order 1 or AR(1) model.\nIn theory, the exact properties of an AR(1) process depend on both the autoregressive parameter \\(\\phi\\) as well as the specific type of white noise process denoted by \\(\\boldsymbol{\\omega}\\). In practice, we often assume \\(\\boldsymbol{\\omega}\\) to be Gaussian white noise, leaving \\(\\phi\\) as the only input which needs estimation.\nCode\npar(mfrow=c(3,3),mar=c(3.1,3.1,3.1,1.1))\nset.seed(0106)\nw &lt;- rnorm(100)\nphivec &lt;- c(-1.5,-1.0,-0.5,0,0.3,0.6,0.9,1.0,1.5)\nY &lt;- t(rep(w[1],9))\nfor (i in 2:100){Y &lt;- rbind(Y, t(rep(w[i],9)+phivec*Y[i-1,]))}\nfor (j in 1:9) plot(Y[,j],main=bquote(phi==.(phivec[j])),\n                    type='l',xlab=NA,ylab=NA,sub=NA)\n\n\n\n\n\nNine AR(1) processes with the same innovations\nThe plots above provide examples of some specific properties of AR(1) processes:",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR models</span>"
    ]
  },
  {
    "objectID": "ar.html#ar1-process",
    "href": "ar.html#ar1-process",
    "title": "AR models",
    "section": "",
    "text": "Note\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). Let \\(\\boldsymbol{\\omega}\\) be a white noise process observed at the same time periods. If,\n\\[Y_t = \\phi Y_{t-1} + \\omega_t\\]\nFor some \\(\\phi \\in \\mathbb{R}\\) and all \\(t \\in T\\), then we say that \\(\\boldsymbol{Y}\\) is an autoregressive process with order 1\n\n\n\n\n\n\nWhen \\(\\phi \\gt 1\\) or \\(\\phi \\lt -1\\), we say the process is explosive. After a short-to-moderate ‘fuse’, the innovations begin to cumulate exponentially, rapidly heading toward infinity.\nWhen \\(\\phi = 1\\), we say that the process is a random walk, since we now have the familiar model \\(Y_t = Y_{t-1} + \\omega_t\\). When \\(\\phi = -1\\) we have a variant on a random walk in which both the odd and even observations begin closely-related random walks headed in opposite directions.\nWhen \\(-1 \\lt \\phi \\lt 1\\) we observe a stationary process. This includes the case where \\(\\phi = 0\\) (pure white noise), but also other series with noticeable positive or negative autocorrelation. Despite the autocorrelation, the mean and variance of the unconditional variables \\(Y_t\\) (that is, without knowing the past values of \\(\\boldsymbol{Y}\\)) remain the same, and the autocorrelation of two values depends only on their lag distance, not their time index values.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR models</span>"
    ]
  },
  {
    "objectID": "ar.html#generalizing-to-arp",
    "href": "ar.html#generalizing-to-arp",
    "title": "AR models",
    "section": "Generalizing to AR(p)",
    "text": "Generalizing to AR(p)\nNow we may complicate the scenario, by allowing the current value \\(Y_t\\) to have several, different relationships with arbitrary past lags of the series.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). Let \\(\\boldsymbol{\\omega}\\) be a white noise process observed at the same time periods. If,\n\\[Y_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots + \\phi_p Y_{t-p} + \\omega_t\\]\nFor some \\((\\phi_1, \\phi_2, \\ldots, \\phi_p) \\in \\mathbb{R}^p\\) and all \\(t \\in T\\), then we say that \\(\\boldsymbol{Y}\\) is an autoregressive model with order p.\n\n\nThese AR(p) processes can be difficult to visually distinguish because of the more complicated autocorrelation patterns.\n\n\nCode\npar(mfrow=c(3,3),mar=c(3.1,3.1,3.1,1.1))\nset.seed(0107)\nw &lt;- rnorm(100)\nphi1vec &lt;- c(0.5,0.7, 0.7,0.6,0,  -1.6,0.5,0.3,0)\nphi2vec &lt;- c(0.5,0.7,-0.7,0.2,0.5, 0.8,0.4,0.2,0)\nphi3vec &lt;- c(0,  0,   0,  0,  0,   0,  0.3,0.1,1)\nY &lt;- t(rep(w[1],9))\nY &lt;- rbind(Y, t(rep(w[2],9)+phi1vec*Y[1,]))\nY &lt;- rbind(Y, t(rep(w[3],9)+phi1vec*Y[2,]+phi2vec*Y[1,]))\nfor (i in 4:100){Y &lt;- rbind(Y,\n  t(rep(w[i],9)+phi1vec*Y[i-1,]+phi2vec*Y[i-2,]+phi3vec*Y[i-3,]))}\nfor (j in 1:9) plot(Y[,j],main=bquote(\n  bold(phi) == list(.(phi1vec[j]),.(phi2vec[j]),.(phi3vec[j]))),\n  type='l',xlab=NA,ylab=NA,sub=NA)\n\n\n\n\n\nNine AR(2) and AR(3) processes with the same innovations",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR models</span>"
    ]
  },
  {
    "objectID": "ar.html#recognizing-an-ar-process-from-its-acf-plot",
    "href": "ar.html#recognizing-an-ar-process-from-its-acf-plot",
    "title": "AR models",
    "section": "Recognizing an AR process from its ACF plot",
    "text": "Recognizing an AR process from its ACF plot\nAn autocorrelation function (ACF) plot will sometimes provide helpful summary or diagnostic information about an AR(p) model. In the simplest case of AR(1), the height of the ACF plot at the first lag will be equal to p, and the remaining lags will scale geometrically downward. For example, when \\(p=0.8\\), the second lag will have an autocorrelation of roughly 0.64, and the third lag will have autocorrelation of roughly 0.512\n\n\nCode\npar(mfrow=c(1,2),mar=c(3.1,3.1,3.1,1.1))\nset.seed(0108)\nar08 &lt;- arima.sim(list(ar=0.8),n=1000)\narneg05 &lt;- arima.sim(list(ar=-0.5),n=1000)\n\nacf(ar08,main=expression(paste('ACF when ',phi == 0.8)),lag.max=15)\nacf(arneg05,main=expression(paste('ACF when ',phi == -0.5)),lag.max=15)\n\n\n\n\n\nTwo AR(1) processes and their ACF plots, 1000 obs each\n\n\n\n\nMore complex AR models have correspondingly complex ACF plots, and you will not always be able to easily diagnose the order and coefficient sizes:\n\n\nCode\npar(mfrow=c(1,2),mar=c(3.1,3.1,3.1,1.1))\nset.seed(0109)\nar08neg05 &lt;- arima.sim(list(ar=c(0.8,-0.5)),n=1000)\narneg05neg04 &lt;- arima.sim(list(ar=c(-0.5,-0.4)),n=1000)\n\nacf(ar08neg05,main=expression(paste('ACF when ',phi == list(0.8,-0.5))),lag.max=15)\nacf(arneg05neg04,main=expression(paste('ACF when ',phi == list(-0.5,-0.4))),lag.max=15)\n\n\n\n\n\nTwo AR(2) processes and their ACF plots, 1000 obs each\n\n\n\n\nYou do not need to be able to spot a complex AR model from a brief inspection of its time series or its ACF plot. However, you should know that complex behavior can be very accurately fit by a relatively simple AR model.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR models</span>"
    ]
  },
  {
    "objectID": "ar.html#which-ar-processes-are-stationary",
    "href": "ar.html#which-ar-processes-are-stationary",
    "title": "AR models",
    "section": "Which AR processes are stationary",
    "text": "Which AR processes are stationary\nWe will learn more about this elsewhere, when we discuss unit roots. For now, I will leave you with a simple set of guidelines:\n\nAR(1) processes are stationary when \\(|\\phi_1| \\lt 1\\)\nAR(2) processes are stationary when three conditions are met:\n\n\\[\\begin{aligned}(1) \\qquad & |\\phi_2| \\lt 1 \\\\ (2) \\qquad & \\phi_1 + \\phi_2 \\lt 1 \\\\ (3) \\qquad & \\phi_1 - \\phi_2 \\lt 1 \\end{aligned}\\]\nRepresenting the two parameters \\(phi_1\\) and \\(\\phi_2\\) on the coördinate plane, we would say any process with its parameters inside the shaded region is stationary:\n\n\nCode\nplot(c(-2.25,2.25),c(-1.25,1.25),type='n',main=NA,sub=NA,\n     ylab=expression(phi[2]),xlab=expression(phi[1]),asp=1)\npolygon(x=c(-2,0,2),y=c(-1,1,-1),lty=1,lwd=2,density=-1,col='#0000ff7f')\nabline(h=0,v=0,lty=2,col='#7f7f7f')\n\n\n\n\n\nStationary sets of AR(2) parameters\n\n\n\n\n\nFor AR(3) and higher processes, stationarity will depend upon the roots of the characteristic polynomial, which can be calculated with a unit root test.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR models</span>"
    ]
  },
  {
    "objectID": "ar.html#ar-model-responses-to-system-shocks",
    "href": "ar.html#ar-model-responses-to-system-shocks",
    "title": "AR models",
    "section": "AR model responses to system shocks",
    "text": "AR model responses to system shocks\nAR models translate one-time system shocks into persistent, slowly-decaying signals. Consider a simple AR(1) model which processes a fairly quiet series of innovations, interrupted irregularly by a much larger signal:\n\n\nCode\nset.seed(0110)\nw &lt;- runif(100,-1,1)\nw[8] &lt;- 10; w[15] &lt;- 10; w[25] &lt;- -10; w[45] &lt;- -10;\nar &lt;- arima.sim(list(ar=0.9),n=100,innov=w)\nplot(15+ar,type='s',lwd=2,ylim=c(-10,27),ylab=NA)\nlines(w,type='h',col='#0000ff',lwd=2)\nabline(h=15,lty=2,col='#7f7f7f')\nlegend(x='topright',lwd=2,col=c('#000000','#0000ff'),bty='n',\n       legend=c(expression(paste('Time series ',Y[t])),\n                expression(paste('Innovations ',omega[t]))))\n\n\n\n\n\nPersistence and decay in an AR(1) model",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>AR models</span>"
    ]
  },
  {
    "objectID": "ma.html",
    "href": "ma.html",
    "title": "MA models",
    "section": "",
    "text": "MA(1) process\nA second building block of statistical time series analysis is the moving average (MA) model. Moving average models are not well-named: rather than predicting \\(Y_t\\) as a weighted average of its past values (which is what AR models can do), MA models predict \\(Y_t\\) using a weighted average of the unobserved innovations:\n\\[Y_t = \\beta_0 + \\beta_1 \\varepsilon_{t-1} + \\beta_2 \\varepsilon_{t-2} + \\ldots + \\varepsilon_t\\]\nOnce again I have sketched this idea using the terminology familiar to us from linear regression, but below I will redefine this idea using our new time series notation.\nLet’s take the simplest case, where each value of the series averages only the current and immediate previous innovation. We would name this an moving average model with order 1 or MA(1) model.\nIn theory, the exact properties of a MA(1) process depend on both the moving average parameter \\(\\theta\\) as well as the specific type of white noise process denoted by \\(\\boldsymbol{\\omega}\\). In practice, we often assume \\(\\boldsymbol{\\omega}\\) to be Gaussian white noise, leaving \\(\\theta\\) as the only input which needs estimation.\nCode\npar(mfrow=c(3,3),mar=c(3.1,3.1,3.1,1.1))\nset.seed(0106)\nw &lt;- rnorm(20)\nthetavec &lt;- c(-2,-1,-.6,-.3,0,.3,.6,1,2)\nY &lt;- t(rep(w[1],9))\nfor (i in 2:20){Y &lt;- rbind(Y, t(rep(w[i],9)+thetavec*w[i-1]))}\nfor (j in 1:9) {plot(Y[,j],main=bquote(theta==.(thetavec[j])),\n                     type='l',xlab=NA,ylab=NA,sub=NA)\n  lines(1:20,Y[,5],col='#0000ff',lty=2)}\n\n\n\n\n\nNine MA(1) processes with the same innovations\nIn the plots above, we see that MA processes are a little more subtle than AR processes:\nThe properties and utility of MA models are perhaps better described with higher-order processes.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MA models</span>"
    ]
  },
  {
    "objectID": "ma.html#ma1-process",
    "href": "ma.html#ma1-process",
    "title": "MA models",
    "section": "",
    "text": "Note\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). Let \\(\\boldsymbol{\\omega}\\) be a white noise process observed at the same time periods. If,\n\\[Y_t = \\omega_t + \\theta \\omega_{t-1}\\]\nFor some \\(\\theta \\in \\mathbb{R}\\) and all \\(t \\in T\\), then we say that \\(\\boldsymbol{Y}\\) is an moving average process with order 1\n\n\n\n\n\n\nMA(1) processes remain stationary for any finite value of \\(\\theta\\), even large positive or negative values. (The weighted sum of any two zero-mean random variables still has a mean of zero.)\nPositive and negative values of \\(\\theta\\) produce similar patterns, with no parameter choice creating the alternating series seen in the AR(1) models.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MA models</span>"
    ]
  },
  {
    "objectID": "ma.html#generalizing-to-maq",
    "href": "ma.html#generalizing-to-maq",
    "title": "MA models",
    "section": "Generalizing to MA(q)",
    "text": "Generalizing to MA(q)\nNow we may complicate the scenario, by allowing the current value \\(Y_t\\) to average several, different past innovations of the series.\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). Let \\(\\boldsymbol{\\omega}\\) be a white noise process observed at the same time periods. If,\n\\[Y_t = \\omega_t + \\theta_1 \\omega_{t-1} + \\theta_2 \\omega_{t-2} + \\ldots + \\theta_q \\omega_{t-q}\\]\nFor some \\((\\theta_1, \\theta_2, \\ldots, \\theta_q) \\in \\mathbb{R}^q\\) and all \\(t \\in T\\), then we say that \\(\\boldsymbol{Y}\\) is an moving average model with order q.\n\n\nMA(q) effects tend to become more pronounced at higher orders. For many combinations of parameters \\(\\theta_1, \\ldots, \\theta_q\\), the general effect is a smoothing filter which highlights consecutive large innovations but minimizes single innovations, and preserves no memory of the past beyond its averaging:\n\n\nCode\npar(mfrow=c(3,3),mar=c(3.1,3.1,3.1,1.1))\nset.seed(0107)\nw &lt;- rnorm(50)\ntheta &lt;- list(c(1,1),c(1,1,1,1),c(1,1,1,1,1,1),\n              c(0.8,0.6,0.4,0.2),c(0.4,0.3,0.2,0.1),c(0,0,0,1),\n              c(2,4,2,1),c(0,-1),c(-1,-1,-1,-1))\nY &lt;- matrix(nrow=50,ncol=9)\nfor (i in 1:9) Y[,i] &lt;- arima.sim(list(ma=theta[[i]]),n=50,innov=w)\nfor (j in 1:9) plot(Y[,j],main=bquote(theta==.(paste(theta[[j]],collapse=','))),\n  type='l',xlab=NA,ylab=NA,sub=NA)\n\n\n\n\n\nNine MA processes with the same innovations",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MA models</span>"
    ]
  },
  {
    "objectID": "ma.html#recognizing-a-ma-process-from-its-acf-plot",
    "href": "ma.html#recognizing-a-ma-process-from-its-acf-plot",
    "title": "MA models",
    "section": "Recognizing a MA process from its ACF plot",
    "text": "Recognizing a MA process from its ACF plot\nAn autocorrelation function (ACF) plot will always provide helpful summary or diagnostic information about a MA(q) model. In the simplest case of MA(1), the height of the ACF plot at the first lag will be equal to \\(q/(1+q^2)\\), and the remaining lags will show little or no autocorrelation at all. For example, when \\(p=0.8\\), the first lag will have an autocorrelation of roughly \\(0.8/(1+0.64) \\approx 0.49\\), and all other lags will show only minimal autocorrelation:\n\n\nCode\npar(mfrow=c(1,2),mar=c(3.1,3.1,3.1,1.1))\nset.seed(0108)\nma08 &lt;- arima.sim(list(ma=0.8),n=1000)\nma080706 &lt;- arima.sim(list(ma=c(0.8,0.7,0.6)),n=1000)\n\nacf(ma08,main=expression(paste('ACF when ',theta == 0.8)),lag.max=15)\nacf(ma080706,main=expression(paste('ACF when ',theta == list(0.8,0.7,0.6))),lag.max=15)\n\n\n\n\n\nTwo MA processes and their ACF plots, 1000 obs each\n\n\n\n\nThe key feature here is that for values \\(s \\lt (t - q)\\), \\(Y_t\\) and \\(Y_s\\) have no elements in common and are completely independent. Any small sample autocorrelation is purely spurious.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MA models</span>"
    ]
  },
  {
    "objectID": "ma.html#all-ma-processes-are-stationary",
    "href": "ma.html#all-ma-processes-are-stationary",
    "title": "MA models",
    "section": "All MA processes are stationary",
    "text": "All MA processes are stationary\nSince MA processes are simply weighted sums of zero-mean white noise, they themselves are zero-mean and meet all the requirements for weak stationarity, no matter the choice(s) of \\(\\boldsymbol{\\theta}\\). In the equations which follow, assume that \\(\\mathbb{E}[\\omega_t] = 0\\) and \\(\\mathbb{V}[\\omega_t] = \\sigma^2\\):\n\\[\\begin{aligned} (1) \\qquad \\mathbb{E}[Y_t] &= \\mathbb{E}[\\omega_t + \\sum_{i=1}^q \\theta_i \\omega_{t-i}] = 0 + \\sum_{i=1}^q \\theta_i \\mathbb{E}[\\omega_{t-i}] = 0 + \\sum_{i=1}^q 0 = 0 \\\\  \\\\ (2) \\qquad \\mathbb{V}[Y_t] &= \\mathbb{V}[\\omega_t + \\sum_{i=1}^q \\theta_i \\omega_{t-i}] = \\sigma^2 + \\sum_{i=1}^q \\theta^2_i \\mathbb{V}[\\omega_{t-i}] = \\left(1 + \\sum_i \\theta_i^2 \\right) \\sigma^2 \\\\ \\\\ (3) \\qquad \\gamma_{st} &= \\textrm{Cov}\\left(\\omega_s + \\sum_{i=1}^q \\theta_i \\omega_{s-i},\\, \\omega_t + \\sum_{i=1}^q \\theta_i \\omega_{t-i} \\right) \\\\ &= \\left\\{ \\begin{array}{ll} 0 & \\textrm{if} \\; |s - t| \\gt q \\\\ \\theta_q \\sigma^2 & \\textrm{if} \\; |s - t| = q \\\\ (\\theta_{q-1} + \\theta_2 \\theta_q) \\sigma^2 & \\textrm{if} \\; |s - t| = q - 1 \\\\ \\cdots & \\cdots \\end{array} \\right\\}\\end{aligned}\\]\nWe can see from the above that the mean and variance are constant and that the autocovariance of two entries depends only on the lag between them, which are the requirements of weak stationarity.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MA models</span>"
    ]
  },
  {
    "objectID": "ma.html#ma-model-responses-to-system-shocks",
    "href": "ma.html#ma-model-responses-to-system-shocks",
    "title": "MA models",
    "section": "MA model responses to system shocks",
    "text": "MA model responses to system shocks\nMA models dampen one-time system shocks by averaging them among the other innovations, and after the moving average window has passed, the system shock disappears entirely from the series. Consider a MA(3) model which processes a fairly quiet series of innovations, interrupted irregularly by a much larger signal:\n\n\nCode\nset.seed(0110)\nw &lt;- runif(100,-1,1)\nw[8] &lt;- 10; w[15] &lt;- 10; w[25] &lt;- -10; w[45] &lt;- -10;\nma &lt;- arima.sim(list(ma=c(0.9,0.6,0.3)),n=100,innov=w)\nplot(15+ma,type='s',lwd=2,ylim=c(-10,27),ylab=NA)\nlines(w,type='h',col='#0000ff',lwd=2)\nabline(h=15,lty=2,col='#7f7f7f')\nlegend(x='topright',lwd=2,col=c('#000000','#0000ff'),bty='n',\n       legend=c(expression(paste('Time series ',Y[t])),\n                expression(paste('Innovations ',omega[t]))))\n\n\n\n\n\nPersistence and decay in an MA(0.9,0.6,0.3) model",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MA models</span>"
    ]
  },
  {
    "objectID": "backshift.html",
    "href": "backshift.html",
    "title": "Backshift notation",
    "section": "",
    "text": "The backshift operator\nWhen we learned linear regression techniques, it was convenient to move away from an arithmetic expansion such as:\n\\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_k x_{k,i} + \\varepsilon_i\\]\nAnd instead move toward a matrix representation, such as:\n\\[\\boldsymbol{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\]\nWe will now learn a similar technique for time series analysis, which collapses all lags and their coefficients using the backshift operator.\nWritten thus, we can treat \\(B\\) almost as a separate variable,1 and use it to remove lagged terms from our equations:\n\\[\\begin{aligned} & \\, Y_t = 1.3\\,Y_{t-1} - 0.36\\,Y_{t-2} + \\omega_t \\\\ \\\\ \\Longrightarrow & \\,Y_t = 1.3\\,B\\,Y_t - 0.36\\,B^2\\,Y_t + \\omega_t \\\\ \\\\ \\Longrightarrow & \\, Y_t = Y_t(0.5B - 0.36B^2) + \\omega_t \\\\ \\\\ \\Longrightarrow & \\,\\omega_t = Y_t(1 - 1.3B + 0.36B^2)  \\end{aligned}\\]\nThe model above was an AR(2) process, but we can equally use this notation with MA models:\n\\[\\begin{aligned} & \\, Y_t = \\omega_t + 0.8\\,\\omega_{t-1} + 0.16\\,\\omega_{t-2} \\\\ \\\\ \\Longrightarrow & \\,Y_t = \\omega_t + 0.8\\,B\\,\\omega_t + 0.16\\,B^2\\,\\omega_t \\\\ \\\\ \\Longrightarrow & \\, Y_t = \\omega_t(1 + 0.8B + 0.16B^2) \\end{aligned}\\]",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backshift notation</span>"
    ]
  },
  {
    "objectID": "backshift.html#the-backshift-operator",
    "href": "backshift.html#the-backshift-operator",
    "title": "Backshift notation",
    "section": "",
    "text": "Note\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). Then the unary operator \\(B\\) (backshift) is defined as follows:\n\\[B \\, Y_t = Y_{t-1}\\]\n\\[B^k \\, Y_t = Y_{t-k}\\]",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backshift notation</span>"
    ]
  },
  {
    "objectID": "backshift.html#the-ar-and-ma-characteristic-polynomials",
    "href": "backshift.html#the-ar-and-ma-characteristic-polynomials",
    "title": "Backshift notation",
    "section": "The AR and MA characteristic polynomials",
    "text": "The AR and MA characteristic polynomials\nSo far, this hasn’t seemed to save much space or offer us any new possibilities. We can bring it altogether by introducing two characteristic polynomials. These equations aren’t any sort of theorem or result, just a definition:\n\n\n\n\n\n\nNote\n\n\n\nThe characteristic polynomial of an AR process of order p is defined as follows:\n\\[\\Phi(B) = 1 - \\phi_1 B - \\phi_2 B^2 - ... - \\phi_p B^p\\]\nThe characteristic polynomial of an MA process of order q is defined as follows:\n\\[\\Theta(B) = 1 + \\theta_1 B + \\theta_2 B^2 + ... + \\theta_q B^q\\]\n\n\nWith these two “shortcuts” we can represent every AR process as follows:\n\\[\\omega_t = \\Phi(B) \\cdot Y_t\\]\nAnd we can represent every MA process as follows:\n\\[Y_t = \\Theta(B) \\cdot \\omega_t\\]",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backshift notation</span>"
    ]
  },
  {
    "objectID": "backshift.html#roots-of-the-characteristic-polynomials",
    "href": "backshift.html#roots-of-the-characteristic-polynomials",
    "title": "Backshift notation",
    "section": "Roots of the characteristic polynomials",
    "text": "Roots of the characteristic polynomials\nIn the examples I gave above, we could say that for the AR(2) process,\n\\[\\Phi(B) = 1 - 1.3B + 0.36B^2\\]\nKeen-eyed mathemagicians might have already noticed that we can factor and solve for the roots of this equation:\n\\[\\Phi(B) = (1 - 0.4B)(1 - 0.9B)\\]\n\\[\\Phi(B) = 0 \\iff B \\in \\{2.5, 1.111\\ldots\\}\\]\nLikewise, we could solve for the roots of the MA(2) characteristic polynomial\n\\[\\Theta(B) = 1 + 0.8B + 0.16B^2\\]\n\\[\\Theta(B) = (1 + 0.4B)^2\\]\n\\[\\Theta(B) = 0 \\iff B = -2.5\\]\nThe dramatic reveal of why we would want to solve the roots of these polynomials must wait for the next page.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backshift notation</span>"
    ]
  },
  {
    "objectID": "backshift.html#footnotes",
    "href": "backshift.html#footnotes",
    "title": "Backshift notation",
    "section": "",
    "text": "Not a random variable, but a variable we could solve for, such as \\(x\\) in the equation \\(x^2 - 4x = -3\\)↩︎",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Backshift notation</span>"
    ]
  },
  {
    "objectID": "arima.html",
    "href": "arima.html",
    "title": "ARIMA models",
    "section": "",
    "text": "ARMA models\nHaving learned about AR models and MA models and difference-stationarity we are now ready to bring these pieces together into the first major modeling and forecasting technique: ARIMA models.\nARIMA is in fact an extension of a slightly simpler model called ARMA, which contains both AR and MA components. We will learn about ARMA models first before extending to the full ARIMA model.\nARMA models quite simply combine AR and MA models in one. They allow the current value of the time series (\\(Y_t\\)) to depend both on past values as well as past shocks. Like other time series we have seen, ARMA models are sometimes written as though they are zero-mean, but can also be expressed with a level or a deterministic trend:\n\\[Y_t = c + \\omega_t + \\sum_{i=1}^{p}\\phi_i Y_{t-i} + \\sum_{j=1}^q \\theta_j \\omega_{t-j}\\]\n\\[Y_t = c + \\delta t + \\omega_t + \\sum_{i=1}^{p}\\phi_i Y_{t-i} + \\sum_{j=1}^q \\theta_j \\omega_{t-j}\\]",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ARIMA models</span>"
    ]
  },
  {
    "objectID": "arima.html#arma-models",
    "href": "arima.html#arma-models",
    "title": "ARIMA models",
    "section": "",
    "text": "Note\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). Let \\(\\boldsymbol{\\omega}\\) be a white noise process observed at the same time periods. If,\n\\[Y_t = \\omega_t + \\sum_{i=1}^{p}\\phi_i Y_{t-i} + \\sum_{j=1}^q \\theta_j \\omega_{t-j}\\]\nFor some \\(\\Phi = (\\phi_1, \\ldots, \\phi_p) \\in \\mathbb{R}^p\\) and some \\(\\Theta = (\\theta_1, \\ldots, \\theta_q) \\in \\mathbb{R}^q\\) and all \\(t \\in T\\), then we say that \\(\\boldsymbol{Y}\\) is an autoregressive and moving average (ARMA) model written ARMA(p,q).",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ARIMA models</span>"
    ]
  },
  {
    "objectID": "arima.html#characteristic-polynomials-their-roots-and-reciprocals",
    "href": "arima.html#characteristic-polynomials-their-roots-and-reciprocals",
    "title": "ARIMA models",
    "section": "Characteristic polynomials, their roots and reciprocals",
    "text": "Characteristic polynomials, their roots and reciprocals\nUsing the notation developed earlier, we can represent this ARMA model using the characteristic polynomials for the AR and MA components:\n\\[\\begin{aligned} Y_t &= \\omega_t + \\sum_{i=1}^{p}\\phi_{t-i} Y_i + \\sum_{j=1}^q \\theta_j \\omega_{t-j} \\Longrightarrow \\\\ \\\\ Y_t - \\sum_{i=1}^{p}\\phi_i Y_{t-i} &= \\omega_t + \\sum_{j=1}^q \\theta_j \\omega_{t-j} \\Longrightarrow \\\\ \\\\ Y_t \\left(1 - \\sum_{i=1}^{p}\\phi_i B^i \\right) &= \\omega_t \\left(1 + \\sum_{j=1}^{q}\\theta_j B^j \\right) \\Longrightarrow \\\\ \\\\ Y_t \\Phi(B) &= \\omega_t \\Theta(B) \\end{aligned}\\] Each of these two polynomials, \\(\\Phi(B)\\) and \\(\\Theta(B)\\), have real, imaginary, or complex roots. The location of these roots determines the properties of the ARMA series.\nSometimes, we find it mathematically convenient to work with the characteristic polynomials, and other times we find it useful to factor them in a specific way. Consider the AR equation:\n\\[Y_t (1 - \\phi_1 B - \\phi_2 B^2 - \\ldots \\phi_p B^p) = \\omega_t\\]\nWe can solve directly for the real or complex roots \\(B^0_1, \\ldots, B^0_p\\), or we could factor the equation like so:\n\\[Y_t (1 - \\lambda_1B) \\cdot (1 - \\lambda_2B) \\cdot \\ldots \\cdot (1 - \\lambda_pB)\\]\nWhy would we do that? Because of a convenient formula for the sum of an infinite geometric series: For any real or imaginary constant \\(r: |r| \\lt 1\\), we have:\n\\[\\frac{1}{1-r} = 1 + r + r^2 + \\ldots\\] Which means we can write,\n\\[\\begin{aligned} Y_t &= \\omega_t \\frac{1}{(1 - \\lambda_1B) \\cdot (1 - \\lambda_2B) \\cdot \\ldots \\cdot (1 - \\lambda_pB)} \\Longrightarrow \\\\ \\\\ &= \\omega_t \\frac{1}{\\prod_{i=1}^p (1 - \\lambda_i B)} = \\omega_t \\sum_{i=1}^p \\frac{c_i}{1 - \\lambda_i B} = \\omega_t \\sum_{i=1}^p \\sum_{j=1}^\\infty \\lambda_i^j B^j \\end{aligned}\\]\nTherefore, every AR process can be represented by the sum of several infinite series of geometrically dampened shocks from the past innovations. This AR process will have constant variance only if the sum of every geometric series is finite, which means that \\(|\\lambda_i| \\lt 1\\) for each and every \\(\\lambda_i\\).1 Because each \\(\\lambda_i\\) is a reciprocal of the roots of the characteristic polynomial,2, this is equivalent to the condition that each and every root of the characteristic polynomial must lie outside the unit circle.\nA very similar line of reasoning holds with the characteristic polynomial of the MA component, \\(\\Theta(B)\\), and its roots.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ARIMA models</span>"
    ]
  },
  {
    "objectID": "arima.html#properties-of-an-arma-process",
    "href": "arima.html#properties-of-an-arma-process",
    "title": "ARIMA models",
    "section": "Properties of an ARMA process",
    "text": "Properties of an ARMA process\nThe characteristic polynomials and their roots help us to describe some ARMA processes as having one or more of the following properties.\n\nStationarity (AR)\nAs mentioned above, the roots of the AR characteristic polynomial determine whether the process is stationary or not. Specifically,\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\), and let the generating process for \\(\\boldsymbol{Y}\\) follow an ARMA(p,q) process (where either p or q can be 0).\nLet \\(z_1, z_2, \\ldots, z_p\\) be the (possibly non-unique) real or complex roots of the AR characteristic polynomial:\n\\[\\Phi(z) = 1 - \\phi_1 z - \\ldots - \\phi_p z^p\\]\n\nThe ARMA process \\(\\boldsymbol{Y}\\) is stationary if and only if all AR roots lie outside the unit circle: \\(|z_i| \\gt 1 \\quad \\forall i \\in {1,\\ldots,p}\\)\nThe ARMA process \\(\\boldsymbol{Y}\\) is a random walk if and only if exactly one AR root lies on the unit circle and all others lie outside the unit circle: \\(\\exists \\, i \\in {1,\\ldots,p}: |z_i| = 1\\) and \\(|z_j| \\gt 1 \\quad \\forall j \\in {1,\\ldots,p}: j \\ne i\\)\nThe ARMA process \\(\\boldsymbol{Y}\\) is explosive if any AR roots lie within the unit circle: \\(\\exists \\, i \\in {1,\\ldots,p}: |z_i| \\lt 1\\)\n\n\n\n\n\nInvertibility (MA)\nWe observe the past values \\(Y_1, \\ldots, Y_t\\) — these concrete observations help us to identify and estimate an AR process. However, we do not directly observe the innovations \\(\\omega_1, \\ldots, \\omega_t\\), and have to estimate them from our own (estimated) parameters \\(\\boldsymbol{\\phi} = (\\phi_1, \\ldots, \\phi_p)\\) and \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_q)\\). The cyclical nature of these estimates often allow multiple MA processes to fit the data equally well.\nFor example, consider the MA(1) series:\n\\[Y_t = \\omega_t + 2 \\omega_{t-1}, \\qquad \\omega_i \\stackrel{iid}{\\sim} \\textrm{Normal}(0,1^2)\\]\nFrom this definition we could calculate the following (stationary) moments:\n\\[\\begin{aligned} \\mathbb{E}[Y_t] &= \\mathbb{E}[\\omega_t] + 2\\mathbb{E}[\\omega_{t-1}] = 0 + 2(0) = 0 \\\\ \\\\ \\mathbb{V}[Y_t] &= \\mathbb{V}[\\omega_t] + 2^2\\mathbb{V}[\\omega_{t-1}] = 1 + 4(1) = 5 \\\\ \\\\ \\gamma_{st} &= \\left\\{\\begin{array}{ll} \\textrm{Cov}(\\omega_t,\\omega_t) + \\textrm{Cov}(2\\omega_{t-1},2\\omega_{t-1}) = 5 & \\textrm{if}\\; s=t \\\\ \\textrm{Cov}(2\\omega_{t-1},\\omega_{t-1}) = 2 & \\textrm{if}\\; |s - t| = 1 \\\\ 0 & \\textrm{else} \\end{array} \\right\\} \\end{aligned}\\]\nHowever, other MA processes would match these same moments, for example:\n\\[Y_t = \\omega_t + 0.5 \\omega_{t-1}, \\qquad \\omega_i \\stackrel{iid}{\\sim} \\textrm{Normal}(0,2^2)\\]\nRecalculating the moments, we would see that:\n\\[\\begin{aligned} \\mathbb{E}[Y_t] &= \\mathbb{E}[\\omega_t] + 0.5\\mathbb{E}[\\omega_{t-1}] = 0 + 0.5(0) = 0 \\\\ \\\\ \\mathbb{V}[Y_t] &= \\mathbb{V}[\\omega_t] + 0.5^2\\mathbb{V}[\\omega_{t-1}] = 4 + 0.25(4) = 5 \\\\ \\\\ \\gamma_{st} &= \\left\\{\\begin{array}{ll} \\textrm{Cov}(\\omega_t,\\omega_t) + \\textrm{Cov}(0.5\\omega_{t-1},0.5\\omega_{t-1}) = 5 & \\textrm{if}\\; s=t \\\\ \\textrm{Cov}(0.5\\omega_{t-1},\\omega_{t-1}) = 2 & \\textrm{if}\\; |s - t| = 1 \\\\ 0 & \\textrm{else} \\end{array} \\right\\} \\end{aligned}\\]\nBecause the first and second moments are the same, any sample of data will support either model equally well.3 So, do we have any reason to prefer one over the other?\nNot to hold you in suspense: yes. We do prefer one over the other. If we adopt the first model, we would write:\n\\[\\begin{aligned} Y_t &= \\omega_t + 2\\omega_{t-1} \\Longrightarrow \\\\ \\omega_t &= Y_t - 2\\omega_{t-1} \\\\ &= Y_t - 2(Y_{t-1} - 2\\omega_{t-2}) \\\\ &= Y_t - 2(Y_{t-1} - 2(Y_{t-1} - 2\\omega_{t-3})) \\\\ & = Y_t - 2Y_{t-1} + 4Y_{t-2} - 8Y_{t-2} + 16Y_{t-4} - \\ldots \\end{aligned}\\]\nIn other words, each current innovation would be dependent upon an explosive series of the past observations of \\(\\boldsymbol{Y}\\). Any small amount of past error (measurement error, misspecification, etc.) would compound to the point where the current values of the series would be effectively arbitrary.\nHowever, if we use the second model — which fits the data equally well — we see that:\n\\[\\begin{aligned} Y_t &= \\omega_t + 0.5\\omega_{t-1} \\Longrightarrow \\\\ \\omega_t &= Y_t - 0.5\\omega_{t-1} \\\\ &= Y_t - 0.5(Y_{t-1} - 0.5\\omega_{t-2}) \\\\ &= Y_t - 0.5(Y_{t-1} - 0.5(Y_{t-1} - 0.5\\omega_{t-3})) \\\\ & = Y_t - 0.5Y_{t-1} + 0.25Y_{t-2} - 0.125Y_{t-2} + 0.0625Y_{t-4} - \\ldots \\end{aligned}\\]\nNow we see that the current innovation can be estimated as a finite, converging sum of all the past values of the series, which soon have vanishingly small effects on the present. Our ability to recover this dampened trend relies upon the roots of the MA characteristic polynomial:\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\), and let the generating process for \\(\\boldsymbol{Y}\\) follow an ARMA(p,q) process (where either p or q can be 0).\nLet \\(z_1, z_2, \\ldots, z_p\\) be the (possibly non-unique) real or complex roots of the MA characteristic polynomial:\n\\[\\Theta(z) = 1 + \\theta_1 z + \\ldots + \\theta_p z^p\\]\n\nThe ARMA process \\(\\boldsymbol{Y}\\) is invertible if and only if all MA roots lie outside the unit circle: \\(|z_i| \\gt 1 \\quad \\forall i \\in {1,\\ldots,q}\\)\nThe ARMA process \\(\\boldsymbol{Y}\\) is non-invertible otherwise.\n\nIf any of the MA roots lie exactly on the unit circle, there exists no alternative invertible representation of \\(\\boldsymbol{Y}\\) with the same first- and second- degree moments.\nIf none of the MA roots lie exactly on the unit circle, an alternative invertible representation of \\(\\boldsymbol{Y}\\) with the same first- and second- degree moments can be found by reciprocating each root within the unit circle: \\(z'_i = \\frac{1}{z_i} \\;\\forall \\; z_i: |z_i| \\lt 1\\)\n\n\n\n\n\n\nReducibility (both)\nThe existence of duplicate AR or MA roots is not generally a problem. For example, consider the AR model:\n\\[Y_t = Y_{t-1} - 0.25Y_{t-2} + \\omega_t\\]\nWhich can be factored as \\(\\omega_t = Y_t(1 - B + 0.25B^2) = Y_t(1 - 0.5B)(1 - 0.5B)\\) meaning that it has a double root at \\(z = 2\\). These roots lie outside the unit circle, the process is stationary, and \\(\\boldsymbol{Y}\\) would be a fine target for further analysis.\nHowever, some ARMA representations reveal one or more identical roots on both the AR and MA sides at once. This situation unnecessarily complicates the model and can mislead the analyst on matters of stationarity, invertibility, or the need for differencing.\nAs an example, consider an ARMA(2,1) model,\n\\[Y_t = 3.5Y_{t-1} - 1.5Y_{t-2} + \\omega_t - 3\\omega_{t-1}\\]\nThis model looks gnarly, with large autoregressive coefficients we would normally associate with explosive behavior. However, note that we can refactor \\(\\boldsymbol{Y}\\) as:\n\\[Y_t(1 - 0.5B)(1 - 3B) = \\omega_t(1 - 3B)\\]\nAnd, canceling out the common terms, arrive at:\n\\[Y_t(1 - 0.5B) = \\omega_t\\]\nWhich expands to \\(Y_t = 0.5Y_{t-1} + \\omega_t\\), a perfectly well-behaved stationary AR(1) model.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ARIMA models</span>"
    ]
  },
  {
    "objectID": "arima.html#arima-models-and-integration",
    "href": "arima.html#arima-models-and-integration",
    "title": "ARIMA models",
    "section": "ARIMA models and integration",
    "text": "ARIMA models and integration\nWhen discussing stationarity and the roots of the AR characteristic polynomial, I left out one possible condition: if the presence of a single root on the unit circle implies a random walk, what happens if multiple roots can be found on the unit circle?\nIf we do observe a random walk, such as the ARMA(1,1) process \\(Y_t = Y_{t-1} + \\omega_t + 0.75\\omega_{t-1}\\), we can always difference to find a stationary model, as described elsewhere:\n\\[\\nabla Y_t = Y_t - Y_{t-1} = \\omega_t + 0.75 \\omega_{t-1}\\]\nSometimes we do see multiple roots on the unit circle, in which case we need to difference multiple times, one for each root, to retrieve a stationary series. Consider the clearly nonstationary AR(3) process \\(Y_t = 2.5Y_{t-1} - 2Y_{t-2} + 0.5Y_{t-3} + \\omega_t\\):\n\n\nCode\nset.seed(0112)\nw &lt;- rnorm(103)\nY &lt;- vector(); Y[1:3] &lt;- w[1:3]\nfor (i in 4:103) Y[i] &lt;- 2.5*Y[i-1] - 2*Y[i-2] + 0.5*Y[i-3] + w[i]\nplot(Y[4:103],type='l',ylab=expression(Y[t]),xlab='Time index t')\n\n\n\n\n\nExplosive AR(3) process before differencing\n\n\n\n\nThis AR model’s characteristic polynomial factors to \\(Y_t(1 - B)(1 - B)(1 - 0.5B) = \\omega_t\\). The difference operator itself can be defined as \\(\\nabla Y_t = Y_t - Y_{t-1} = Y_t(1 - B)\\), and from this we see that the first difference of our AR(3) model would have the factored representation \\(\\nabla Y_t(1 - B)(1 - 0.5B) = \\omega_t\\), or when expanded, \\(\\nabla Y_t = 1.5 \\nabla Y_{t-1} - 0.5 \\nabla Y_{t-2}\\). This AR(2) series is still non-stationary, but no longer explosive!\n\n\nCode\nplot(diff(Y[4:103]),type='l',ylab=expression(nabla*y[t]),xlab='Time index t')\n\n\n\n\n\nExplosive AR(3) process after first differencing\n\n\n\n\nWe can now apply a second round of differencing: \\(\\nabla^2 Y_t = Y_t(1 - B)^2\\), leading to the factored representation \\(\\nabla^2 Y_t (1 - 0.5B) = \\omega_t\\) and the expanded form \\(\\nabla^2 Y_t = 0.5 \\nabla^2 Y_{t-1} + \\omega_t\\), which is a stationary AR(1) series:\n\n\nCode\nplot(diff(Y[4:103],differences=2),type='l',ylab=expression(nabla^2*y[t]),xlab='Time index t')\n\n\n\n\n\nExplosive AR(3) process after twice differencing\n\n\n\n\nWhen one or more exact unit roots create random walk or explosive behavior, we can remove this nonstationary behavior by differencing. We call the amount of differencing the order of integration:\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\), and let the generating process for \\(\\boldsymbol{Y}\\) follow an ARMA(\\(p'\\),\\(q'\\)) process (where either \\(p'\\) or \\(q'\\) can be 0).\n\nIf \\(Y_t\\) is not stationary, but \\(\\nabla^d Y_t\\) is weakly stationary, then we say that \\(\\boldsymbol{Y}\\) is integrated with order d.\nIn which case, if the stationary series \\(\\nabla^d Y_t\\) can be written as an ARMA(p,q) process, we say that \\(\\boldsymbol{Y}\\) follows an autoregressive integrated moving average (ARIMA) model written ARIMA(p,d,q).",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ARIMA models</span>"
    ]
  },
  {
    "objectID": "arima.html#footnotes",
    "href": "arima.html#footnotes",
    "title": "ARIMA models",
    "section": "",
    "text": "We do know that the AR process is mean-zero since we have now expressed it as a weighted sum of mean-zero innovations.↩︎\nRemember, the characteristic polynomial can be factored into \\(\\Phi(B) = \\prod_i (1 - \\lambda_i B)\\)↩︎\nSo long as we use techniques like Method of Moments, Least Squares, or Maximum Likelihood which rely on first- and second-moment-based estimators.↩︎",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ARIMA models</span>"
    ]
  },
  {
    "objectID": "identificationinr.html",
    "href": "identificationinr.html",
    "title": "ARIMA identification in R",
    "section": "",
    "text": "Time series functions used in this document\nIn the table below, packages with italicized names will need to be installed, while the package names in a standard font face can be found in most base R distributions (though they may need to be loaded into your workspace).",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#time-series-functions-used-in-this-document",
    "href": "identificationinr.html#time-series-functions-used-in-this-document",
    "title": "ARIMA identification in R",
    "section": "",
    "text": "Time series functions used in the code below\n\n\nPackage\nFunction name\nPurpose\n\n\n\n\nzoo\nna.locf\nFill NAs from surrounding values\n\n\nMASS\nboxcox\nSuggest Box-Cox parameter lambda\n\n\ntseries\nadf.test\nADF stationarity test\n\n\ntseries\nkpss.test\nKPSS stationarity test\n\n\nstats\nacf\nCompute and plot autocorrelation\n\n\nstats\npacf\nCompute and plot partial autocor\n\n\nforecast\nauto.arima\nIdentify ARIMA order and coefs\n\n\nstats\narima\nFit a specific ARIMA model\n\n\nforecast\nforecast\nLook-ahead prediction of a ts model\n\n\nstats\nshapiro.test\nTest if data could be normal\n\n\nstats\nks.test\nTest data against a distribution\n\n\nlmtest\ndwtest\nDurbin-Watson test for lag 1 autocor\n\n\nstats\nBox.test\nLjung-Box test for multilag autocor",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#the-box-jenkins-approach-to-arima-identification",
    "href": "identificationinr.html#the-box-jenkins-approach-to-arima-identification",
    "title": "ARIMA identification in R",
    "section": "The Box-Jenkins approach to ARIMA identification",
    "text": "The Box-Jenkins approach to ARIMA identification\nThere are two parts to learning ARIMA identification and estimation: conceptually, you should understand the flowchart of what steps to take at what time, and practically, you should know which commands to run in R and how to use them.\nFor the conceptual part, we will adopt the classic “Box-Jenkins” approach:\n\nRecover a stationary series through reversible transformations\n\nBox-Cox transformations to stabilize the variance and/or linearize a trend\nLinear de-trending\nDifferencing (or multiple differencing), if needed\nSeasonal adjustment (either remove seasonality here or fit it in Step 2)\nYou can move on to the next step when stationarity tests suggest the transformations have been successful (give or take any seasonal effects)\n\nFit an ARIMA or SARIMA model to the transformed series\n\nEstimate the order through examination of the ACF and PACF plots\nRather than fit the differenced data to an ARMA model, explicitly fit a difference through the integration term \\(I=d\\).\nIf using SARIMA terms, fit the seasonal terms first before the main AR and MA terms\nUse AIC (or AICc), the standard errors of the parameters, and domain expertise to reach a stopping point\nYou can move on to the next step when you have a parsimonious, sensical model which ideally possesses both stationarity and invertibility and does not “lose” badly to other models\n\nConfirm the strengths and/or weaknesses of the model\n\nRe-plot the ACF and PACF to examine lingering dependencies or seasonality\nTest the innovations/residuals for autocorrelation or heteroskedasticity\nYou can stop when you are satisfied, or return to Step 1 if you feel a need for further improvement\n\n\nWe will explore a set of practical, functional ways to implement this method below.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#cleaning-the-data",
    "href": "identificationinr.html#cleaning-the-data",
    "title": "ARIMA identification in R",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nAs an example, let’s take a daily series of gold prices from the forecast package. The help file does not describe them very well; we know the series is meant to start Jan 1st 1985 and end March 31st 1989. There are a few missing values here, which can complicate any time series analysis.\n\nhelp(gold)\nplot(gold)\n\n\n\n\n\n\n\nsummary(gold)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  285.0   337.7   403.2   392.5   443.7   593.7      34 \n\n\nHow we treat missing values depends on how they appeared in our data. Did we fail to observe these values, or were there no values be observed? Are these weekends, exchange holidays, days where the data firm had a system outage, or records which were later deemed unreliable?\nFor now, we will drag the prices — preserving the last known price across any missing observations until a new price is recorded.\n\ngold.fill &lt;- na.locf(gold)\ngold[61:70]\n\n [1] 329.90 328.75 329.80 324.65 317.00 321.10 317.00     NA     NA 323.10\n\ngold.fill[61:70]\n\n [1] 329.90 328.75 329.80 324.65 317.00 321.10 317.00 317.00 317.00 323.10\n\n\nIn the original plot of the data, we see a very large spike about 2/3 of the way through the series. This could be a useful and interesting (albeit extreme) price movement, or it could be a data error. It’s worth examining, but without a date index it’s hard to know exactly when it happened.\n\n#which observations are we seeing?\ngold.fill[gold.fill&gt;500]\n\n[1] 502.75 593.70\n\n#which index values are they?\n(1:length(gold.fill))[gold.fill&gt;500]\n\n[1] 769 770\n\n#about when does this take place?\nas.Date('1985-01-01') + 769*(7/5)\n\n[1] \"1987-12-13\"\n\n#how big are the changes?\ndiff(gold.fill)[769:770]\n\n[1]   90.95 -106.65\n\n#what are the largest daily changes before and after this?\nrange(diff(gold.fill)[1:768])\n\n[1] -20.85  32.65\n\nrange(diff(gold.fill)[-1:-770])\n\n[1] -12.05  10.40\n\n\nAfter combing through several websites with their own historical gold series and even finding a scanned copy of a same-day newspaper article, I myself am tentatively concluding observavtion 770 ($593.70) to be a bad observation, possibly a finger slip from 9 to 0 ($503.70 fits much better with the historical data). Rather than make the change assuming it to be a typo, I will drag the prior day’s price:\n\ngold.fill[770] &lt;- gold.fill[769]\nplot(gold.fill)",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#assessing-and-recovering-stationarity",
    "href": "identificationinr.html#assessing-and-recovering-stationarity",
    "title": "ARIMA identification in R",
    "section": "Assessing and recovering stationarity",
    "text": "Assessing and recovering stationarity\nOur work here has already been outlined in one or two previous notes, to which I now refer the reader.\nInspecting the plot above, the gold prices look like they may be a random walk. They certainly don’t look stationary. First, let’s examine whether a Box-Cox transformation might be helpful.\n\n\n\n\n\n\nWarning\n\n\n\nNote that a Box-Cox transformation cannot be applied to series with negative values. Because of this, we usually apply Box-Cox transformations before differencing the data.\n\n\n\nboxcox(gold.fill~1)\n\n\n\n\n\n\n\n\nThe value \\(\\lambda=1\\) is within the confidence interval, suggesting that the variance is already stabilized.1\nWith no need for transformation, we can directly test the hypothesis that the original series is a random walk and the differenced series is stationary:\n\n#testing the original series: 'TRUE' suggests stationarity\nsuppressWarnings(adf.test(gold.fill)$p.value &lt; 0.05)\n\n[1] FALSE\n\nsuppressWarnings(kpss.test(gold.fill)$p.value &gt; 0.05)\n\n[1] FALSE\n\n#testing the first differences: 'TRUE' suggests stationarity\nsuppressWarnings(adf.test(diff(gold.fill))$p.value &lt; 0.05)\n\n[1] TRUE\n\nsuppressWarnings(kpss.test(diff(gold.fill))$p.value &gt; 0.05)\n\n[1] TRUE",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#identifying-the-arima-order-and-estimating-the-parameters",
    "href": "identificationinr.html#identifying-the-arima-order-and-estimating-the-parameters",
    "title": "ARIMA identification in R",
    "section": "Identifying the ARIMA order and estimating the parameters",
    "text": "Identifying the ARIMA order and estimating the parameters\nAlthough automated routines exist to help us identify the ARIMA parameters, they sometimes produce nonsensical or unhelpful results. It’s very useful to first form prior hypotheses about the correct order from domain knowledge and/or diagnostic analysis.\n\nFirst, we will assume that the correct integration order is \\(d=1\\), that is, first differences. From here, we will only be plotting the differenced cleaned data.\nSecond, we will not (at this time) fit any seasonal components. Although the data may suggest a slight seasonality, we will put it aside for now and focus on the main ARMA components.\nThird, we will be open to the possibility of a constant but non-zero mean for the differenced data, which is equivalent to a linear trend in the original undifferenced data.\nFourth, we will view models with high orders (say, \\(p \\gt 4\\) or \\(q \\gt 4\\)) with suspicion. Most processes worth describing with an ARIMA model have fairly low orders for \\(p,d\\) and \\(q\\).\n\nTo explore the AR and MA orders, it’s often helpful to look at an autocorrelation function (ACF) plot and a partial autocorrelation function (PACF) plot.\n\nacf(diff(gold.fill),lag.max=15,main='ACF of gold price changes')\n\n\n\n\n\n\n\npacf(diff(gold.fill),lag.max=15,main='PACF of gold price changes')\n\n\n\n\n\n\n\n\nThe ACF and PACF both show modest but significant correlations and partial correlations at the 1st lag. Beyond that, there are no clear patterns, though perhaps some slight evidence for weekly (lag=5) or bimonthly (lag=11, which accounting for weekends would be 15 days) cycles. As mentioned earlier, we will discard any hypothesis of seasonality at this time.\nTaken together, the ACF and PACF suggest a simple model, where the AR and MA orders are each either 1 or 0. The four models worth exploring, then, are:\n\nARIMA(0,1,0)\nARIMA(1,1,0)\nARIMA(0,1,1)\nARIMA(1,1,1)\n\nLet’s see what R comes up with. Here we will use the automated routine forecast::auto.arima, though readers may prefer the tidyverse alternative fable::ARIMA, both of which are simply convenience wrappers for a lot of tedious fiddling with the base function stats::arima:\n\nauto.arima(gold.fill,d=1,seasonal=FALSE,max.p=4,max.q=4,\n           stepwise=FALSE,approximation=FALSE)\n\nSeries: gold.fill \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.1398\ns.e.   0.0297\n\nsigma^2 = 18.45:  log likelihood = -3183.64\nAIC=6371.29   AICc=6371.3   BIC=6381.31\n\n\nR suggests a simple ARIMA(0,1,1) model, meaning that the first difference of the daily gold prices is weakly negatively correlated with each prior day’s innovation: gold prices correct their own shocks to some extent, and if new information causes them to climb or dip one day, they are likely to dip back or climb back (respectively) the next day.2\nThe estimated coefficient on the AR(1) parameter is small, but it’s more than four times its standard error, so we can be fairly sure that it’s a useful and significant modeling term. If we compare the AIC of 6371.56 to the other models, we should find that it’s the best fit (i.e. least AIC) amongst our four candidates:\n\nc(ARIMA011=arima(gold.fill,order=c(0,1,1))$aic,\n  ARIMA110=arima(gold.fill,order=c(1,1,0))$aic,\n  ARIMA111=arima(gold.fill,order=c(1,1,1))$aic,\n  ARIMA010=arima(gold.fill,order=c(0,1,0))$aic)\n\nARIMA011 ARIMA110 ARIMA111 ARIMA010 \n6371.289 6371.555 6373.279 6390.805 \n\n\nWe can tentatively model these daily gold prices with an ARIMA(0,1,1) model, though an inspection of the AICs reveals that an ARIMA(1,1,0) model would work almost as well.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#prediction",
    "href": "identificationinr.html#prediction",
    "title": "ARIMA identification in R",
    "section": "Prediction",
    "text": "Prediction\nSince we choose the ARIMA(0,1,1) representation, we can only meaningfully predict one day into the future. By the time we reach two days into the future, the series will be reacting only to future information (the new innovation at \\(t+2\\) and a small negative weight on the prior innovation at \\(t+1\\), both of which we do not observe and cannot estimate and have expectation 0.)\nIf we instead chose the ARIMA(1,1,0) representation, we could predict further into the future: the price change on day \\(t+2\\) would be weakly correlated with the price change on day \\(t+1\\), which we do not observe but is itself weakly correlated with the price change on day \\(t\\), which we do observe. However, the small AR coefficient and this geometric dampening mean that our forecasts will quickly lose any practical utility.\nStill, for those interested, we can hazard a guess for the first out-of-sample date, April 4th, 1989:\n\ngold.model &lt;- arima(gold.fill,order=c(0,1,1),include.mean=FALSE)\nforecast(gold.model,h=1)\n\n     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n1109       382.5475 377.0459 388.0491 374.1335 390.9615\n\npredict(gold.model,n.ahead=1)\n\n$pred\nTime Series:\nStart = 1109 \nEnd = 1109 \nFrequency = 1 \n[1] 382.5475\n\n$se\nTime Series:\nStart = 1109 \nEnd = 1109 \nFrequency = 1 \n[1] 4.292947\n\n\nI’ve shown the outputs from two different prediction functions, forecast::forecast and stats::predict.arima0, which differ in their presentation of results but, as you can see, recover exactly the same point forecast of $382.5475.\nOne prediction, taken in isolation, is never a great way to judge a model. We will discuss model metrics and model validation techniques later in this course. But as a tiny, tiny weight on the scale, consider the following:\n\nDaily gold prices were shown to be an unforecastable random walk\nDifferences in gold prices were shown to be mean-zero and stationary\nTherefore, we might assume the best prediction for April 3rd, 1989 would be the prior trading day’s price of $382.30 on March 31st, 1989 (call this the “naive” predictor)\nHowever, our ARIMA model suggests that the price will instead ‘correct’ itself slightly to $382.55\nAs it happens, the true closing price on April 3rd, 1989 was $385.30: both our predictions would have been too low, but the ARIMA prediction was somewhat closer than the naive prediction.",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#cleanup",
    "href": "identificationinr.html#cleanup",
    "title": "ARIMA identification in R",
    "section": "Cleanup",
    "text": "Cleanup\nIt’s always best to test your final model against some of your assumptions:\n\n#check for normality of the estimated innovations\n#  (note, only affects standard errors, not main findings)\nshapiro.test(gold.model$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  gold.model$residuals\nW = 0.90507, p-value &lt; 2.2e-16\n\nks.test(gold.model$residuals,pnorm,sd=sd(gold.model$residuals))\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  gold.model$residuals\nD = 0.095658, p-value = 3.124e-09\nalternative hypothesis: two-sided\n\n#check for lingering autocorrelation in the estimated innovations\ndwtest(gold.model$residuals~1)\n\n\n    Durbin-Watson test\n\ndata:  gold.model$residuals ~ 1\nDW = 2.0012, p-value = 0.5081\nalternative hypothesis: true autocorrelation is greater than 0\n\nBox.test(gold.model$residuals,fitdf=1,type='Ljung-Box')\n\n\n    Box-Ljung test\n\ndata:  gold.model$residuals\nX-squared = 0.00053447, df = 0, p-value &lt; 2.2e-16\n\nacf(gold.model$residuals)\n\n\n\n\n\n\n\npacf(gold.model$residuals)\n\n\n\n\n\n\n\n#check for stationarity of the estimated innovations\nsuppressWarnings(adf.test(gold.model$residuals))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  gold.model$residuals\nDickey-Fuller = -9.1815, Lag order = 10, p-value = 0.01\nalternative hypothesis: stationary\n\nsuppressWarnings(kpss.test(gold.model$residuals))\n\n\n    KPSS Test for Level Stationarity\n\ndata:  gold.model$residuals\nKPSS Level = 0.28653, Truncation lag parameter = 7, p-value = 0.1\n\nplot(gold.model$residuals)\n\n\n\n\n\n\n\nhist(gold.model$residuals)",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "identificationinr.html#footnotes",
    "href": "identificationinr.html#footnotes",
    "title": "ARIMA identification in R",
    "section": "",
    "text": "Further EDA would actually suggest otherwise, but this teaching example must move forward. Those interested might try plotting a rolling standard deviation calculation.↩︎\nDiscussion point: how would this be different than an ARIMA(1,1,0) model with the same coefficient sign and size?↩︎",
    "crumbs": [
      "2. ARIMA models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ARIMA identification in R</span>"
    ]
  },
  {
    "objectID": "ets.html",
    "href": "ets.html",
    "title": "ETS models",
    "section": "",
    "text": "Simple exponential smoothing\nHaving learned one major class of time series models — ARIMA models — let’s learn a second class of models to compare and contrast. This new group of models is collectively referred to as exponential smoothing. These models are sometimes also called “ETS” which stands for error, trend, seasonality.\nThe motivation for exponential smoothing is described well by Hyndman and Athansopoulous when they note the usefulness of a middle ground between two simple forecasting methods: dragging the last observations, and averaging the whole series:\nCode\npar(mfrow=c(1,2))\nset.seed(0117)\ny &lt;- arima.sim(list(ar=0.9),50)[5:40]\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main='Forecasting from last observation')\nlines(x=c(36,45),y=rep(y[36],2),lwd=2,lty=2,col='#0000ff')\npoints(x=36,y=y[36],pch=1,cex=2,col='#7f7f7f',lwd=2)\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main='Forecasting from series average')\nlines(x=c(36,45),y=rep(mean(y),2),lwd=2,lty=2,col='#0000ff')\nlines(x=c(1,36),y=rep(mean(y),2),col='#7f7f7f',lwd=2,lty=2)\n\n\n\n\n\nSimple forecasting strategies\nThe last observation tends to overfit — time series rarely stay in exactly the same place – and it also ignores the vast majority of the data. The series average tends to underfit — it doesn’t use any time series information at all — and it equally weights observations which may no longer be relevant.\nThere is a middle way. The most recent observation can get the most weight, and yet all observations will get some weight, with older observations receiving exponentially less weight:\n\\[\\hat{y}_{t+1} = \\alpha y_t + \\alpha (1 - \\alpha) y_{t-1} + \\alpha (1 - \\alpha)^2 y_{t-2} + \\ldots\\]\nLet us note a few properties of this equation before going further:\nBy changing \\(\\alpha\\) we can change the “memory” of the equation. Higher values of \\(\\alpha\\) correspond to a short memory, closer to the last observation. Lower values of \\(\\alpha\\) correspond to a long memory, closer to the series average:\nCode\npar(mfrow=c(1,2))\nses80 &lt;- y%*%(0.8*0.2^(35:0))\nses20 &lt;- y%*%(0.2*0.8^(35:0))\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main=expression(paste('SES using ',alpha == 0.8)))\nlines(x=c(36,45),y=rep(ses80,2),lwd=2,lty=2,col='#0000ff')\nsegments(x0=1:36,y0=-3,y1=2*0.8*0.2^(35:0)-3,col='#0000ff')\ntext(0,-2.8,'Weights',col='#0000ff',pos=4)\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main=expression(paste('SES using ',alpha == 0.2)))\nlines(x=c(36,45),y=rep(ses20,2),lwd=2,lty=2,col='#0000ff')\nsegments(x0=1:36,y0=-3,y1=2*0.2*0.8^(35:0)-3,col='#0000ff')\ntext(0,-2.8,'Weights',col='#0000ff',pos=4)\n\n\n\n\n\nSimple exponential smoothing models\nHyndman and Athansopoulos reformulate exponential smoothing into two phases. First, they define a smoothing series which iterates over each observation in the data, using the previous smoothing series value to help estimate the next smoothing series value. Second, they define a forecasting equation which uses the smoothing series to predict arbitrarily far into the future from any point.\n(This may feel very anticlimactic or unnecessarily complicated, but it is a scaffolding on which we will build features to capture trend and seasonality.)",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "ets.html#simple-exponential-smoothing",
    "href": "ets.html#simple-exponential-smoothing",
    "title": "ETS models",
    "section": "",
    "text": "I have written the equation using \\(y_t\\) instead of \\(Y_t\\), signaling to you that we need not associate this time series with probability distributions of a set of random variables. We can view exponential smoothing as a type of machine learning model, not parametric statistical inference.1\nThe sum of the weights, \\(\\sum_{i=0}^\\infty \\alpha (1 - \\alpha)^i\\) will add up to 1 only when \\(\\alpha \\in (0,1)\\) and when the series is truly infinite. However, our sample of data is finite, so the weights of our smoothing will not exactly equal 1.\nAt this time, the equation does not account for any trend over time or any seasonality. In other words, the forecast for all future forecast periods will be equal to the forecast for time \\(t+1\\).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{y}\\) be a time series observed during the regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). For some parameters \\(\\alpha \\in (0,1)\\) and \\(\\ell_0 \\in \\mathbb{R}\\), iteratively define the smoothing series \\(\\boldsymbol{\\ell}\\) as follows:\n\\[\\ell_t = \\alpha y_t + (1 - \\alpha) \\ell_{t-1}\\]\nAnd for a specific time index \\(\\tau\\) and an integer look-ahead period \\(h\\), define the forecasting series \\(\\hat{y}_{\\tau+h|\\tau}\\) as follows:\n\\[\\hat{y}_{\\tau+h|\\tau} = \\ell_\\tau\\]\nTogether, we refer to these two series as a simple exponential smoothing (SES) model.",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "ets.html#estimating-an-exponential-smoothing-model",
    "href": "ets.html#estimating-an-exponential-smoothing-model",
    "title": "ETS models",
    "section": "Estimating an exponential smoothing model",
    "text": "Estimating an exponential smoothing model\nRecently, exponential smoothing models have been synthesized with state space models, which allows them to be solved using methods common to the rest of statistical inference: distributional assumptions followed by likelihood optimization (including partial, quasi, or conditional likelihood) via gradient descent or other search techniques.\nHowever, we will discuss more basic techniques here which do not require distributional theory.\nThe forecasting method above suggests an error equation for each observation \\(y_t\\) as compared to its prior-period forecast, \\(\\hat{y}_{t|t-1}\\):\n\\[e_t = y_t - \\hat{y}_{t|t-1}; \\qquad \\textrm{SSE} = \\sum_{t=1}^n e_t^2\\]\nThe two parameters which control this sum of squared errors are \\(\\alpha\\), the smoothing parameter, and \\(\\ell_0\\), the choice of the initial level (recall that each \\(\\ell_i\\) is defined iteratively, and so there is no definition for \\(\\ell_0\\)). Therefore, using a Least Squares solution method, we may estimate:\n\\[\\hat{\\alpha}, \\hat{\\ell}_0 = \\mathop{\\textrm{argmin}}_{\\alpha \\in (0,1); \\ell \\in \\mathbb{R}} \\textrm{SSE}\\]\nAlthough this optimization cannot be solved as easily as OLS equations (we do not have a closed form solution which works for every dataset, such as \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^t\\boldsymbol{y}\\)), we can still solve easily enough with algorithmic techniques.",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "ets.html#adding-a-trend-component",
    "href": "ets.html#adding-a-trend-component",
    "title": "ETS models",
    "section": "Adding a trend component",
    "text": "Adding a trend component\nWhen our data trend in one direction due to random walk behavior, we should not incorporate the seeming trend into our predictions, but when the data trend due to deterministic drift, then it would make sense to incorporate this trend into our forecasts.\nThe slope of the trend will depend on which observations we use as our training sample. Just as before, we have a choice between using all of the data (underfitting) or only the most recent data (overfitting). And just as before, it makes sense to split the difference with another exponential smoothing filter:2\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{y}\\) be a time series observed during the regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). For some parameters \\(\\alpha, \\beta \\in (0,1)\\) and \\(\\ell_0, b_0 \\in \\mathbb{R}\\), iteratively define the level series \\(\\boldsymbol{\\ell}\\) and the trend series \\(\\boldsymbol{b}\\) as follows:\n\\[\\ell_t = \\alpha y_t + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1}\\]\nAnd for a specific time index \\(\\tau\\) and an integer look-ahead period \\(h\\), define the forecasting series \\(\\hat{y}_{\\tau+h|\\tau}\\) as follows:\n\\[\\hat{y}_{\\tau+h|\\tau} = \\ell_\\tau + hb_t\\]\nTogether, we refer to these three series as an exponential smoothing (SES) model with linear trend.\n\n\nNotice that the new level estimate at each time period is a weighted average between (i) the current observation and (ii) the predicted value of the same observation. Likewise, the new trend estimate at each time period is a weighted average between (i) the most current change in the estimated levels and (ii) the predicted value of the same change in levels.\nTo estimate this model we would solve a least squares equation for \\(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\ell}_0, \\textrm{and } \\hat{b}_0\\).\n\n\nCode\npar(mfrow=c(1,2))\ntrend1 &lt;- ets(y,model='AAN',damped=FALSE,alpha=0.4,beta=0.3)\ntrend2 &lt;- ets(y,model='AAN',damped=FALSE)\n\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main=expression(paste('ETS using ',list(alpha == 0.4, beta == 0.3))))\nlines(x=c(37:45),y=forecast(trend1,h=9)$mean,lwd=2,lty=2,col='#7f00ff')\nsegments(x0=-0.2+(1:36),y0=-3,y1=2*0.4*0.6^(35:0)-3,col='#0000ff')\nsegments(x0=0.2+(1:36),y0=-3,y1=2*0.3*0.7^(35:0)-3,col='#ff0000')\ntext(0,-2.4,expression(paste(alpha,' weights')),col='#0000ff',pos=4)\ntext(0,-2.8,expression(paste(beta,' weights')),col='#ff0000',pos=4)\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main=expression(paste('ETS using ',list(alpha %~~% 0.9, beta &lt; 0.01))))\nlines(x=c(37:45),y=forecast(trend2,h=9)$mean,lwd=2,lty=2,col='#7f00ff')\nsegments(x0=-0.2+(1:36),y0=-3,y1=2*trend2$par[1]*(1-trend2$par[1])^(35:0)-3,col='#0000ff')\nsegments(x0=0.2+(1:36),y0=-3,y1=2*trend2$par[2]*(1-trend2$par[2])^(35:0)-3,col='#ff0000')\ntext(0,-2.4,expression(paste(alpha,' weights')),col='#0000ff',pos=4)\ntext(0,-2.8,expression(paste(beta,' weights')),col='#ff0000',pos=4)\n\n\n\n\n\nExponential smoothing models with trend",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "ets.html#adding-a-damped-trend-component",
    "href": "ets.html#adding-a-damped-trend-component",
    "title": "ETS models",
    "section": "Adding a damped trend component",
    "text": "Adding a damped trend component\nExtrapolating trends into the future is a dangerous business. My newborn daughter has gained about 30 grams (or 1 ounce) per day over the past two weeks. This trend, well-defined over my entire sampling period, suggests that by the time she’s twenty years old, she will weigh 220 kilograms (or 480 pounds).\nBecause linear extrapolation can so often lead to unrealistic forecasts, sometimes we choose to “dampen” a forecasted trend, meaning that we allow the slope to gradually become flat. The mechanism here will be a new exponential term \\(\\phi\\), which is usually set close to 1. From the equations below you can see that when \\(\\phi \\approx 1\\) it will not greatly change the iterative smoothing series, but that it will increasingly change the forecasted slope when extrapolating to large look-ahead periods \\(h\\):\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{y}\\) be a time series observed during the regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). For some parameters \\(\\alpha, \\beta, \\phi \\in (0,1)\\) and \\(\\ell_0, b_0 \\in \\mathbb{R}\\), iteratively define the level series \\(\\boldsymbol{\\ell}\\) and the trend series \\(\\boldsymbol{b}\\) as follows:\n\\[\\ell_t = \\alpha y_t + (1 - \\alpha)(\\ell_{t-1} + \\phi b_{t-1})\\]\n\\[b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta) \\phi b_{t-1}\\]\nAnd for a specific time index \\(\\tau\\) and an integer look-ahead period \\(h\\), define the forecasting series \\(\\hat{y}_{\\tau+h|\\tau}\\) as follows:\n\\[\\hat{y}_{\\tau+h|\\tau} = \\ell_\\tau + (\\phi + \\phi^2 + \\ldots + \\phi^h)b_t\\]\nTogether, we refer to these three series as an exponential smoothing (SES) model with a damped linear trend.\n\n\n\n\nCode\nrequire(forecast)\npar(mfrow=c(1,2))\ntrend1 &lt;- ets(y,model='AAN',damped=TRUE,alpha=0.8,beta=0.1,phi=0.95)\ntrend2 &lt;- ets(y,model='AAN',damped=TRUE,alpha=0.8,beta=0.1,phi=0.8)\n\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main=expression(paste('ETS damped with ',phi == 0.95)))\nlines(x=c(37:45),y=forecast(trend1,h=9)$mean,lwd=2,lty=2,col='#7f00ff')\nsegments(x0=37:45,y0=-3,y1=0.95^(1:9)-3,col='#7f00ff')\ntext(10,-2.8,expression(paste('weight on ',b[t])),col='#7f00ff',pos=4)\nplot(y,type='l',xlim=c(0,45),ylab='Y',ylim=c(-3,2),\n     main=expression(paste('ETS damped with ',phi == 0.8)))\nlines(x=c(37:45),y=forecast(trend2,h=9)$mean,lwd=2,lty=2,col='#7f00ff')\nsegments(x0=37:45,y0=-3,y1=0.8^(1:9)-3,col='#7f00ff')\ntext(10,-2.8,expression(paste('weight on ',b[t])),col='#7f00ff',pos=4)\n\n\n\n\n\nExponential smoothing models with trend",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "ets.html#adding-an-additive-seasonal-component-holt-winters-models",
    "href": "ets.html#adding-an-additive-seasonal-component-holt-winters-models",
    "title": "ETS models",
    "section": "Adding an additive seasonal component (Holt-Winters models)",
    "text": "Adding an additive seasonal component (Holt-Winters models)\nOften the data show such a clear seasonality (let’s say with period \\(m\\)) that it would be foolish not to use this information when performing forecasting or smoothing the historical values. Once again we are confronted with the dilemma of estimating the seasonality from the entire series (which uses more data, but possibly includes stale data), or estimating the seasonality from only the most recent season (which keeps only the freshest values, but might lose helpful historical context). And once again, we thread the needle by exponentially up-weighting the most recent data and down-weighting the oldest data, using a new smoothing parameter which may be estimated from the data.\nExponential smoothing models with seasonal components are sometimes called Holt-Winters models. Let us start by defining an additive seasonal component.3\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{y}\\) be a time series observed during the regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\). For some parameters \\(\\alpha, \\beta, \\phi, \\gamma \\in (0,1)\\) and \\(\\ell_0, b_0, s_0, s_{-1}, \\ldots, s_{1-m} \\in \\mathbb{R}\\), iteratively define the level series \\(\\boldsymbol{\\ell}\\), the trend series \\(\\boldsymbol{b}\\), and the seasonal series \\(\\boldsymbol{s}\\) as follows:\n\\[\\ell_t = \\alpha (y_t - s_{t-m}) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1})\\]\n\\[b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta) b_{t-1}\\]\n\\[s_t = \\gamma(y_t - \\ell_t) + (1 - \\gamma) s_{t-m}\\]\nAnd for a specific time index \\(\\tau\\) and an integer look-ahead period \\(h\\), define the forecasting series \\(\\hat{y}_{\\tau+h|\\tau}\\) as follows:\n\\[\\hat{y}_{\\tau+h|\\tau} = \\ell_\\tau + hb_t + s_{\\tau - (h \\,\\textrm{mod}\\, m)}\\]\nTogether, we refer to these four series as a Holt-Winters model with additive seasonality.\n\n\n\n\nCode\nair.ets1 &lt;- ets(AirPassengers,model='AAA',damped=FALSE)\nplot(air.ets1)\n\n\n\n\n\nETS model as a sum of three components\n\n\n\n\nThe plot above shows how an ETS model with trend and seasonality might be fit to a real-world dataset (here, the classic Box-Jenkins dataset of monthly airline passengers). Note that we see signs of an imperfect fit:\n\nLooking at the top plot of the original data, the seasonal variance seems to be growing proportionally with the mean number of passengers.\nLooking at the bottom plot of the estimated seasonal effects, they seem to be nearly constant over time.\nLooking at the second plot of the estimated level, we see that the level has been co-opted into “fixing” the mistakes being made by the seasonal series. In earlier years the level counteracts the seasonality, muting the effects, while in later years the level complements the seasonality, amplifying the effects.\n\nThis model isn’t necessarily bad, but we might be able to improve it.",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "ets.html#exponential-smoothing-models-with-multiplicative-components",
    "href": "ets.html#exponential-smoothing-models-with-multiplicative-components",
    "title": "ETS models",
    "section": "Exponential smoothing models with multiplicative components",
    "text": "Exponential smoothing models with multiplicative components\nSometimes the seasonality effects on a series are additive/linear: for example, the difference between daytime and nighttime temperatures is fairly steady in Chicago whether it’s January (mean temperature 0 C) or July (mean temperature 30 C).\nOther times the seasonality effects on a series are multiplicative/proportional: for example, a small florist’s shop might notice a +15% revenue increase in February, around Valentine’s Day. This 15% increase might persist as the business grows from monthly revenues of $10,000 to $100,000 to $1,000,000.\nHyndman and Athanasopoulous present the formulae describing multiplicative seasonality and present a taxonomy of ETS models which include multiplicative trend effects as well.\nEven in cases when seasonality looks multiplicative, a logarithmic (or any appropriate Box-Cox) transformation might convert the series back to additive errors and additive seasonality. Compare three models for the classic Box-Jenkins airline passengers data:\n\n\nCode\nair.ets1 &lt;- ets(AirPassengers,model='AAA',damped=FALSE)\nplot(air.ets1)\nlegend(x='topleft',legend=c('Observations','Smoothing fit'),lty=1,col=c('#000000','#0000ff'),bty='n')\n\n\n\n\n\nETS with additive seasonality for original data\n\n\n\n\n\n\nCode\nair.ets2 &lt;- ets(AirPassengers,model='AAA',damped=FALSE,lambda='auto',biasadj=TRUE)\nplot(air.ets2)\nlegend(x='topleft',legend=c('Observations','Smoothing fit'),lty=1,col=c('#000000','#0000ff'),bty='n')\n\n\n\n\n\nETS with additive seasonality for Box-Cox transformed data\n\n\n\n\n\n\nCode\nair.ets3 &lt;- ets(AirPassengers,model='MAM',damped=FALSE)\nplot(air.ets3)\nlegend(x='topleft',legend=c('Observations','Smoothing fit'),lty=1,col=c('#000000','#0000ff'),bty='n')\n\n\n\n\n\nETS with multiplicative trend and seasonality for original data\n\n\n\n\nAlthough all of these models succeed in decomposing or explaining much of the variance in this seasonal time series, we might find ways to choose between them. We could look at which series had the cleanest residual patterns, or best look-ahead forecasting accuracy, or lowest AIC, or (as here) lowest in-sample RMSE:\n\n\nCode\nair.ets.summ &lt;- rbind(sqrt(air.ets1$mse),\n                      sqrt(mean((AirPassengers-air.ets2$fitted)^2)),\n                      sqrt(air.ets3$mse))\ncolnames(air.ets.summ) &lt;- c('In-sample RMSE')\nrownames(air.ets.summ) &lt;- c('AAA, no Box-Cox', 'AAA, w/ Box-Cox', 'MAM, no Box-Cox')\nprint(air.ets.summ)\n\n\n                In-sample RMSE\nAAA, no Box-Cox       17.01495\nAAA, w/ Box-Cox       10.44758\nMAM, no Box-Cox       11.26797",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "ets.html#footnotes",
    "href": "ets.html#footnotes",
    "title": "ETS models",
    "section": "",
    "text": "I say we can view them as ML techniques, but it’s also possible to frame them as Gaussian state space models with well-defined distributions and likelihood functions.↩︎\nHyndman and Athanasopoulos use an alternative parameterization for \\(\\beta\\) which takes a range of \\((0,\\alpha)\\). What these lecture notes refer to as \\(\\beta\\) is what their textbook refers to as \\(\\beta^*\\).↩︎\nHyndman and Athanasopoulos use an alternative parameterization for \\(\\gamma\\) which takes a range of \\((0,1-\\alpha)\\). What these lecture notes refer to as \\(\\gamma\\) is what their textbook refers to as \\(\\gamma^*\\).↩︎",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>ETS models</span>"
    ]
  },
  {
    "objectID": "sarima.html",
    "href": "sarima.html",
    "title": "SARIMA models",
    "section": "",
    "text": "Motivation\nWe have already seen several datasets which display moderate or severe seasonality, but the ARIMA models presented earlier did not explicitly model that seasonality. We can now add that functionality to the basic ARIMA structure, creating a type of ARIMA model often known as seasonal ARIMA or SARIMA.\nLet’s say that we are looking at a new monthly time series for the first time, and try to analyze it using the Box-Jenkins ARIMA modeling approach. The first five years of a ten-year dataset are plotted below:\nCode\nset.seed(0120)\ny &lt;- 2+ arima.sim(model=list(ar=c(0.5,rep(0,10),0.3)),n=120)\nplot(y[1:60],type='l',ylab=expression(Y[t]),ylim=c(0,6))  \nabline(v=0.5+12*(1:4),lty=2,col='#7f7f7f')\ntext(x=12*(0:4),y=0.1,labels=paste('Year',1:5),pos=4,col='#7f7f7f')\n\n\n\n\n\nFirst five years of a seasonal time series\nEven upon a quick visual inspection, we see some signs of seasonality, with generally two peaks per year. The structure of these internal dependencies become clearer when we inspect the ACF and PACF plots:\nCode\npar(mfrow=c(1,2))\nacf(y,lag.max=25,main='ACF plot',ylab='Autocorrelation')\nmtext(c('1 year','2 years'),side=1,line=0,at=c(12,24),col='#7f7f7f',cex=0.75)\npacf(y,lag.max=25,main='PACF plot',ylab='Partial autocorrelation')\nmtext(c('1 year','2 years'),side=1,line=0,at=c(12,24),col='#7f7f7f',cex=0.75)\n\n\n\n\n\nAutocorrelation structure of a seasonal timeseries\nThe ACF plot shows the persistence typical of an AR process, but rather than scaling geometrically down to zero, we see long-term cyclical behavior with peaks at lags 12 and 24 (1 and 2 years). The PACF plot shows the dramatic dropoff after lag 1 typical of an AR(1) process, but then a second peak at lag 12 (1 year).\nOur conclusion is that this time series process can be at least weakly predicted not only by what happened last month, but also what happened last year at this time. Many real-world datasets display this type of double dependency — for example, my inspiration for this simulated data was rainfall totals. There are wet years and dry years, so knowing if the previous month’s rainfall totals were high or low is often helpful to determining whether this month’s rainfall levels were high or low \\((Y_t \\approx Y_{t-1})\\). But there are also wet months and dry months, so knowing the rainfall levels 12 months ago is also helpful in determining this month’s rainfall levels \\((Y_t \\approx Y_{t-12})\\).\nWe can model this data as a long-order AR process with nonzero coefficients at 1 and 12 lags, something like AR(0.5,0,0,0,0,0,0,0,0,0,0,0.3), but this seems (and is) unnecessarily cumbersome for a data property we expect to encounter again and again. Instead, we will find new efficiencies through the use of the backshift operator.",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SARIMA models</span>"
    ]
  },
  {
    "objectID": "sarima.html#definition",
    "href": "sarima.html#definition",
    "title": "SARIMA models",
    "section": "Definition",
    "text": "Definition\n\n\n\n\n\n\nNote\n\n\n\nLet \\(\\boldsymbol{Y}\\) be a time series random variable observed at regular time periods \\(T = \\{1, 2, \\ldots, n\\}\\), with a known seasonal period of \\(m\\). Let \\(\\boldsymbol{\\omega}\\) be a white noise process observed at the same time periods. If there exist characteristic polynomials \\(\\Phi(B),\\varPhi_s(B^m), \\Theta(B), \\varTheta_s(B^m)\\) with polynomial power \\(p, P, q, Q\\) respectively, such that the relationship between \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{\\omega}\\) can be written:\n\\[Y_t \\cdot \\Phi(B) \\cdot \\varPhi_s(B^m) (1 - B)^d (1 - B^m)^D= \\omega_t \\cdot \\Theta(B) \\cdot \\varTheta_s(B^m)\\]\nThen we say that \\(\\boldsymbol{Y}\\) follows an seasonal autoregressive integrated moving average (SARIMA) model written SARIMA(p,d,q)(P,D,Q)m.\n\n\nThat equation looks menacingly dense, so maybe we can tease it apart:\n\\[Y_t \\cdot \\Phi(B) = 1 - \\phi_1Y_{t-1} - \\phi_2Y_{t-2} - \\ldots - \\phi_pY_{t-p}\\]\nThe first polynomial \\(\\Phi(B)\\) describes autocorrelation between \\(Y_t\\) and its immediate lags.\n\\[Y_t \\cdot \\varPhi_s(B^m) = 1 - \\varphi_1Y_{t-m} - \\varphi_2Y_{t-2m} - \\ldots - \\varphi_PY_{t-Pm}\\]\nThe second polynomial \\(\\varPhi_s(B)\\) describes autocorrelation between \\(Y_t\\) and its same-season values in each prior cycle.\n\\[Y_t \\cdot (1 - B)^d = (Y_t - Y_{t-1})(1 - B)^{d-1} \\ldots = \\nabla^dY_t\\]\nThe third polynomial \\((1 - B)^d\\) differences the series \\(d\\) times to achieve stationarity.\n\\[Y_t \\cdot (1 - B^m)^D = (Y_t - Y_{t-m})(1 - B)^{D-1} \\ldots = \\nabla^D_mY_t\\]\nThe fourth polynomial \\((1 - B^m)^D\\) shifts the analysis to differences (or second differences, etc.) from same-period values in the past seasonal periods.1\n\\[\\omega_t \\cdot \\Theta(B) = 1 + \\theta_1\\omega_{t-1} + \\theta_2\\omega_{t-2} + \\ldots + \\theta_q\\omega_{t-q}\\]\nThe fifth polynomial \\(\\Theta(B)\\) describes correlation between \\(Y_t\\) and the immediate prior innovations in the times series.\n\\[\\omega_t \\cdot \\varTheta_s(B^m) = 1 + \\vartheta_1\\omega_{t-m} + \\vartheta_2\\omega_{t-2m} + \\ldots + \\vartheta_q\\omega_{t-Qm}\\]\nThe sixth polynomial \\(\\varTheta_s(B^m)\\) describes correlation between \\(Y_t\\) and the same-season innovations in each prior cycle.",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SARIMA models</span>"
    ]
  },
  {
    "objectID": "sarima.html#examples",
    "href": "sarima.html#examples",
    "title": "SARIMA models",
    "section": "Examples",
    "text": "Examples\n\nHypothetical monthly series\nLet’s return to our hypothetical monthly dataset. We saw the first five years above. Let’s plot the last three years, with room for forecasting:\n\n\nCode\nplot(c(rep(NA,84),y[85:120]),type='l',ylab=expression(Y[t]),xlim=c(85,132),ylim=c(0,6))  \nabline(v=0.5+12*(8:10),lty=2,col='#7f7f7f')\ntext(x=12*(7:10),y=0.1,labels=paste('Year',8:11),pos=4,col='#7f7f7f')\n\n\n\n\n\nLast three years of a seasonal time series\n\n\n\n\nNotice that the seasonal pattern has shifted. Instead of two peaks in spring and autumn, we now see one valley in summer and one peak in winter (this change began as early as Year 5, if you compare with the plot above). Has this wrecked the seasonality of the series? No! In a SARIMA model, all we require is that there be some correlation with the past same-season values, not that the first full period looks just like the last.\nAlthough we could let an automated routine pick our model for us, based on the ACF and PACF plots above we might feel comfortable diagnosing a SARIMA(1,0,0)(1,0,0)12:\n\n\nCode\n(y.sar &lt;- arima(y,order=c(1,0,0),seasonal=list(order=c(1,0,0),period=12)))\n\n\n\nCall:\narima(x = y, order = c(1, 0, 0), seasonal = list(order = c(1, 0, 0), period = 12))\n\nCoefficients:\n         ar1    sar1  intercept\n      0.5146  0.3706     2.5023\ns.e.  0.0803  0.0897     0.2888\n\nsigma^2 estimated as 1.06:  log likelihood = -174.84,  aic = 357.67\n\n\nWe can plot the forecast of this SARIMA model and, since ARIMA models are based in distributional theory and an assumption of Gaussian white noise, we can plot an 80% confidence region for our predictions:\n\n\nCode\nplot(forecast(y.sar,h=12),include=36,ylab=expression(Y[t]),ylim=c(0,6),\n     fcol='#0000ff',pi.col='#0000ff7f',xlab='Index')  \nabline(v=0.5+12*(8:10),lty=2,col='#7f7f7f')\ntext(x=12*(7:10),y=0.1,labels=paste('Year',8:11),pos=4,col='#7f7f7f')\n\n\n\n\n\n12-month lookahead forecasts from SARIMA model\n\n\n\n\n\n\nAirline passengers data\nWe also examined this data when learning about exponential smoothing models.\n\n\nCode\nplot(AirPassengers)\n\n\n\n\n\nBox-Jenkins airline passenger data\n\n\n\n\nThe data show both clear seasonality as well as clear non-stationarity. A Box-Cox transform might help:\n\n\nCode\npar(mfrow=c(1,2))\nboxcox(AirPassengers~1)\ntitle(main=expression(paste('Likelihoods for Box-Cox ',lambda)))\nplot(log(AirPassengers))\ntitle(main=expression(paste('Box-Cox transformed data, ',lambda == 0)))\n\n\n\n\n\n\n\n\n\nWe seem to have reached trend stationarity. Tests will confirm:\n\n\nCode\nsuppressWarnings(adf.test(log(AirPassengers)))\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(AirPassengers)\nDickey-Fuller = -6.4215, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\nsuppressWarnings(kpss.test(log(AirPassengers),null='Trend'))\n\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  log(AirPassengers)\nKPSS Trend = 0.11267, Truncation lag parameter = 4, p-value = 0.1\n\n\nWe are ready to fit a SARIMA model:\n\n\nCode\nair.sarima &lt;- auto.arima(AirPassengers,lambda=0)\nsummary(air.sarima)\n\n\nSeries: AirPassengers \nARIMA(0,1,1)(0,1,1)[12] \nBox Cox transformation: lambda= 0 \n\nCoefficients:\n          ma1     sma1\n      -0.4018  -0.5569\ns.e.   0.0896   0.0731\n\nsigma^2 = 0.001371:  log likelihood = 244.7\nAIC=-483.4   AICc=-483.21   BIC=-474.77\n\nTraining set error measures:\n                     ME     RMSE      MAE          MPE     MAPE     MASE\nTraining set 0.05140415 10.15504 7.357553 -0.004079018 2.623636 0.229706\n                    ACF1\nTraining set -0.03689682\n\n\nNotice that although the series has a clear nonzero mean and a clear deterministic trend, neither effect appears in the model summary! We will explore this more in a moment.\nWe see a sensible model with few parameters, each with estimates several times larger than their standard errors, which is usually comforting to a statistician. This SARIMA(0,1,1)(0,1,1)12 model suggests the following:\n\nFirst, we are looking at a double-differenced model: once differenced against its first lag, and then re-differenced against its 12th lag. The double differencing is why we do not need to estimate a mean/level or a trend/slope: the double difference of a linear trend is zero.\nSecond, there are no AR terms or seasonal AR terms (somewhat surprisingly!), only MA and seasonal MA terms. Rather than creating an echoing chain of dependency across months and across years, we will simply focus on what happened last month and what happened 12 months ago.\nThird, the MA(1) term and seasonal MA(1) term are negative, meaning that we see some degree of error correction. If the innovation from last month was estimated to be extreme, then we will partially ignore it when modeling this month’s difference. If the innovation from 13 months ago was estimated to be extreme, then we will partially ignore it when constructing the year-ago monthly difference which becomes the reference point for this month’s difference.\nFourth, consider that this model implies the following approximate equivalency: \\(\\log Y_t - \\log Y_{t-1} \\approx \\log Y_{t-12} - \\log Y_{t-13}\\). We can re-arrange terms to say that:\n\n\\[Y_t = e^{\\log Y_{t-1} + \\log(Y_{t-12}/Y_{t-13})} = Y_{t-1} \\times \\frac{Y_{t-12}}{Y_{t-13}}\\]\nIn other words, our model suggests that the best prediction of this month’s airline passenger volume is equal to last month’s volume, plus the prior-year proportional increase between the same two months, plus or minus some error correction which is meant to mute the effect of one-off shocks and unusually large innovations. Between a log-transform and the two types of differencing, we were able to describe a multiplicative growth and multiplicative seasonality model using a representation intended for linear, stationary time series!",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SARIMA models</span>"
    ]
  },
  {
    "objectID": "sarima.html#footnotes",
    "href": "sarima.html#footnotes",
    "title": "SARIMA models",
    "section": "",
    "text": "Recall that \\(\\nabla^D\\) represents the Dth-order difference, while \\(\\nabla_m\\) represents the first-order difference between \\(Y_t\\) and its mth lag \\(Y_{t-m}\\), so \\(\\nabla^D_m\\) is the Dth-order difference using the mth lag.↩︎",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SARIMA models</span>"
    ]
  },
  {
    "objectID": "seasonalityinr.html",
    "href": "seasonalityinr.html",
    "title": "Seasonal models in R",
    "section": "",
    "text": "Time series functions used in this document\nIn the table below, packages with italicized names will need to be installed, while the package names in a standard font face can be found in most base R distributions (though they may need to be loaded into your workspace).",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Seasonal models in R</span>"
    ]
  },
  {
    "objectID": "seasonalityinr.html#time-series-functions-used-in-this-document",
    "href": "seasonalityinr.html#time-series-functions-used-in-this-document",
    "title": "Seasonal models in R",
    "section": "",
    "text": "Time series functions used in the code below\n\n\nPackage\nFunction name\nPurpose\n\n\n\n\nstats\nwindow\nSubset a time series\n\n\nforecast\nets\nFit exponential smoothing models\n\n\nforecast\nforecast\nLook-ahead prediction of a ts model\n\n\ntseries\nadf.test\nADF stationarity test\n\n\ntseries\nkpss.test\nKPSS stationarity test\n\n\nforecast\nplot.Arima\nPlot the inverse AR and MA roots\n\n\nforecast\nplot.forecast\nPlot predictions from a ts model\n\n\nstats\nBox.test\nBox-Ljung test for autocorrelation",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Seasonal models in R</span>"
    ]
  },
  {
    "objectID": "seasonalityinr.html#fitting-ets-models-in-r",
    "href": "seasonalityinr.html#fitting-ets-models-in-r",
    "title": "Seasonal models in R",
    "section": "Fitting ETS models in R",
    "text": "Fitting ETS models in R\nThe workhorse function forecast::ets (or its tidyverse equivalent, fable::ETS) can fit any of the exponential smoothing models described in Forecasting: Principles and Practice 2nd edition or 3rd edition, respectively.\nLet’s apply it to a quarterly series of UK quarterly gas consumption:1\n\nplot(UKgas)\n\n\n\n\n\n\n\ngas.train &lt;- window(UKgas,end=1985.25)\ngas.test &lt;- window(UKgas,start=1985.5)\n\nThe data are clearly non-stationary, with a multiplicative seasonality (perhaps corresponding to a growing population). We might be able to control this multiplicative effect with a Box-Cox transformation, or through the explicit use of multiplicative errors and seasonal effects. Let’s examine those two options, holding the last six quarters back as a prediction testbed.\n\nFitting an additive-only model with a Box-Cox transformation\n\nets.aaa &lt;- ets(gas.train,model='AAA',lambda='auto')\nsummary(ets.aaa)\n\nETS(A,Ad,A) \n\nCall:\nets(y = gas.train, model = \"AAA\", lambda = \"auto\")\n\n  Box-Cox transformation: lambda= -0.4329 \n\n  Smoothing parameters:\n    alpha = 0.0306 \n    beta  = 0.0281 \n    gamma = 0.5365 \n    phi   = 0.98 \n\n  Initial states:\n    l = 2.0142 \n    b = 6e-04 \n    s = -3e-04 -0.0452 0.0084 0.0371\n\n  sigma:  0.0102\n\n      AIC      AICc       BIC \n-453.1036 -450.6861 -426.8539 \n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE       ACF1\nTraining set 3.536558 30.18087 19.39172 0.5496738 6.064928 0.7319039 -0.1072051\n\nplot(ets.aaa)\n\n\n\n\n\n\n\n\nThe low values of the smoothing parameters \\(\\alpha\\) and \\(\\beta\\) suggest that long-ago observations are still quite useful in setting the level and the trend for the current quarter’s gas prices. In contrast, the relatively high value of the smoothing parameter \\(\\gamma\\) suggests that the most recent seasonal pattern is more helpfully informative than prior years’ seasonal patterns.\nA damped trend was selected over an undamped trend — we can manually change this with the damped=FASE argument, or keep it. Since the damping parameter \\(\\phi\\) is 0.98, the difference between the two models will be relatively small.\nNote the significant transformation achieved with the Box-Cox transformation (where \\(\\lambda \\approx -0.43\\)), which changes the series to something more closely approximating trend-stationarity. And yet, the seasonal plot still shows nonconstant variance (which is likely why the parameter \\(\\gamma\\) is relatively high). It’s hard to know whether this is a bug or a feature, i.e. whether a transformation could draw useful more information out of the past or not.\n\n\nFitting a multiplicative model without Box-Cox transformation\n\nets.zam &lt;- ets(gas.train,model='ZAM',lambda=NULL)\nsummary(ets.zam)\n\nETS(M,A,M) \n\nCall:\nets(y = gas.train, model = \"ZAM\", lambda = NULL)\n\n  Smoothing parameters:\n    alpha = 0.0279 \n    beta  = 0.0277 \n    gamma = 0.5994 \n\n  Initial states:\n    l = 123.9697 \n    b = 0.7846 \n    s = 0.9453 0.6771 1.0464 1.3312\n\n  sigma:  0.1164\n\n     AIC     AICc      BIC \n1170.080 1172.037 1193.705 \n\nTraining set error measures:\n                   ME     RMSE     MAE        MPE     MAPE     MASE       ACF1\nTraining set 3.835205 30.89018 20.1769 0.06514202 6.418552 0.761539 -0.1309169\n\nplot(ets.zam)\n\n\n\n\n\n\n\n\nBy using the argument ‘Z’, we can tell the ets() function to choose additive, multiplicative, or none for each component. In this case, I was sure about requesting an additive trend (ets() won’t actually fit most multiplicative trends), and a multiplicative seasonality, but was unsure about whether the error process was best viewed as additive or multiplicative. R chose multiplicative.\nHowever, the summary is disappointing. This looks like a strictly worse version of our earlier model. Similar smoothing parameters, similar (but worse) model metrics, and a more chaotic slope plot which suggests that the model is trying to cram a lot of information into a slope in order to incorporate large and unexpected changes in the series. Let’s try restoring the Box-Cox transformation.\n\n\nLetting R decide\n\nets.zzz &lt;- ets(gas.train,model='ZZZ',lambda='auto',biasadj=TRUE)\nsummary(ets.zzz)\n\nETS(A,Ad,A) \n\nCall:\nets(y = gas.train, model = \"ZZZ\", lambda = \"auto\", biasadj = TRUE)\n\n  Box-Cox transformation: lambda= -0.4329 \n\n  Smoothing parameters:\n    alpha = 0.0306 \n    beta  = 0.0281 \n    gamma = 0.5365 \n    phi   = 0.98 \n\n  Initial states:\n    l = 2.0142 \n    b = 6e-04 \n    s = -3e-04 -0.0452 0.0084 0.0371\n\n  sigma:  0.0102\n\n      AIC      AICc       BIC \n-453.1036 -450.6861 -426.8539 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -1.311548 30.33744 19.56546 -0.5017562 6.134186 0.7384615\n                    ACF1\nTraining set -0.08345354\n\n\nR confirms our earlier choice of the additive Holt-Winters model with damped trend. By using the biasadj=TRUE argument, we can remove bias from our forecast estimates (after Box-Cox transforming the data to fit the model, the inverse-Box-Cox forecasts will be for the median value of \\(\\hat{y}_t\\), not the mean).\nHow would this model finish out our series?\n\nplot(forecast(ets.zzz,h=6),include=12,PI=FALSE)\npoints(gas.test)\n\n\n\n\n\n\n\n\nIn the code above I’ve set h=6 to define the number of forecasted periods, include=12 to limit how much of the prior oobserved series appears on the plot, and PI=FALSE to remove the prediction intervals (which rely upon distributional theory we have not yet decided to use).\nOn balance, I would say these predictions are quite good! The seasonal cycle and past exponential growth both carry forward well into 1986.",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Seasonal models in R</span>"
    ]
  },
  {
    "objectID": "seasonalityinr.html#fitting-sarima-models-in-r",
    "href": "seasonalityinr.html#fitting-sarima-models-in-r",
    "title": "Seasonal models in R",
    "section": "Fitting SARIMA models in R",
    "text": "Fitting SARIMA models in R\nWe can try to fit a SARIMA model to the same dataset. Because ARIMA models assume a stationary series, we will first have to do what we can to achieve stationarity:\n\ngas.sarima &lt;- auto.arima(gas.train,approximation=FALSE,stepwise=FALSE,seasonal=TRUE,lambda='auto')\nsummary(gas.sarima)\n\nSeries: gas.train \nARIMA(2,0,2)(0,1,1)[4] with drift \nBox Cox transformation: lambda= -0.4328729 \n\nCoefficients:\n         ar1      ar2      ma1     ma2     sma1   drift\n      1.4412  -0.4796  -1.6261  0.7314  -0.4244  0.0013\ns.e.  0.1918   0.1956   0.1498  0.1548   0.1151  0.0004\n\nsigma^2 = 9.742e-05:  log likelihood = 316.18\nAIC=-618.37   AICc=-617.12   BIC=-600.27\n\nTraining set error measures:\n                    ME    RMSE    MAE        MPE     MAPE      MASE      ACF1\nTraining set -1.358139 30.5077 19.739 -0.3803901 6.282248 0.7450115 0.1790798\n\n\nBy using the arguments approximation=FALSE and stepwise=FALSE, I ensure that R takes its time to fully calculate every reasonable model before choosing the best one. These arguments may be inconvenient or even impossible with large datasets or with long-period seasonality, where R may take several minutes to find the best model.\nR returns a reasonable idea: The changes in gas prices vs. the same quarter last year are weakly mean-reverting around an exponential drift, but with some persistence (when this quarter’s year-over-year change is negative or positive, the next one will likely be similar) and some error correction (unexpected anomalies rarely happen in consecutive years). But is it a reasonable model?\n\nplot(gas.sarima)\n\n\n\n\n\n\n\nplot(gas.sarima$residuals)\n\n\n\n\n\n\n\nsuppressWarnings(adf.test(gas.sarima$residuals))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  gas.sarima$residuals\nDickey-Fuller = -3.6612, Lag order = 4, p-value = 0.03112\nalternative hypothesis: stationary\n\nsuppressWarnings(kpss.test(gas.sarima$residuals))\n\n\n    KPSS Test for Level Stationarity\n\ndata:  gas.sarima$residuals\nKPSS Level = 0.19341, Truncation lag parameter = 4, p-value = 0.1\n\nBox.test(gas.sarima$residuals,type='Ljung')\n\n\n    Box-Ljung test\n\ndata:  gas.sarima$residuals\nX-squared = 0.041022, df = 1, p-value = 0.8395\n\npar(mfrow=c(1,2))\nacf(gas.sarima$residuals)\npacf(gas.sarima$residuals)\n\n\n\n\n\n\n\n\nWe see that none of the roots are perilously close to the edge of the unit circle, which is a good sign (R will always try to estimate a stationary ARIMA model, so when the roots are very close to the unit circle we have some evidence that the true process is actually nonstationary). We also see that the estimated innovations are relatively “clean” (stationary, no signs of remaining autocorrelation).\n\nplot(forecast(gas.sarima,h=6),include=12)\npoints(gas.test)\n\n\n\n\n\n\n\n\nOur predictions are very good with this method too! We should not really be surprised: as described by Hyndman and Athanasopoulos, purely additive ETS models can be represented as ARIMA models and vice versa. Although the exact form of our chosen ETS model and our SARIMA model are not equivalent, they are close.\nNote that these predictions come with confidence intervals, since ARIMA models suppose a Gaussian white noise error process. We can recover these intervals and/or select new ones using the forecast function or alternatively with the predict.Arima function.\n\nforecast(gas.sarima,h=6)\n\n        Point Forecast    Lo 80     Hi 80    Lo 95     Hi 95\n1985 Q3       240.4994 210.7762  276.6159 197.1367  298.9494\n1985 Q4       785.2351 630.3943 1000.9633 565.6710 1150.9506\n1986 Q1      1163.0804 898.4302 1555.4689 792.2109 1843.8957\n1986 Q2       561.4655 463.5813  691.9078 421.4312  779.1419\n1986 Q3       252.2123 214.0962  300.8558 197.1917  332.2017\n1986 Q4       851.1074 649.1042 1156.9429 569.1086 1386.0739\n\n\nWhich model we use, ETS or SARIMA, is a judgment call at this point: neither model is terribly flawed nor much better or worse than the other. We might prioritize in-sample RMSE, out-of-sample RMSE, other measures such as MAE, domain expertise, parsimony, prior literature, etc. We should not use AIC or other likelihood-based techniques, as these are sensitive to how we represent Y, which can change with orders of differencing or model classes.\n\n#ETS metrics\nc(is.rmse=sqrt(mean((gas.train-ets.zzz$fitted)^2)),\n  oos.rmse=sqrt(mean((gas.test-forecast(ets.zzz,h=6)$mean)^2)))\n\n is.rmse oos.rmse \n30.33744 54.96811 \n\n#SARIMA metrics\nc(is.rmse=sqrt(mean((gas.train-gas.sarima$fitted)^2)),\n  oos.rmse=sqrt(mean((gas.test-forecast(gas.sarima,h=6)$mean)^2)))\n\n is.rmse oos.rmse \n30.50770 54.93136",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Seasonal models in R</span>"
    ]
  },
  {
    "objectID": "seasonalityinr.html#footnotes",
    "href": "seasonalityinr.html#footnotes",
    "title": "Seasonal models in R",
    "section": "",
    "text": "Gas here meaning natural gas, and not gasoline, which in the UK would be called petrol.↩︎",
    "crumbs": [
      "3. ETS and SARIMA models",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Seasonal models in R</span>"
    ]
  },
  {
    "objectID": "forecasts.html",
    "href": "forecasts.html",
    "title": "Forecast strategies",
    "section": "",
    "text": "Notation\nSo far we’ve learned about two classes of time series models: ARIMA (including variants like SARIMA), and exponential smoothing models. Both of these models describe a data generating process in which each new observation evolves from an iterative calculation dependent upon the immediate prior observation(s).\nForecasting both of these models involves “walking” or “unfolding” the process forward, one step at a time. The easiest way to understand our projection for \\(\\hat{y}_{t+2}\\) from the information gathered at time \\(t\\) is to first compute our projection for \\(\\hat{y}_{t+1} | y_t\\), and then continue to calculate \\(\\hat{y}_{t+2} | y_t, \\hat{y}_{t+1}\\).\nThis is a very intuitive idea, and for many datasets it produces effective results with relatively modest compute and memory requirements. In the right contexts, however, alternative strategies will outcompete these models. We will learn a little more about the different strategies available to us below.\nIn all of the discussion on this page, I will try to keep to the following notation, as similar as possible to what we have already been using, and as similar as possible to the discussion found in Ben Taieb, et al. (2012).",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecast strategies</span>"
    ]
  },
  {
    "objectID": "forecasts.html#notation",
    "href": "forecasts.html#notation",
    "title": "Forecast strategies",
    "section": "",
    "text": "Term\nMeaning\n\n\n\n\n\\(y_1, \\ldots, y_N\\)\nThe training data used to build all models\n\n\n\\(y_N\\)\nThe last observation in the training data\n\n\n\\(y_t\\)\nAny generic observation (not necesarily the last observation)\n\n\n\\(\\hat{y}_{N+t}\\)\nAny generic forecasted future observation\n\n\n\\(f\\)\nA conceptual model of the data generating process\n\n\n\\(w\\)\nA “fudge factor” including all noise, anomalies, and modeling error\n\n\n\\(\\hat{f}\\)\nA “one-ahead” forecasting function, prefit using all training data\n\n\n\\(\\hat{f_h}\\)\nAn “h-ahead” forecasting function, prefit using all training data\n\n\n\\(d\\)\nThe embedding dimension (how many past observations are used as forecasting inputs)\n\n\n\\(H\\)\nThe maximum number of periods for which a forecast is needed\n\n\n\\(\\boldsymbol{w}\\)\nA vectorized “fudge factor”, \\(\\boldsymbol{w} \\in \\mathbb{R}^H\\)\n\n\n\\(F\\)\nA multivariate model of the data generating process, \\(F:\\mathbb{R}^d \\rightarrow \\mathbb{R}^H\\)\n\n\n\\(\\hat{F}\\)\nA multivariate forecasting function, prefit using all training data\n\n\n\\(I\\)\nThe number of forecasting “blocks” in a DIRMO strategy\n\n\n\\(s\\)\nThe size of each forecasting “block” in a DIRMO strategy",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecast strategies</span>"
    ]
  },
  {
    "objectID": "forecasts.html#recursive-forecasting",
    "href": "forecasts.html#recursive-forecasting",
    "title": "Forecast strategies",
    "section": "Recursive forecasting",
    "text": "Recursive forecasting\n\nTheory\nThis strategy is the first we have learned, and applies to ARIMA models, ETS models, state space models, some KNN (k-nearest neighbor) models, and some RNN (recursive neural network) models, among others.\nAll of these models use a data generating process which can be expressed as follows:\n\\[y_t = f(y_{t-d}, \\ldots, y_{t-1}) + w\\]\nNote that the true data generating process \\(f\\) is not itself data-dependent. For example, if we are studying an ARMA(1,2) process, the true parameters \\(\\phi_1, \\theta_1, \\theta_2\\) are fixed in stone (yet usually unknown). The data may guide us close to them or wildly mislead us, but \\(\\phi_1, \\theta_1, \\theta_2\\) do not change when we observe our sample.\nHowever, even though the nature of the function itself is not data-dependent, the specific outputs of any function depend on the inputs, and if we were again considering an ARMA(1,2) process (where \\(d=2\\)) we could say that \\(y_t\\) is a function of \\(y_{t-1}\\) and \\(y_{t-2}\\) plus some white noise, misspecification, parameter drift, etc., which we can aggregate into the error term \\(w\\).\nForecasting from this model involves estimating the data generating process \\(f\\) from the data, resulting in the forecasting function \\(\\hat{f}_h\\) for any observation \\(h\\) steps into the future. The inputs to this function are all “real” observations, all “forecasts”, or a mix of the two, depending on the embedding dimension \\(d\\) and the forecast horizon \\(h\\):\n\\[\\hat{y}_{N+h} = \\left\\{ \\begin{array}{ll} \\hat{f}(y_{N-d+1}, \\ldots, y_N) & \\textrm{if} \\; h=1 \\\\ \\hat{f}(y_{N-d+h}, \\ldots, y_N,\\hat{y}_{N+1}, \\ldots, \\hat{y}_{N+h-1}) & \\textrm{if} \\; h=2,\\ldots,d \\\\ \\hat{f}(\\hat{y}_{N+h-d}, \\ldots, \\hat{y}_{N+h-1}) & \\textrm{if} \\; h \\gt d \\end{array} \\right\\}\\]\n\n\nExample: AR(2) model\nSuppose we model a time series sample (with 100 observations) using an AR(2) process. That is, we believe the embedding dimension \\(d=2\\), and that:\n\\[\\begin{aligned} y_t &= \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\omega_t \\\\ \\\\ &= f(y_{t-1},y_{t-2}) + w \\end{aligned}\\]\nUsing all 100 observations, we train our model and conclude that \\(\\phi_1 = 0.5\\) and \\(\\phi_2 = 0.4\\). This allows us to use a resursive forecast function to define forecasts for time indices 101, 102, and 103:\n\nCode\ny &lt;- c(1.8, 2.5, 2.1, 3.0, 2.0)\nyhat &lt;-    0.5*y[5]    + 0.4*y[4]\nyhat[2] &lt;- 0.5*yhat[1] + 0.4*y[5]\nyhat[3] &lt;- 0.5*yhat[2] + 0.4*yhat[1]\n\nplot(96:100,y,xlim=c(96,103),pch=19,type='b',xlab='Time index')\npoints(99:100,y[4:5],pch=0,cex=1.5,col='#ff0000',lwd=2)\npoints(101,yhat[1],pch=17,col='#0000ff')\nlegend(x='topright',legend=c('Observations','Forecasting inputs','Forecasts'),bty='n',pch=c(19,0,17),col=c('#000000','#ff0000','#0000ff'))\nplot(96:100,y,xlim=c(96,103),pch=19,type='b',xlab='Time index')\npoints(100:101,c(y[5],yhat[1]),pch=0,cex=1.5,col='#ff0000',lwd=2)\npoints(101:102,yhat[1:2],pch=17,col='#0000ff')\nlegend(x='topright',legend=c('Observations','Forecasting inputs','Forecasts'),bty='n',pch=c(19,0,17),col=c('#000000','#ff0000','#0000ff'))\nplot(96:100,y,xlim=c(96,103),pch=19,type='b',xlab='Time index')\npoints(101:102,yhat[1:2],pch=0,cex=1.5,col='#ff0000',lwd=2)\npoints(101:103,yhat[1:3],pch=17,col='#0000ff')\nlegend(x='topright',legend=c('Observations','Forecasting inputs','Forecasts'),bty='n',pch=c(19,0,17),col=c('#000000','#ff0000','#0000ff'))\n\n\n\n\n\n\n\nRecursive forecast for h=1\n\n\n\n\n\n\n\n\n\nRecursive forecast for h=2\n\n\n\n\n\n\n\n\n\nRecursive forecast for h=3\n\n\n\n\n\n\n\nProperties\nStrengths of recursive forecasting:\n\nIntuitive: For models like ARIMA and ETS, the forecasting method matches the data generation process. For all recursive forecasting, we predict by simulating an evolving world one step at a time, matching our own experience of reality.\nPreserves time-dependency: Autocorrelations and other dependencies between observations are preserved over a multi-period forecast.\nLightweight: Neither the model nor the forecasting function need to be re-fit, simply re-applied in a single loop.\nUser-proof: No hyperparameters mean that nothing that can be set “wrong”.\n\nWeaknesses of recursive forecasting:\n\nCompounding errors: Small errors in each period’s forecast usually aggregate and get fixed into future-period forecasting, dragging later forecasts far from their actual values.\nStrong assumptions: Recursive models imply that the data generating process is constant (or at least similarly parameterized) throughout the past and future. They are less effective in the face of mixture models, high volatility, structural breaks, etc.",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecast strategies</span>"
    ]
  },
  {
    "objectID": "forecasts.html#direct-forecasting",
    "href": "forecasts.html#direct-forecasting",
    "title": "Forecast strategies",
    "section": "Direct forecasting",
    "text": "Direct forecasting\n\nTheory\nRather than understanding future time period \\(N+2\\) as a function of the current time period \\(N\\) and the next time period \\(N+1\\), we could simply create a new forecasting function \\(\\hat{f_2}\\) custom-built for two-ahead prediction. This new forecasting function would not need the prediction \\(\\hat{y}_{N+1}\\) as an input.\nThis forecasting method matches well with neural networks, some KNN models, and many series displaying weak time-series behavior which might otherwise be modeled with time-invariant techniques such as regression.\nEven if the “true” data generating process evolves iteratively, direct forecasting strategies believe that each observation \\(y_t\\) can be usefully modeled by several different functions, each of which use a different set of prior values:\n\\[\\begin{aligned} y_t &= f_1(y_{t-d}, \\ldots, y_{t-1}) + w \\\\ y_t &= f_2(y_{t-d-1}, \\ldots, y_{t-2}) + w \\\\ \\vdots & \\\\ y_t &= f_H(y_{t-d-H+1}, \\ldots, y_{t-H}) + w \\end{aligned}\\]\nThis structure allows us to estimate a set of forecasting functions, which use only observed values \\(y_t\\) as inputs:\n\\[\\hat{y}_{N+h} = \\hat{f_h}(y_{N-d+1}, \\ldots, y_N)\\]\n\n\nExample: Non-exponential smoothing\nSuppose an economist is making predictions about future inflation rates. They know that inflation has averaged 2% per annum over the last several decades, and they also know that, in the short-term, the inflation rate this quarter is a good predictor of the inflation next quarter. They devise the following forecast models:\n\\[\\begin{aligned} \\hat{y}_{N+1} = {f_1}(y_N) = 0.8y_N + 0.2(0.02) \\\\ \\hat{y}_{N+2} = {f_2}(y_N) = 0.6y_N + 0.4(0.02) \\\\ \\hat{y}_{N+3} = {f_3}(y_N) = 0.4y_N + 0.6(0.02) \\\\ \\hat{y}_{N+4} = {f_4}(y_N) = 0.2y_N + 0.8(0.02) \\end{aligned}\\]\n(Implicitly, we might add that \\(\\hat{y}_{N+h} = 0.02\\) for any \\(h \\gt 4\\)). This model shows a mean reversion from the current value which could also be modeled with an AR(1) process; however, the AR decay is exponential and our economist may have empirical evidence to suggest that inflation rates revert to the mean at a steadier pace.\n\n\nCode\ny &lt;- c(0.025, 0.021, 0.02, 0.03)\nyhat &lt;- c(0.8,0.6,0.4,0.2)*y[4] + c(0.2,0.4,0.6,0.8)*0.02\n\nplot(1:4,y,xlim=c(1,8),pch=19,type='b',xlab='Quarter',ylab='Inflation rate',xaxt='n')\naxis(side=1,at=1:8,labels=c(paste0('2025Q',1:4),paste0('2026Q',1:4)))\npoints(4,y[4],pch=0,cex=1.5,col='#ff0000',lwd=2)\npoints(5:8,yhat,pch=17,col='#0000ff')\nlegend(x='topleft',legend=c('Observations','Forecasting inputs','Forecasts'),bty='n',pch=c(19,0,17),col=c('#000000','#ff0000','#0000ff'))\n\n\n\n\n\nDirect forecasts for h=1,2,3,4\n\n\n\n\n\n\nProperties\nStrengths of direct forecasting:\n\nRobust: By fitting separate models for each time horizon, we stop large errors in one forecast from contaminating future-period forecasts.\nOnly uses observed data: Conceptually, these models do not need any “guesses” about an intermediate future state in order to make their forecasts.\nResource needs independent of forecast horizon: Generally, a forecast for time \\(N+100\\) requires the same amount of resources as the forecast for time \\(N+1\\), unlike recursive models which must simulate 99 additional steps.\n\nWeaknesses of direct forecasting:\n\nErases interdependence between forecast values: The forecasts are all conditionally independent of each other, and so direct forecasts for adjacent time periods might suggest improbable or nonsensical single-period changes, or ignore useful information from intermediate forecast periods.\nMore resource-intensive: Separate models must be trained and stored for each forecast horizon.",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecast strategies</span>"
    ]
  },
  {
    "objectID": "forecasts.html#multiple-input-multiple-output-mimo-forecasting",
    "href": "forecasts.html#multiple-input-multiple-output-mimo-forecasting",
    "title": "Forecast strategies",
    "section": "Multiple-input multiple-output (MIMO) forecasting",
    "text": "Multiple-input multiple-output (MIMO) forecasting\n\nTheory\nBoth of the strategies described above forecast a single future period; that is, each forecasting model \\(\\hat{f}\\) or \\(\\hat{f_h}\\) is a real-valued function of one or more inputs which produces one output, our forecast \\(\\hat{y}_{N+h}\\).\nOne tension between these two strategies is that recursive forecasts understands the interdependency between different forecast points, but is prone to accumulate and amplify forecasting error. Contrariwise, direct forecasting avoids the compounding errors by destroying the links between forecasted values.\nMultiple-input multiple-output (MIMO) forecasting attempts to resolve this tension by forecasting all future periods within the time horizon \\(H\\) at once. No matter how the current values of the series evolved, any subset of \\(H\\) observations can be usefully modeled en masse by the \\(d\\) prior values:\n\\[(y_{t-H+1}, \\ldots, y_t) = F(y_{t-d-H+1}, \\ldots, y_{t-H}) + \\boldsymbol{w}\\]\nThis structure allows us to estimate a multivariate forecasting function which will allow us to predict the entire forecasting horizon at once:\n\\[(\\hat{y}_{N+1}, \\ldots, \\hat{y}_{N+h}) = \\hat{F}(y_{N-d+1}, \\ldots, y_N)\\]\nThe outputs of these models can display correlations and other dependencies (like recursive forecasting), while imposing set-level constraints which discourage one unusual forecast from unduly influencing the other forecasts.\nThese forecasting strategies appear commonly in transformer-based architectures, which seek to predict the next several tokens/events at the same time, using a shared representation space for all of the tokens/events. MIMO strategies also do very well in probabilistic forecasting, which does not seek to predict the future mean of a series, but instead allows for probabilistic statements about how likely certain combinations of future events will be.\n\n\nExample: Transit state-space modeling\nSuppose a bus route through town is served by five buses. The bus route is a circuit which takes roughly 50 minutes to complete, and so the average wait time for a bus at any given stop is roughly 10 minutes.\nWe could try to predict the arrival time for the next few buses at any given stop from the arrival times of the prior buses at that same stop. Our knowledge of the real world constraints and goals of the bus drivers can help us model complex behavior:\n\nIf the last arrival time \\(y_t\\) was five minutes ago, the next one would be expected in five minutes and the one after that in fifteen minutes (ten minutes more).\nIf the last arrival time \\(y_t\\) was fifteen minutes ago, the next one is probably due very soon, and the one after that is also due very soon (bunching.)\nFive buses should make a stop within roughly 50 minutes of the most recent bus stop.\n\nThis behavior can be well-approximated using a state space model, which we will learn about later. Given this setup, we would do well to forecast the next five bus stops from the times of the previous five bus stops:\n\\[(\\hat{y}_{N+1}, \\ldots, \\hat{y}_{N+5}) = \\hat{F}(\\hat{y}_{N-4}, \\ldots, \\hat{y}_{N})\\]\n\nCode\ny &lt;- c(-60,-55,-48,-33,-30,-19,-5)\nyhat &lt;- c(5,15,25,35,45)\n\nplot(y,rep(0,7),xlim=c(-75,50),pch=19,type='p',xlab='Arrival time (min)',ylab=NA,yaxt='n',xaxt='n',ylim=c(-1,3))\naxis(side=1,at=seq(-75,50,25),labels=TRUE,pos=-0.25)\npoints(y[3:7],rep(0,5),pch=0,cex=1.5,col='#ff0000',lwd=2)\npoints(yhat,rep(0,5),pch=17,col='#0000ff')\nlegend(x='topleft',legend=c('Observations','Forecasting inputs','Forecasts'),bty='n',pch=c(19,0,17),col=c('#000000','#ff0000','#0000ff'))\ny &lt;- c(-70,-55,-43,-38,-35,-26,-22)\nyhat &lt;- c(1,4,10,27,47)\n\nplot(y,rep(0,7),xlim=c(-75,50),pch=19,type='p',xlab='Arrival time (min)',ylab=NA,yaxt='n',xaxt='n',ylim=c(-1,3))\naxis(side=1,at=seq(-75,50,25),labels=TRUE,pos=-0.25)\npoints(y[3:7],rep(0,5),pch=0,cex=1.5,col='#ff0000',lwd=2)\npoints(yhat,rep(0,5),pch=17,col='#0000ff')\nlegend(x='topleft',legend=c('Observations','Forecasting inputs','Forecasts'),bty='n',pch=c(19,0,17),col=c('#000000','#ff0000','#0000ff'))\n\n\n\n\n\n\n\nMIMO forecasts for one set of bus arrivals\n\n\n\n\n\n\n\n\n\nMIMO forecasts for a second set of bus arrivals\n\n\n\n\n\n\n\nProperties\nStrengths of MIMO forecasting:\n\nSafely handles complexity: Can learn and replicate intricate dependencies between the forecasts without compounding errors.\nOnly uses observed data: Conceptually, these models do not need any “guesses” about an intermediate future state in order to make their forecasts.\n\nWeaknesses of MIMO forecasting:\n\nImposes one model across entire forecast period: For large values of \\(H\\) (the maximum forecast period), MIMO methods force a single function (or state space, or “worldview”) to describe the whole forecast range. Long-term forecasts are allowed to differ from near-term forecasts, but they must both arise from the same model.\nHeavily resource-intensive: Multivariate outputs usually require more estimation and more compute/memory resources.\nBurns through training data quickly: Relatedly, these models do not converge quickly and often require large amounts of data for effective accuracy, especially for large values of \\(d\\) and/or \\(H\\).",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecast strategies</span>"
    ]
  },
  {
    "objectID": "forecasts.html#hybrid-strategies",
    "href": "forecasts.html#hybrid-strategies",
    "title": "Forecast strategies",
    "section": "Hybrid strategies",
    "text": "Hybrid strategies\nForecasters have explored techniques which combine these three strategies, with the goal of preserving the strengths of each stratgy while mitigating the weaknesses.\n\nDirRec\nOne of these hybrids is called “DirRec” (Direct + Recursive), which outperforms both direct and recursive strategies on some real-world datasets — which should immediately raise suspicions of selection bias, confirmation bias, and publication bias.\nIn DirRec forecasting, each future period \\(h\\) is modeled with a separate function (similar to direct forecasting), but the inputs to later forecast periods include the estimated forecasts from earlier periods (similar to recursive forecasting). In other words, observations from the series can be represented as outputs from several possible generating functions, each incorporating more and more lags:\n\\[\\begin{aligned} y_t &= f_1(y_{t-d}, \\ldots, y_{t-1}) + w \\\\ y_t &= f_2(y_{t-d-1}, \\ldots, y_{t-1}) + w \\\\ \\vdots & \\\\ y_t &= f_H(y_{t-d-H+1}, \\ldots, y_{t-1}) + w \\end{aligned}\\]\nWhich in turn suggest forecasting functions which need to be defined and estimated iteratively:\n\\[\\hat{y}_{N+h} = \\left\\{ \\begin{array}{ll} \\hat{f_1}(y_{N-d+1}, \\ldots, y_N) & \\textrm{if} \\; h=1 \\\\ \\hat{f_h}(y_{N-d+1}, \\ldots, y_N,\\hat{y}_{N+1}, \\ldots, \\hat{y}_{N+h-1}) & \\textrm{if} \\; 2 \\le h \\le H \\end{array} \\right\\}\\]\nStrengths of DirRec forecasting:\n\nImplicit dependency between forecasts: Although each forecast period is modeled with a separate function, which removes explicit recursive dependencies, the presence of early forecasts among the inputs to later forecasts allows the forecasting model to re-learn those dependencies.\nSensible constraints on forecast outputs: The use of forecasted values as inputs can propagate errors, but since each period’s forecast function is trained separately, extreme forecasts arising from compounded errors are usually penalized and avoided.\n\nWeaknesses of DirRec forecasting:\n\nHeavily resource-intensive: To make a set of \\(1 - H\\)-step ahead forecasts, we must train \\(H\\) different models, and the later models require earlier model outputs as their inputs, complicating and lengthening the training process.\nLoss-minimizing, not truth-seeking: Rather than seeking to parametrically understand and describe the linkages between forecast periods, DirRec locally approximates each transition.\n\n\n\nDIRMO\nA second hybrid is called “DIRMO” (Direct + Multiple Output), which establishes a spectrum of models bounded by the direct and MIMO approaches described above.\nIn MIMO forecasting we modeled the entire horizon of \\(h \\in \\{1, \\ldots, H\\}\\) with a single multivariate forecasting function \\(\\hat{F}\\). In DIRMO forecasting, we instead split the horizon into \\(I\\) blocks, each of size \\(s: 1 \\le s \\le H\\). The future observations in each of these blocks will be forecasted with a separate forecasting function:\n\\[\\begin{aligned} (\\hat{y}_{N+1}, \\ldots, \\hat{y}_{N+s}) &= \\hat{F_1}(y_{N-d+1}, \\ldots, y_N) \\\\ &\\vdots \\\\ (\\hat{y}_{N+s(i-1)+1}, \\ldots, \\hat{y}_{N+si}) &= \\hat{F_i}(y_{N-d+1}, \\ldots, y_N) \\\\ &\\vdots \\\\ (\\hat{y}_{N+H-s+1}, \\ldots, \\hat{y}_{N+H}) &= \\hat{F_I}(y_{N-d+1}, \\ldots, y_N) \\end{aligned}\\]\nWhere for each \\(i \\in \\{1, \\ldots, I\\}\\), the forecast function \\(\\hat{F_i}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^s\\).\nNote two important edge cases. When \\(s=H\\), then we have only a single forecasting function, and return to MIMO forecasting. When \\(s=1\\), then each forecasting function returns only a single output, and we have returned to direct forecasting.\nStrengths of DIRMO forecasting:\n\nLoses nothing: The framework above could collapse to either MIMO or direct forcasting if and when those techniques fit the data best, or it could suggest a new approach at an optimal balance point between the two techniques.\nAllows the far future to differ mechanically from the near future: By breaking the MIMO horizon into blocks, we allow the rules which govern far-future blocks to be different than those which estimate the near-future blocks.\n\nWeaknesses of DIRMO forecasting:\n\nHeavily resource-intensive: We now need to estimate multiple multivariate functions, and in fact will need to repeat this many times if we are optimizing the choice of block size \\(s\\).\nParameter-dependent The block size \\(s\\) will affect our explanatory power and model metrics, and different datasets will benefit from different sizes of \\(s\\) in ways which can’t easily be predicted and will be subject to overfitting when estimated from our dataset.",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecast strategies</span>"
    ]
  },
  {
    "objectID": "validation.html",
    "href": "validation.html",
    "title": "Model validation",
    "section": "",
    "text": "Backtesting your model\nModel validation refers to how we build confidence that we have constructed a model worth using to solve real-world problems. Importantly, model validation is not the same as model performance or model accuracy. A low-performance model may be the best we can reasonably fit to the data. A high-performance model may not be sufficiently better than a simple “null” model to justify its use.\nTime series models can be used simply to understand the past, much like cross-sectional techniques such as linear regression. However, when time series models are used specifically to forecast the future, we need to be very wary of in-sample performance on the training data. After all, the whole point of forecasting is out-of-sample prediction, so we should try to understand the model’s out-of-sample performance as best as we can.",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model validation</span>"
    ]
  },
  {
    "objectID": "validation.html#backtesting-your-model",
    "href": "validation.html#backtesting-your-model",
    "title": "Model validation",
    "section": "",
    "text": "As compared to ordinary cross-validation\nNon-time series models frequently use cross-validation to build confidence in the models. (In cross-validation, the training data is split into \\(k\\) blocks of roughly equal size, and each block is used as a test dataset for a model trained on the remaining \\(k-1\\) blocks.)\nWe can’t use this technique in time series analysis because it cannot preserve the time indexing of the data, which in turn affects our model predictions. Having a block of observations missing from the middle of our data is often enough to “break” our model, and in any case to predict for these points would involve using future information to predict the past, which would not validate our model usefulness.\n\n\nTime series cross-validation\nOne best practice which implements cross-validation concepts for time series modeling is called, plainly enough, time series cross-validation. (Sometimes this technique is also called “backtesting”, though backtesting could also refer to less formal methods or simply a single prior validation period.)\nThe basic idea of time series cross-validation is to forecast each observation in our training data (as far back as practical while retaining enough of a prior training sample), and after each forecast is complete we then incorporate that observation into our training set for the new data:\n\n\nCode\nplot(x=c(1,20),y=c(0.5,6.5),type='n',yaxt='n',ylab='CV Iteration',xlab='Time index')\naxis(side=2,at=1:6,labels=6:1)\narrows(x0=0,y0=6:1,x1=20,length=0.1)\nsymbols(x=unlist(sapply(10:15,function(z) 1:z)),y=rep(6:1,times=(10:15)),circles=rep(0.3,75),fg='#000000',bg='#000000',inches=FALSE,add=TRUE)\nsymbols(x=11:16,y=6:1,circles=rep(0.3,6),fg='#0000ff',bg='#0000ff',inches=FALSE,add=TRUE)\nlegend(x='top',pch=19,col=c('#000000','#0000ff'),legend=c('Training data','Test data'),horiz=TRUE,xjust=0.5,bty='n',inset=-0.15,xpd=TRUE)\n\n\n\n\n\nBasic time series cross-validation structure\n\n\n\n\nOnce we have forecasted as much of the training data as possible through time series cross-validation, we can compute out-of-sample performance metrics such as MAE or RMSE by aggregating across the test forecasts.\n\n\nVariations on basic time series cross-validation\nAs a forecaster (and more generally as a data scientist), you must do more than simply memorize how to drop a pre-written routine into your workflow: you will need to know how to choose and adapt a technique to the problem that you’re solving. As an example, consider three variations on the basic timeseries cross-validation method described above.\n\nFixed-width training windows\nSome models avoid parameter drift by de-weighting old observations (e.g. ETS, neural networks), while other models avoid parameter drift by avoiding parameters altogether (e.g. KNN). When using these models, the dangers of inlcuing stale training data are largely minimized.\nOther models, like ARIMA models, VARs, or dynamic regressions, equal-weight the entire training range and are sensitive to parameter drift or structural breaks. When running these models over a large amount of data, you might want to limit the size of the training sets used for time series cross-validation, to more accurately reflect the limited training datasets you intend to provide your model “in production”:\n\n\nCode\nplot(x=c(1,20),y=c(0.5,6.5),type='n',yaxt='n',ylab='CV Iteration',xlab='Time index')\naxis(side=2,at=1:6,labels=6:1)\narrows(x0=0,y0=6:1,x1=20,length=0.1)\nsymbols(x=unlist(sapply(10:15,function(z) (z-9):z)),y=rep(6:1,each=10),circles=rep(0.3,60),fg='#000000',bg='#000000',inches=FALSE,add=TRUE)\nsymbols(x=unlist(sapply(1:5,function(z) 1:z)),y=rep(5:1,times=1:5),circles=rep(0.3,15),fg='#000000',bg='#ffffff',inches=FALSE,add=TRUE)\nsymbols(x=11:16,y=6:1,circles=rep(0.3,6),fg='#0000ff',bg='#0000ff',inches=FALSE,add=TRUE)\nlegend(x='top',pch=c(1,19,19),col=c('#000000','#000000','#0000ff'),legend=c('Unused','Training data','Test data'),horiz=TRUE,xjust=0.5,bty='n',inset=-0.15,xpd=TRUE)\n\n\n\n\n\nTime series cross-validation with fixed-width training windows\n\n\n\n\nAs an example, econometricians and legal experts sometimes analyze daily securities prices (stocks, hedges, commodities, futures, options, etc.) using time series models. A greater amount of training data (prior daily prices) helps to better estimate the model, but too much data runs the risk of parameter drift, structural break, or contaminating the sample with otherwise outdated information. In most literature, one year of daily price returns is seen as a good compromise, with six months or two years being common alternatives. The basic cross-validation procedure might involve fitting models with 10+ years of training data, which is not how these models would ever be used in practice.\n\n\nCross-validating h-ahead forecasts\nIn some instances, the one-ahead forecast is not an organizational priority, but forecasting on a longer time horizon is more valuable. A realtor or a bond market trader may want to gain insight into the expected interest climate next year, not just next quarter. In these cases, we should match our validation to the forecast window(s) which will be used for decision-making, even if the metrics for the one-ahead forecast period are stronger (which will almost always be true):\n\n\nCode\nplot(x=c(1,20),y=c(0.5,6.5),type='n',yaxt='n',ylab='CV Iteration',xlab='Time index')\naxis(side=2,at=1:6,labels=6:1)\narrows(x0=0,y0=6:1,x1=20,length=0.1)\nsymbols(x=unlist(sapply(10:15,function(z) 1:z)),y=rep(6:1,times=(10:15)),circles=rep(0.3,75),fg='#000000',bg='#000000',inches=FALSE,add=TRUE)\nsymbols(x=unlist(sapply(11:16,function(z) z:(z+3))),y=rep(6:1,each=4),circles=rep(0.3,24),fg='#000000',bg='#ffffff',inches=FALSE,add=TRUE)\nsymbols(x=15:20,y=6:1,circles=rep(0.3,6),fg='#0000ff',bg='#0000ff',inches=FALSE,add=TRUE)\nlegend(x='top',pch=c(19,1,19),col=c('#000000','#000000','#0000ff'),legend=c('Training data','Unused','Test data'),horiz=TRUE,xjust=0.5,bty='n',inset=-0.15,xpd=TRUE)\n\n\n\n\n\nTime series cross-validation of 5-ahead forecasts\n\n\n\n\nDepending on the problem and the organizational interest in solving said problem, multiple forecasting horizons might be important to understand. The same forecasting methods for interest rates could be validated at a 1-month, 12-month, and 60-month horizon. Different models could be chosen for each horizon, or a model could be chosen which performs as well as possible across all three horizons simultaneously, depending on the user’s needs and preferences.\n(Note that a direct forecasting approach naturally dovetails with the idea of validating each time horizon separately, while a recursive forecasting approach naturally suggests itself when choosing one model to be “good enough” at all horizons of interest.)\n\n\nCross-validating MIMO forecasts\nTime series cross-validation extends simply to MIMO forecasts, though even here we may find nuance and user judgment calls. For example, say I were a meteorologist tasked with five-day forecasts (of daily weather, or perhaps the track of a hurricane). I could train a five-day MIMO model, and I could cross-validate it by rolling the training window through my data one day at a time:\n\n\nCode\nplot(x=c(1,20),y=c(0.5,6.5),type='n',yaxt='n',ylab='CV Iteration',xlab='Time index')\naxis(side=2,at=1:6,labels=6:1)\narrows(x0=0,y0=6:1,x1=20,length=0.1)\nsymbols(x=unlist(sapply(10:15,function(z) 1:z)),y=rep(6:1,times=(10:15)),circles=rep(0.3,75),fg='#000000',bg='#000000',inches=FALSE,add=TRUE)\nsymbols(x=unlist(sapply(15:20,function(z) (z-4):z)),y=rep(6:1,each=5),circles=rep(0.3,30),fg='#0000ff',bg='#0000ff',inches=FALSE,add=TRUE)\nlegend(x='top',pch=19,col=c('#000000','#0000ff'),legend=c('Training data','Test data'),horiz=TRUE,xjust=0.5,bty='n',inset=-0.15,xpd=TRUE)\n\n\n\n\n\nTime series cross-validation of 5-day MIMO forecasts\n\n\n\n\nEven though the test regions overlap, this setup would provide perfectly acceptable validation of my model! In fact, by allowing the test windows to overlap, we preserve crucial details about the dependence structure. If we tested only the observations \\(y_{11}, \\ldots, y_{15}\\) as one iteration and then the observations \\(y_{16}, \\ldots, y_{20}\\) as the next iteration, we would never ask our model to incorporate both \\(y_{15}\\) and \\(y_{16}\\) in the same test period. That particular transition might be extreme or unexpected; failing to test it could obscure weaknesses in our model.\nHowever!! What if, instead, I worked as a grocery store manager. Each Wednesday I place my orders with suppliers to restock my store Friday morning ahead of the weekend rush. If I have to resupply midweek I can, but I need to know ahead of time, and if I order too much food it may end up as costly spoilage.\nIn this situation, it would be more appropriate to slide the training window one week at a time, predicting each future week’s sales Friday to Thursday. We have no current use for a model which predicts grocery sales Tuesday to Monday. (Though we did have a use for a hurricane forecasting tool which could be used every day.)\n\n\nCode\nplot(x=c(1,34),y=c(0.5,4.5),type='n',yaxt='n',xaxt='n',ylab='CV Iteration',xlab='Time index')\naxis(side=1,at=seq(5,33,7),labels=rep('Fri',5))\naxis(side=2,at=1:4,labels=4:1)\narrows(x0=0,y0=4:1,x1=34,length=0.1)\nsymbols(x=c(1:3,1:10,1:17,1:24),y=rep(4:1,times=c(3,10,17,24)),circles=rep(0.3,54),fg='#000000',bg='#000000',inches=FALSE,add=TRUE)\nsymbols(x=c(5:11,12:18,19:25,26:32),y=rep(4:1,each=7),circles=rep(0.3,28),fg='#0000ff',bg='#0000ff',inches=FALSE,add=TRUE)\nlegend(x='top',pch=19,col=c('#000000','#0000ff'),legend=c('Training data','Test data'),horiz=TRUE,xjust=0.5,bty='n',inset=-0.15,xpd=TRUE)\n\n\n\n\n\nTime series cross-validation of 7-day MIMO forecasts",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model validation</span>"
    ]
  },
  {
    "objectID": "validation.html#choosing-a-model",
    "href": "validation.html#choosing-a-model",
    "title": "Model validation",
    "section": "Choosing a model",
    "text": "Choosing a model\nValidation is one part of a multistep process which ends with our adoption of a forecasting model. Let’s examine the other parts to see how validation contributes to the final decision.\n\n1. Identify the status quo\nIn some organizational settings, a forecasting model is already in place, and you are asked (or you are offering) to improve upon it.\n\nIf the existing codebase and data is available to you, start by using the exact model, on the real data.\nIf you simply have a description of the status quo, replicate the model and the data as best as you can. Even small details can create large changes in forecast accuracy, so treat this step very carefully.\n\nIn other settings, your model will be the first used to address the business problem or research question.\n\nDetermine the best “null model” and its associated forecast method which would make sense for this application. Depending on the problem, a null model might be a random walk (where the forecast is the last observed value), white noise (where the forecast is the mean of the historical data), linear regression (no true interdependency, just extrapolation using a time-based covariate), etc.\n\nUse the status quo or the null model to prove the worth of your work. If your modeling efforts do not meaningfully surpass the status quo, then you should stop the analysis. Do not see this outcome as a personal failure or an unsuccessful project. Instead, you will be saving your organization the cost (in money and time and risk) of changing forecast methods, and delivering new knowledge to decisionmakers about the ability or inability to meaningfully predict the future of this data series.\n\n\n2. After EDA, pick a very limited number of contenders\nStudents and junior professionals often mistakenly select many different models and run them against each other to “prove” they have found the right approach. Usually, they achieve the opposite result:\n\nBy selecting models which are not well-fit to the problem, they suggest either domain inexperience or data science inexperience,\nBy selecting many different models, they rarely give proper attention to each, in terms of the feature engineering or hyperparameter tuning they would realistically require.\nBy selecting many models, they make it more difficult to communicate their work to others and have it be easily understood and trusted.\nBy selecting many models, they increase the chance of overfitting the problem and seeing much worse performance in production than during testing.\nBy selecting many models, they increase their own workload and place needless strain on their deadlines.\n\nInstead, aim to exit this step with one or two classes of model, perhaps with one or two variants each. Choose models that fit the problem: don’t bring an ARIMA model to a GARCH problem, or choose a neural network model (RNNs, LSTMs) with inputs which fail to capture the long-period seasonality in your data.\nIn other words, confirm the functional form of your model candidates through EDA. Do not use extensive model validation simply to show that an AR representation fits the data better than an MA representation: you can and should do this without time series cross-validation. Only bring true contenders to the validation phase.\n\n\n3. Pick a forecasting method and one or two metrics to help you evaluate the models both in absolute and relative terms.\nAt this point you will be choosing between the status quo model (the “champion”) and one or two new models which have been fully built out and polished. If those models involve tuning parameters or other hyperparameters, then ideally these choices will have already been set using additional training data different than the validation training set.1\n\nWhich forecast method will you use? Generally, there will only be one right choice, fitted to your model type(s) and business problem, though occasionally you may simultaneously test the models over multiple time horizons.\nWhich metric will be computed from the time series cross-validation?\n\nRoot mean squared error (RMSE): The classic. Heavily penalizes large errors. Often described as the size of an “average” error although this is not mathematically true. Some models minimize the in-sample RMSE by construction, so it’s a fitting comparison to see how they perform out-of-sample.\nMean absolute error (MAE): Actually the size of the “average” error. Does not disproportionally penalize large errors. Most useful when errors are naturally and tightly bound (e.g. when predicting proportions), or when the large errors are not particularly costly (e.g. when predicting future daily wind speed).\nMean absolute percentage error (MAPE): A metric which solves some problems but introduces others. Unitless, and therefore good for measuring predictions which vary wildly in their mean as well as comparing models trained on different datasets or different periods. However, the metric does not handle values near zero very well and it assigns an asymmetrically larger penalty to negative errors as compared to positive errors.\nMean absolute scaled error (MASE): Introduced by Hyndman and Koehler, solves many problems with MAPE. Benchmarks the challenger model’s forecast errors using the errors from a naive “null model”.\nCustom cost function or value function: Often overlooked by data scientists. When you can estimate this function, it will generally be more useful than any of the above model metrics. Examples would be the costs of spoiled overage or empty shelves at a grocery store (as opposed to the forecast error in estimated demand), or the market value of an RMBS security (as opposed to the forecast error in the default rate on the underlying mortgages).\n\n\nYour goal when choosing these metrics is to simultaneously learn (and communicate) whether any given model is (a) better than other models, and (b) acceptably good enough to be put into production. Both relative and aboslute performance are important.\nIt helps to have a goal post set before you begin validation, perhaps even before you begin modeling. What is a useful level of forecasting performance? Where is the benchmark for “worth organizational resources”? Check in with your stakeholders. Giving them the best model is not helpful when even the best model simply isn’t very good.\n\n\n4. Be prepared to defend your model on its other strengths and weaknesses beyond the validation metrics\nThe world does not stop what it’s doing and bow down to you for achieving a remarkable forecast accuracy, sad to say. There may be skeptics among your audience or stakeholders. In fact, there should be skeptics, or at least those who will only be persuaded by a body of evidence, or else you run the risk of pushing bad models into production which have not survived legitimate scrutiny. Invite the toughest critic you know (and who is allowed to see your model) to kick the tires on your work.\nYou will likely be asked about some or all of the following model properties, so you should do whatever work you can to prepare for these types of questions:\n\nHow does this model fit in with the previously accepted or previously used theory? If it differs, why does it differ, and is this reasonable? If it uses the same model class as the status quo, why did the small differences in specification create large differences in model performance?\nHow does this choice of model characterize the world? What does it say about the underlying number generating process, and is that a statement which your organization would feel comfortable supporting? For example, are you saying that prices are not random walks? Are you saying that crime is best predicted through racial demographics?\nDo the residuals or estimated innovations suggest that you have underfit the model? If any structure remains — non-stationarity, autocorrelation structures, volatility clustering, etc. — then likely you have missed a modeling opportunity which others might observe and point out to your audience.\nHow would you persuade someone that you have not overfit the model, through cherrypicking, “throwing spaghetti at the wall and seeing what sticks” (using many model classes and variants), information leakage, or other sources?\nHow resource-intensive is your model, in terms of deployment time, runtime, compute and storage requirements, employee skill sets needed for maintenance and model refreshes, data engineering tasks, etc.?\nDo you like your model? Why or why not? Do you trust your model? Why or why not?",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model validation</span>"
    ]
  },
  {
    "objectID": "validation.html#footnotes",
    "href": "validation.html#footnotes",
    "title": "Model validation",
    "section": "",
    "text": "If need be, the rolling training window can include a short tuning sequence before each true test forecast(s). After using the tuning sequence to set the hyperparameters, the model is fixed in place: the tuning sequence enters the embedding horizon for the forecast without altering the training model.↩︎",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model validation</span>"
    ]
  },
  {
    "objectID": "validationinr.html",
    "href": "validationinr.html",
    "title": "Validation in R",
    "section": "",
    "text": "Time series functions used in this document\nI’m a firm believer that we should not become too dependent upon our tools. Fifteen years from now, we may still be forecasting and validating, but in a new language other than R or Python — if we don’t know how these methods work, we will have trouble relearning the techniques in the languages of the future.\nOr, looking toward the past, I can’t count the number of times I have had to hard-code a data science model into an Excel spreadsheet to help an organization or stakeholder who simply cannot use a modern ML pipeline. If you can’t represent these techniques in Excel, you may not know them very well.\nTherefore, the R code below does not use the automated validation routines suggested by the forecast or fable packages, even though I think they are very helpful and worth learning.\nIn the table below, packages with italicized names will need to be installed, while the package names in a standard font face can be found in most base R distributions (though they may need to be loaded into your workspace).",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Validation in R</span>"
    ]
  },
  {
    "objectID": "validationinr.html#time-series-functions-used-in-this-document",
    "href": "validationinr.html#time-series-functions-used-in-this-document",
    "title": "Validation in R",
    "section": "",
    "text": "Time series functions used in the code below\n\n\nPackage\nFunction name\nPurpose\n\n\n\n\nstats\ntime\nExtract the time indices of a ts\n\n\nstats\nstart\nThe first time index of a ts\n\n\nstats\nstop\nThe last time index of a ts\n\n\nstats\nwindow\nSubset a time series\n\n\nforecast\nauto.arima\nAutomatically select a well-fit ARIMA model\n\n\nforecast\nplot.Arima\nPlot the inverse AR and MA roots\n\n\nforecast\nets\nFit exponential smoothing models\n\n\nforecast\nfitted\nIn-sample predictions of a ts model\n\n\nstats\nembed\nCreate a matrix of lags for a ts\n\n\nnnet\nnnet\nSimple neural network modeling\n\n\nnnet\npredict.nnet\nDirect or MIMO nnet forecasting\n\n\nforecast\nforecast\nLook-ahead prediction of a ts model",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Validation in R</span>"
    ]
  },
  {
    "objectID": "validationinr.html#use-case-and-objective-4-month-ahead-prediction-of-treasury-spreads",
    "href": "validationinr.html#use-case-and-objective-4-month-ahead-prediction-of-treasury-spreads",
    "title": "Validation in R",
    "section": "Use case and objective: 4-month-ahead prediction of Treasury spreads",
    "text": "Use case and objective: 4-month-ahead prediction of Treasury spreads\nThe United States Department of the Treasury issues a number of government debt investments: U.S. Treasury bills, Treasury notes, and Treasury bonds (collectively, “treasuries”), each of various maturities, which trade daily in secondary markets. For any treasury security, the yield (an effective return-on-investment) can be computed from the market price, the maturity (payback date), and the coupon rate (annualized interest), and these “yields” are generally how the performance of treasuries are measured over time.\nThe yields of short-term treasuries and long-term treasuries reflect market expectations of near-term and longer-term economic uncertainty, and so the difference in yields between a short-term treasury and a long-term treasury (the “spread”) can serve as a useful proxy for certain types of market uncertainty.\nWe will pull in a monthly history of 1-year and 10-year treasury yields, and then use their difference to establish a yield spread. (Students wanting to make a different set of judgmet calls might consider creating a daily version of this analysis.)\nOur goal for this analysis is to predict the monthly treasury spread four months in advance. For example, if we have December’s data, we wish to predict what the spread will be in April.\n\n# make sure the two time series are ready to combine\ndata(tcm)\nall(time(tcm1y)==time(tcm10y))\n\n[1] TRUE\n\nc(sum(is.na(tcm1y)),sum(is.na(tcm10y)))\n\n[1] 0 0\n\n# define new spread series as their difference\nspread &lt;- tcm10y - tcm1y\nsummary(spread)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -3.070   0.120   0.730   0.716   1.397   3.290 \n\nc(start(spread),end(spread))\n\n[1] 1953    4 1999    9\n\nplot(spread)",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Validation in R</span>"
    ]
  },
  {
    "objectID": "validationinr.html#model-class-selection",
    "href": "validationinr.html#model-class-selection",
    "title": "Validation in R",
    "section": "Model class selection",
    "text": "Model class selection\n\nStatus quo model: naive estimator (last observation)\nFirst, let’s choose a null model. Here, the naive estimator of the most recent spread suggests itself. We do not have any reason to believe seasonal adjustments should be made, and the overall mean of the series does not seem appropriate (the spreads show slowly evolving regimes, not true stationarity.)\n\n\nDismissed contender: ARIMA\nNext, let’s dismiss ARIMA models as being a poor fit for this data. A glance at the plot and what little we may know of macroeconomics suggest that these spreads are subject to regime changes, structural breaks, parameter drift, and long-term persistence (as opposed to mean-recursion) which all work against a multi-decade ARIMA expression. To quickly confirm this impression, consider the following information.\n\nThe automated ARIMA selection routines pick a dubious model with both differencing and high AR and MA orders after differencing:\n\n\nauto.arima(spread)\n\nSeries: spread \nARIMA(3,1,4) \n\nCoefficients:\n          ar1      ar2     ar3     ma1     ma2      ma3      ma4\n      -0.5169  -0.1661  0.6565  0.8943  0.2438  -0.7462  -0.4064\ns.e.   0.1472   0.1749  0.1458  0.1423  0.2020   0.1843   0.0554\n\nsigma^2 = 0.06784:  log likelihood = -38\nAIC=91.99   AICc=92.26   BIC=126.57\n\n\n\nThe model this automated routine selects has AR and MA roots very near the unit circle, suggesting the true process is neither stationary nor invertible, even after differencing:\n\n\nplot(auto.arima(spread))\n\n\n\n\n\n\n\n\n\n\nSelected contender: Exponential smoothing\nSometimes less is more. I would like to try a simple time series model which does not add a lot of sophistication to the anlaysis (if treasury spreads were very forecastable, we could all soon make our fortunes in fixed income.) Exponential smoothing models with a damped trend and no seasonality provide a nice degree of humility.\nSince some of the data are negative and I see no obvious need for mean-variance stabilization, I will do without a Box-Cox transformation.\n\nspread.ets.full &lt;- ets(spread,model='AAN',damped=TRUE)\nsummary(spread.ets.full)\n\nETS(A,Ad,N) \n\nCall:\nets(y = spread, model = \"AAN\", damped = TRUE)\n\n  Smoothing parameters:\n    alpha = 0.9999 \n    beta  = 0.0672 \n    phi   = 0.8 \n\n  Initial states:\n    l = 0.4028 \n    b = 0.0782 \n\n  sigma:  0.2824\n\n     AIC     AICc      BIC \n2124.681 2124.833 2150.627 \n\nTraining set error measures:\n                        ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -4.696819e-05 0.2810856 0.1727086 13.01432 66.98131 0.2276596\n                  ACF1\nTraining set 0.2524138\n\nplot(tail(spread,n=24),lwd=2,ylab='10Y - 1Y spread',\n     main='In-sample fitted values for ETS(A,Ad,N)')\nlines(tail(fitted(spread.ets.full),n=24),col='#0000ff',lty=2,lwd=2)\n\n\n\n\n\n\n\n\nThis exponential smoothing model heavily weights \\((\\alpha \\approx 1)\\) the most recent observation (in fact, it effectively only uses the most recent observation), but it does include a non-trivial trend \\((b \\gt 0)\\)) from a large historical sample period \\((\\beta \\ll 1\\)) and a moderate dampening effect \\((\\phi \\lnapprox 1)\\).\nSome of these parameters will change throughout the validation period. Even if the final ETS model that R selects would be different, I’m willing to try this one because I believe it offers flexibility without overfitting.\n\n\nSelected contender: Feedforward neural network\nKnowing that we are predicting several periods into the future (a weakness of ETS’s recursive structure), I am also interested in an “outside opinion” of sorts. A basic neural network functions somewhat similarly to a non-linear regression; no time series dependencies are learned, just features correlated with outputs. I wonder if this would be a good match for treasury spreads. I’ll use one year of past monthly yields as features.\n\n# define and scale the features (note: h=1 here)\nlagged &lt;- embed(spread,13)\nX &lt;- lagged[,2:13]\nY &lt;- lagged[,1]\nX_mean &lt;- colMeans(X)\nX_sd   &lt;- apply(X,2,sd)\nX_scaled &lt;- scale(X,X_mean,X_sd)\n#run the network on the scaled features (note: h=1 here)\nset.seed(0127)\nspread.nn.full &lt;- nnet(X_scaled,Y,\n                       size=7,linout=TRUE,maxit=1000,trace=FALSE)\nplot(tail(spread,n=24),lwd=2,ylab='10Y - 1Y spread',\n     main='In-sample fitted values for neural network')\nlines(ts(tail(predict(spread.nn.full),n=24),start=1997.75,frequency=12),\n      col='#0000ff',lty=2,lwd=2)",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Validation in R</span>"
    ]
  },
  {
    "objectID": "validationinr.html#validation",
    "href": "validationinr.html#validation",
    "title": "Validation in R",
    "section": "Validation",
    "text": "Validation\nFrom here, let’s decide to use cross-validated RMSE and MASE as our two model validation metrics. I’d like to show a “manual” coding of the cross-validation process, though automated versions exist using either the forecast or fable packages.\n\nJudgment calls\nWe begin with a series of judgment calls:\n\nWe have 558 observations. Let’s save the last 24 for a final model trial (simulating the use of our winning model in production).\nLet’s enforce a rolling 240-observation training window, which is equivalent to 20 years of monthly returns. That should be enough data to train both an ETS model and a feedforward neural net, without mixing too many different economic regimes, and still allowing a large holdout validation set.\nWe will compute the four-ahead ETS forecasts recursively while computing the four-ahead neural network forecasts directly. (For recursive neural network behavior, I would use forecast::nnetar.)\nThat means our first cross-validation training set will use spread observations 13 though 252 (observations 1 through 12 are used to build the neural net features) and predict spread 256 (July 1974). Our last cross-validation training set will use spread observations 291 through 530 and predict spread 534 (September 1997).\n\n\n\nCode and initial results\n\nh &lt;- 4 # forecasting horizon\nd &lt;- 240 # training sample size\nt &lt;- 256:534 # validation forecasting points (not training points)\nset.seed(0130) # note the nn will give different results with different seeds\n\n# compute naive forecasts\ncv.null &lt;- spread[t - 4]\n\n# compute ets forecasts\nets.iter &lt;- function(yhat.index){\n  y.iter &lt;- spread[yhat.index - h - d + (1:d)]\n  model.iter &lt;- ets(y.iter,model='AAN',damped=TRUE)\n  forecast(model.iter,h=h)$mean[h]}\n\ncv.ets &lt;- sapply(t,ets.iter)\n\n# compute nn forecasts\np &lt;- 12 # feature count for nn\nlagged &lt;- embed(spread,h+p)\nX &lt;- lagged[,h+(1:p)]\nY &lt;- lagged[,1]\n\nnn.iter &lt;- function(yhat.index){\n  embed_row &lt;- yhat.index - p - h + 1\n  X.iter &lt;- X[embed_row - (d-1):0,]\n  Y.iter &lt;- Y[embed_row - (d-1):0]\n  X_mean &lt;- colMeans(X.iter)\n  X_sd   &lt;- apply(X.iter,2,sd)\n  X_scaled &lt;- scale(X.iter,X_mean,X_sd)\n  model.iter &lt;- nnet(X_scaled,Y.iter,size=7,linout=TRUE,maxit=1000,trace=FALSE)\n  predict(model.iter)[d]}\n          \ncv.nn &lt;- sapply(t,nn.iter)\n\nrbind(actual=spread[256:265],\n      cv.null=round(cv.null[1:10],2),\n      cv.ets=round(cv.ets[1:10],2),\n      cv.nn=round(cv.nn[1:10],2))\n\n         [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8] [,9] [,10]\nactual  -0.99 -1.32 -0.83 -0.15  0.02  0.12  0.67  1.41 1.62  1.33\ncv.null -0.55 -1.11 -1.20 -1.13 -0.99 -1.32 -0.83 -0.15 0.02  0.12\ncv.ets  -0.66 -1.38 -1.46 -1.23 -1.01 -1.45 -0.80 -0.15 0.08  0.31\ncv.nn   -0.75 -1.32 -0.52 -0.18 -0.03  0.20  0.60  1.41 1.47  1.34\n\nplot(window(spread,start=1974.5,end=1977.49),lwd=2,ylab='10Y - 1Y spread',\n     main='First 36 cross-validation forecasts')\nlines(time(spread)[256:291],cv.null[1:36],col='#0000ff')\nlines(time(spread)[256:291],cv.ets[1:36],col='#ff0000')\nlines(time(spread)[256:291],cv.nn[1:36],col='#7f00ff')\nlegend(x='bottomright',legend=c('Actual','Naive','ETS','NNet'),\n       lwd=c(2,1,1,1),col=c('#000000','#0000ff','#ff0000','#7f00ff'),bty='n')\n\n\n\n\n\n\n\n\nExcellent. All signs are positive. None of our models are prone to explosive errors (by design), and the CV forecasts show functional (if imperfect) four-ahead forecasts. Let’s see how they do on our metrics of RMSE and MASE:\n\nrmse &lt;- function(pred,act) sqrt(mean((act-pred)^2))\nc(null=rmse(cv.null,spread[t]),\n  ets=rmse(cv.ets,spread[t]),\n  nn=rmse(cv.nn,spread[t]))\n\n     null       ets        nn \n0.7941786 0.8469242 0.3147148 \n\nmase &lt;- function(pred,act,null) mean(abs((pred-act)/mean(abs(act-null))))\nc(null=mase(cv.null,spread[t],cv.null),\n  ets=mase(cv.ets,spread[t],cv.null),\n  nn=mase(cv.nn,spread[t],cv.null))\n\n     null       ets        nn \n1.0000000 1.0444317 0.4065807 \n\n\nThe results are clear. The exponential smoothing model isn’t terrible, but it does not outperform a naive forecast of the last-known observation. Its average error is 4% larger than the average naive forecaster. Meanwhile, the neural network produces errors that are 60% smaller than the naive forecaster, and typically forecasts four-month-ahead spreads to within about 31bps.\n\n\nExamining a cost function instead of model metrics\nWhat about an actual cost function? This would depend very much on the use of your spread forecasting model. As an illustration, however, consider a trader who bets every month whether the spread will widen or tighten in four months. They will profit in months they guess correctly and lose in months they guess wrong.\n\nc(null=mean(sign(spread[t]-spread[t-4])==sign(cv.null-spread[t-4])),\n  ets=mean(sign(spread[t]-spread[t-4])==sign(cv.ets-spread[t-4])),\n  nn=mean(sign(spread[t]-spread[t-4])==sign(cv.nn-spread[t-4])))\n\n       null         ets          nn \n0.007168459 0.516129032 0.795698925 \n\n\n\nThe null model is of no help to our trader: by construction, it predicts no change over the next four months, and it is right less than 1% of the time.\nThe exponential smoothing model would only call the direction of the spread movement correctly 51% of the time, hardly enough to call a trading strategy.\nThe neural network would predict the spread direction correctly 80% of the time, not a sure bet but much better (and more profitable) than the other models.\n\nWith some hypothetical investment strategies and rules for when to commit a reserve of funds, you could create a true cost function around these ideas which might be enough to attract the attention of actual industry professionals.\n\n\nHypothetical performance “in production”\nWe held back 24 months to simulate the idea that the best model is chosen and put into production. What would that look like?\nIn some scenarios, the model is continually updated and re-run before each new prediction — this would essentially extend the time series cross-validation process from “model selection” to “in-production forecasting” with no change. It would be common to see this used on monthly forecasts, since the model refresh would be infrequent and low-lift.\nIn other scenarios, the model chosen at the end of the validation period is “frozen” and used with exactly the same parameters until a later model refresh. Even though this method would not be common with the scenario we are considering, we can simulate it to notice the drop in accuracy often seen when “really” forecasting:\n\nset.seed(0131)\nembed_row &lt;- max(t) - p - h + 1\nX.final &lt;- X[embed_row - (d-1):0,]\nY.final &lt;- Y[embed_row - (d-1):0]\nX_mean &lt;- colMeans(X.final)\nX_sd   &lt;- apply(X.final,2,sd)\nX_scaled &lt;- scale(X.final,X_mean,X_sd)\nmodel.final &lt;- nnet(X_scaled,Y.final,size=7,linout=TRUE,maxit=1000,trace=FALSE)\n\npreds.final &lt;- predict(model.final,newdata=scale(X[embed_row+1:24,],X_mean,X_sd))\nrmse(preds.final,spread[535:558])\n\n[1] 0.3069447\n\nmase(preds.final,spread[535:558],spread[531:554])\n\n[1] 1.121129\n\nplot(window(spread,start=1997.75),lwd=2,ylab='10Y - 1Y spread',\n     main='Hypothetical production run of frozen Nnet model')\nlines(time(spread)[535:558],preds.final[1:24],col='#0000ff')\nlegend(x='bottomright',legend=c('Actual','Forecasted'),\n       lwd=c(2,1),col=c('#000000','#0000ff'),bty='n')\n\n\n\n\n\n\n\n\nMixed news from our simulated future: the neural network continues to hold its same absolute predictive power (OOS RMSE of 31bps). However, the movements of the yield spread during these twelve months would actually be better predicted by a naive estimator, since the average neural network error is roughly \\12% larger than the those produced by the fourth lag.",
    "crumbs": [
      "4. Forecasting and validation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Validation in R</span>"
    ]
  }
]